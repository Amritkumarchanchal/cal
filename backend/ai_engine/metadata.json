{"Learn Machine Learning Like a GENIUS and Not Waste Time": [{"content": "so you want to learn machine learning and you somehow ended up here well I've got good news and bad news the bad news I'm not here to sell you some become an ml engineer in 3 months fantasy could it happen sure people also win the lottery the good news I've been exactly where you are about 8 years ago I thought I knew enough Basics to just apply for jobs and learn the rest while working spoiler alert I failed miserably but those failures taught me exactly what works and what doesn't and that's what I'm sharing today I've since taught all of this to hundreds of students in several countries so I think I have a good idea on what works and what doesn't hopefully I can save you months of frustration by showing you the smart way to learn machine learning learning data science was one of the best decisions I ever made and most of you can do it and you will learn some cool stuff on the way that even if you don't become a data scientist or machine learning engineer you will have learned programming how to build apps how to analyze and visualize data you will have strong statistics and research skills and be able to communicate data clearly many amazing job options will be open to you even if you don't become a data scientist but you will have to work hard but what should you work hard on and how do we even start that's what I'm here to tell you learning how to learn before we even touch machine learning let's talk about something crucial learning how to learn why because machine learning and AI like most things in Tech are constantly evolving and what matters isn't just what you know but how quickly you can adapt and learn new things a little secret I actually suck at at programming and algorithms but I am really good at learning new stuff here's why this matters specifically for machine learning technology changes fast new platforms and Frameworks drop constantly and new papers come out daily what's hot today might be obsolete tomorrow problem solving is everything machine learning isn't about memorizing algorithms it's about understanding data and patterns about breaking down complex problems and finding Creative Solutions confidence AKA don't get overwhelmed or scared of big problems people who know how to learn and problem solve don't get paralyzed when faced with a new big problem they develop a strategy for how to look at a new problem and break it down into manageable problems they have faced before they know how to look up Solutions and find tools necessary to solve new problems they adapt more quickly when Tech changes efficiency if you know how to learn you won't waste time on unnecessary things time is money learn what you actually need to get where you want to be there's no one size fits all solution for learning something it depends on your style of learning but also on your goals not everyone needs to learn everything so how do you learn how to learn this one you kind of have to figure out for yourself because what works for one person doesn't necessarily work for the next some people learn well with graphs and diagrams others with text others maybe with voice notes some people need to understand the theory before applying it others need to jump right in and use an algorithm before asking what it actually does in this video I will try to show you what worked for me while giving you resources that I believe will get you there as quickly as possible I will just mention a principle that has helped me a lot throughout my career the the Paro principle sometimes called 8020 principle it says that 80% of the results come from 20% of the effort constantly ask yourself why am I doing this is this actually getting me where I want to be or can I do something more useful with my time well the answer to this question isn't always the same for everyone I will try to now give you the 20% of the work that would have gotten me 80% of the way to becoming a data scientist adapt as needed but where do I start now let's build your machine Learning Foundation the right way here's your road map python while the next skill is at least as important I would start with learning python the main reason is that you will get a feeling of achievement fairly quickly and python is super simple why python python is the main language of data analytics data science and machine learning while also being a full-fledged programming language allowing you to write scripts build apps and websites and much more python will allow you to actually start writing real code within days without having to learn super complicated computer science Concepts like pointers memory allocation and garbage collection also with python you will be able to get a job as a programmer or data analyst or web developer even if you don't learn all the hard machine learning stuff I suggest you first install jupyter notebooks as they make learning much easier and jupyter notebooks are also a core tool for data analysts and data scientists all over the world then learn about these Core Concepts programming fundamentals basic syntax indentation rules comments and so on variables math if else Loops printing data types like strings ins floats booleans lists dictionaries functions classes and objects modules packages and importing do a pandas tutorial pandas is Python's primary data manipulation Library built for handling tabular data through data frame objects imagine it as Excel spreadsheets on steroids it will be your main tool for data analysis cleaning and transformation with powerful functions for merging reshaping and analyzing data the library strength lies in combining the power of numpy arrays with spreadsheet like functionality and SQL database like joints it also comes with built-in plotting functionality built on top of Python's powerful met plot lib libraries pandas is a true data analysis Powerhouse if you truly Master pandas you will excel at most data analysis positions in the world also because exploratory data analysis and data preparation are about 60 to 80% of a data scientist job it will also lay the foundation for that 8020 principle remember your First Data analysis project so before you get into any machine learning I would take the time here to work on an actual project to deepen your python pandas and data analysis knowledge as I mentioned in my previous videos real projects beat tutorials at developing a good data scientist find some data you want to analyze maybe from one of your old jobs or school maybe you can export some data from your favorite health tracker or ask some friends if they have some data they want analyzed or maybe download public data from the government the World Bank or a nonprofit any topic you're interested in it could be economics Sports politics video games the board games this last one was a passion of one of my former students work on importing the data into pandas clean up the data make the units uniform decide what to do about missing data and outliers plot the different variables look at correlations between variables and come up with some hypothesis about the data and test them by making more plots turn your results into a slideshow with nice graphs that tell a story that you can present to friends and family Pro tip Jupiter notebooks with data and plots can be turned directly into a slideshow this will also be the first project for your port portfolio which you can show when applying for jobs as a data analyst essential math for machine learning this might be the part that most of you fear the most but I think it is the most important part for anyone wanting to learn machine learning you should take this seriously you don't need to be a math genius or know about all of math to become good at machine learning but you need to really understand the Core Concepts from the areas I'm about to mention for more details on math for machine learning check out my video on the topic basic statistics and probability this for me is the most important Branch as a data analyst and data scientist now there are many online resources for statistics but I highly suggest taking the Con Academy statistics and probability course this course is completely free and is the one I took when I prepared for my first job as a data scientist the full course is probably around 50 hours of content so if you have prior math knowledge you probably won't spend more than 100 hours on this but it might take you longer that's around 2 3 weeks of full-time self-study more if much of this is completely new to you but please take the time to do this it will make everything that follows so much easier and save you much more than 100 hours of headaches later on ideally while you learn new Concepts here you go to your data set from the previous data analysis project phase and try to apply them there to deepen your intuition linear algebra fundamentals while also important linear algebra for machine learning is much more about learning some tools and rules this should be much quicker than learning probability and statistics Concepts the main thing you want to learn is how to operate with vectors and matrices and learn what the different operations mean this is more about mathem iCal tools and notations than Concepts I think learning this will take about a quarter to a third of the time it took you to learn the statistics Concepts so one or two weeks of studying should be enough for people with prior math knowledge I will also leave the link to the Khan Academy linear algebra course in the description calculus here again it's about learning some tools but also understanding what derivatives are conceptually and how they help in optimization problems you should really understand how functions and their derivatives work and know the basic rules of differentiation like the chain rule I would again calculate with one or two weeks if you have prior math knowledge I will also leave a con Academy Link in the description Pro tip you need working knowledge not a math PhD focus on intuition over proofs spend most your time on statistical concepts for linear algebra and calculus focus on learning the tools like Matrix operations and how to take the derivative of a function the core machine learning Concepts and algorithms now here's where many people mess up they jump straight to Deep learning but that's a mistake in my opinion you should spend most of your time on simple algorithms the reasons for that are manifold and discussed in my previous videos but basically many problems don't require complicated solution simple algorithms like linear regression are quicker to run they're more generalizable more interpretable and easier to learn from and communicate and more importantly these algorithms form the basis for the more complicated algorithms like neural networks so truly understanding them will help you understand the more complicated algorithms better too check out my video on machine learning algorithms but basically before getting into neural networks or even svm make sure you understand how linear regression and logistic regression work then look at decision trees and Ensemble algorithms like random forests and gradient boosting I learned most of these topics from the book an introduction to statistical learning the majority of you will prefer learning from videos so just watch the Youtube video series about this book by the authors themselves completely for free on YouTube I will leave a link in the description all the videos together are about 20 hours but since you will want to pause and take notes and read up on certain Concepts I think this will be another 100 hours or so of study time so that should be another two or more weeks of full-time self-study all these numbers are estimates as everyone learns at different speeds do a scikit learn tutorial scit learn is the number one machine learning library in the world for basic machine learning algorithms you can do a basic sklearn tutorial in a day or two and the good thing is that the simple and consistent syntax makes it such that once you know how to use the library for one algorithm you know how to use it for any algorithm as long as you know what algorithm is meant for what which you now know because you just learned it pyit learn also comes with great documentation and toy data sets to play around with I suggest you start using psyit learn while you are learning about the algorithms in the statistical learning course the genius move while you learn about the theory behind new algorithms for example going through the statistical learning course and starting with linear regression Implement and use the algorithm in the following three three ways implemented from scratch using basic python implemented using scikit learn using a toy data set then use both your own implementation and the syit learn toolkit to try out the algorithm on a real data set that you have prepared yourself now there's a common Pitfall that many beginners get stuck in tutorial hell it's where you essentially just keep following tutorials without striking out on your own and actually building something most learning comes from the trial and error of building an application so if you always follow a tutorial you get Stu at a basic level how do you not get stuck there do just one or two tutorials per area Max and then work on a real project your first machine learning project here you can either continue with your data analysis project from before or find a new more interesting data set for a machine learning project but don't forget to still use pandas to do an exploratory data analysis to prepare your data for modeling and form hypothesis about your data more often than not the goal of a machine learning project will be to predict some variable from other variables research the industry of your project a bit look at the data and make some hypothesis about what might influence your target variable either based on intuition about the industry or from looking at correlations and Scatter Plots of different variables design new features based on your knowledge of the problem then start modeling but start with simple algorithms like linear regression logistic regression and decision trees then move on to more complex algorithms like svm random forests or gradient boosting note how as complexity increases accuracy usually increases but interpretability decreases your goal is usually to find a sweet spot also don't forget over fitting as you increase in model complexity keep validation and test sets aside before starting to model and compare your models using the test set at the very end many times the more complex algorithms don't look as good anymore once you use the final test set it might be a good idea to work with data sets that have been published on sites like kaggle to then compare your Solutions and accuracies to other people's Solutions and get an idea of how well your models are in comparison to others but don't get frustrated a lot of people on kaggle are professionals with years of experience if you get anywhere close in accuracy you should be happy don't know what to work on you can start with a tutorial but instead of following it directly after building the core features add some features change some features swap out the data set and try to break your code and then fix it this is one of the best ways to learn while not getting stuck in tutorial hell collaborate and share your projects with others learning ML and isolation is the slowest way to learn instead find coding buddies to work on a project with present your work to friends and family or post it publicly on GitHub or in machine learning communities have someone more advanced than you give you feedback this will speed up your learning 10 times don't know anyone to work on a project with participate in a hackathon or write to people with similar interests on kaggle GitHub Discord Reddit LinkedIn Etc the connections you form this way will not only help you learn better but boost your career in unexpected ways check out my most recent video to learn more about the importance of networking and data science Advanced topics only now should you look at more advanced topics deep learning architectures cnns for computer vision RNN for sequential data or Transformers for NLP Advanced optimization techniques model deployment strategies and the latest research papers remember learn these by need not by fomo you don't need to know everything just learn these techniques if they are important to your project here some dos and don'ts don't don't get stuck in tutorial hell don't try to memorize everything don't learn in isolation don't chase every new trend don't copypaste code without understanding don't try to learn every new fancy tool or research paper instead build real projects focus on understanding share your progress join communities Master fundamentals first Implement from scratch learn by doing if you found this video helpful share it with someone who you think might also like it and get started on one of the tutorials in the description or on this very Channel also consider liking the video and subscri subcribing to be notified about similar content in the future thanks for watching"}], "5 Beginner AWS Cloud Projects To Get You Hired (2025)": [{"content": "hey everyone this is Lucy and in this video I'll be walking you through five beginner friendly AWS Cloud projects now as I've said on this channel over and over again the best way to build your Hands-On skills with AWS is by creating a project certifications are a great starting point but to really bridge that gap between your theoretical knowledge and what's required to Le a job in the cloud it's important for you to think about how you can use multiple AWS Services together to build a solution this solution could be a static website a mobile application or even a chat bot that can respond to simple queries the possibilities are endless but the problem is how do you get started should you read through hundreds of pages of documentation first or should you just jump straight into it and hope for the best well to be honest there's no one correct approach when starting a cloud project Some people prefer to follow a step-by-step tutorial While others might want the flexibility to build something without the exact instructions already provided to them my personal preference and what I recommend to beginners is to follow guided tutorials for your first few Cloud projects more importantly learn how to document them and speak about your projects during Cloud interviews and so over the next few months I'll be releasing a series of videos to help you build AWS Cloud projects at three different skill levels beginner intermediate and advanced if you want to stay updated make sure you subscribe to the channel and turn on notifications all right let's just dive straight into it the first beginner Cloud project is to build a daily task schedule application using party rock this platform allows you to create an application by simply describing what you want making it a great choice for beginners looking to build AI applications to get started visit party Rock's website and create an account select the build your own app option and this will open up an interface for you to create your own application now you might be wondering what can party rock actually be used for well as cheesy as it sounds the possibilities are endless you can create applications for your own personal use such as a to-do list or Resume Builder you could also create an application for your workplace such as a project management tool with Party Rock people have even built things like motivational quote generators and personality quizzes if you'd like to learn how to build a daily task schedule application I've included the full tutorial in the description below I've also created a guide book with the complete step-by-step instructions to all five projects in this video so feel free to check that out as well but yeah here are the overall steps you need to take if you'd like to build it out so start by entering a detailed prompt that describes the daily task schedule application you envision make sure you're descriptive enough so that partyy rock can understand exactly what you want once you submit your prompt the AI gets to work creating an application that fits your needs after the application is generated you'll be introduced to the customization process this gives you the option to add widgets like user input static text image generation and even a chatbot you can also change the layout of your application to match your brand or personal preferences once you finalize your application you can publish it and share it with others the estimated time for this project is around 15 to 20 minutes but of course it all depends on how complex you want your application to be be and how many features you'd like the cost for this project is $0 since party rock is offering all users a free trial to their service all right project number two is to build an image labels generator using Amazon recognition now this project is another fun one because we'll be processing images and labeling them for example if you have a photo of a cat amaz recognition will be able to identify what it is and label the image as a cat as mentioned I have the complete guide book for all five projects in the description below but here's the architectural diagram and the overall steps you'll need to take to build out the project so the Journey Begins with setting up an Amazon S3 bucket which will serve as the repository for the images you wish to analyze next you'll create an IM am roll and make sure Amazon recognition and S3 have access to each other after that you'll need to install the AWS C and write some code to use the detect labels option for images and finally use a python Library called map plot Li to visualize labels and add bounding boxes to items identified in the images this is the result of what one of your images could look like you could see that one use case of Amazon recognition could be in a smart surveillance system to recognize suspicious objects and activities on the road other potential use cases include identifying products in a store for inventory management analyzing customer behavior on retail stores and providing accessibility options to those who are visually impaired as you can probably tell Amazon recognition is a pretty useful service that can be applied in many Industries so I would recommend building a project to get some experience with it and to see it in action the project will take you about 20 minutes to build and Falls within the free tier now before we move on to the next project I'd like to share with you a helpful tool that will significantly help you on your journey of building AWS Cloud projects some of you might have heard of M before they're a platform that offers an innovation workspace for teams to collaborate well miror has recently released a really cool feature called AWS Cloud view AWS Cloud view allows you to instantly visualize your Cloud architecture by directly importing data from your aw account creating a clear view in just 20 seconds as you all know architectural diagrams are a very important part of documenting every cloud project so instead of manually creating your own diagrams you can create them with the help of miror here's how you can use mirror's edu as Cloud view first either link a cross account roll or upload a Json file containing data about your AWS resources next follow the instructions to select the relevant account and region you wish to visualize and finally sit back while they generate the diagram here's an example AWS diagram generated through Cloud view if you're looking to present your Cloud projects professionally or just want a faster way to create Cloud architectures AWS Cloud view in Miro is a complete Game Changer I'll leave a link to it in the description below moving on to the third project this one is to develop a text narrator using Amazon poly now Amazon poly is a service that turns text to speech allowing you to create an application that can talk now this is great when you don't feel like reading something and just want to listen to the audio version one example is is converting a blog post into an audio book here's the architectural diagram for the project and the steps you need to take so firstly find a piece of text that you'd like spoken out this could be anything from books and articles to newsletters and scripts next create an AWS Lambda function that acts as a bridge between your text and Amazon poly after that customize the voice in Amazon poly to match the tone and style of your content adjusting parameters like pitch and speed now even though this project may seem simple it does require a good understanding of AWS services and how to integrate them it also gives you a taste of how powerful text to speech capabilities can be in enhancing customer experience the estimated time to complete this project is 20 minutes and it also falls within the free tier the fourth Cloud project to help you get highed in 2024 is to build a language translation Bard this project uses three AWS Services AWS Lambda Amazon Lex and Amazon translate so for example if you want to translate a word or sentence into another language all you have to do is type into the chat bot and it will output the translation sounds straightforward enough right here's the architectural diagram and the steps you need to take firstly create a chatbot in Amazon Lex and Define clear user intents next specify utterances or phrases that users might say to interact with your Bot once that's done Define slots within the intents such as language or text to capture the specific information needed for the translation after that we're going to need a Lambda function that takes the slot data and perform a translation using Amazon trans translate and finally we'll integrate the Lambda function back into the Amazon Lex chatbot and deliver the translation smoothly to the user some potential use cases of an Amazon Lex translation bot include assisting businesses in communicating with International clients and helping Travelers communicate with locals from different languages you might be wondering why can't I use a translation app that has already been made well the answer is that businesses often require customized translation services especially when dealing with technical terms or industry specific language by using Amazon Lex you have the flexibility to tailor your Bot based on your specific needs overall building a language translation bot not only showcases your knowledge of using AWS services but is also a skill set for companies looking to build chatbots this project will take you about 1 to 2 hours to build and can be done for free through the aeds free tier the fifth and final project for today is to deploy a bucket list tracker application on AWS amplify this project once you build it out will help you keep a track of all the things you want to do in life you can enter bucket list items and delete them once they're complete now you can see that the architectural diagram for this project looks a bit more complex than the other ones and that's because we'll need multiple ads services to build this application the first step is to build your application with react focusing on userfriendly design and functionality that allows users to manage the bucket list items effectively next initialize a GitHub repo and connect your local development environment to GitHub after that use AWS amplifier to host your front end and Implement amplifier authentication to to add user authentication features like login and sign up once that's done develop the back end using AWS appsync and a graph Cur API for efficient data handling you can then integrate it with Dynamo DB for data storage finally deploy your application on AWS amplify test it out and make any necessary adjustments like all the other projects mentioned in this video this one Falls within the free tier however this final project will take you a bit longer to build around 1 and 1/2 to 2 hours and there you have it five AWS Cloud projects to help you get handson get hired and Advance your career if you found this video helpful please remember to give it a like and let me know in the comments which project was your favorite I'd also recommend checking out this video I made on how to document your AWS Cloud projects thanks so much for watching and I'll see you soon bye for now"}], "Justice: What's The Right Thing To Do? Episode 01 \"THE MORAL SIDE OF MURDER\"": [{"content": "Funding for this program is provided by: Additional funding provided by This is a course about Justice and we begin\nwith a story suppose you're the driver of a trolley car, and your trolley car is hurdling down\nthe track at sixty miles an hour and at the end of the track you notice\nfive workers working on the track you tried to stop but you can't your brakes don't work you feel desperate because you know that if you crash into these five workers they will all die let's assume you know that for sure and so you feel helpless until you notice that there is off to the right a side track at the end of that track there's one worker working on track you're steering wheel works so you can turn the trolley car if you want to onto this side track killing the one but sparing the five. Here's our first question what's the right thing to do? What would you do? Let's take a poll, how many would turn the trolley car onto the side track? How many wouldn't? How many would go straight ahead keep your hands up, those of you who'd go straight\nahead. A handful of people would, the vast majority\nwould turn let's hear first now we need to begin to investigate the reasons\nwhy you think it's the right thing to do. Let's begin with\nthose in the majority, who would turn to go onto side track? Why would you do it, what would be your reason?"}, {"content": "Who's willing to volunteer a reason? Go ahead, stand up. Because it can't be right to kill five people\nwhen you can only kill one person instead. it wouldn't be right to kill five if you could kill one person instead that's a good reason that's a good reason who else? does everybody agree with that reason? go ahead. Well I was thinking it was the same reason it was on 9/11 we regard the people who flew the plane who flew the plane into the Pennsylvania field as heroes because they chose to kill the people on the\nplane and not kill more people in big buildings. So the principle there was the same on 9/11 it's tragic circumstance, but better to kill one so that five can\nlive is that the reason most of you have, those\nof you who would turn, yes? Let's hear now from those in the minority those who wouldn't turn. Well I think that same type of mentality that\njustifies genocide and totalitarianism in order to save one type of race you\nwipe out the other. so what would you do in this case? You would to avoid the horrors of genocide you would crash into the five and kill them? Presumably yes."}, {"content": "okay who else?"}, {"content": "That's a brave answer, thank you. Let's consider another trolley car case and see whether those of you in the majority want to adhere to the principle, better that one should die so that five\nshould live. This time you're not the driver of the trolley\ncar, you're an onlooker standing on a bridge overlooking a trolley car track and down the track comes a trolley car at the end of the track are five workers the brakes don't work the trolley car is about to careen into the\nfive and kill them and now you're not the driver you really feel helpless until you notice standing next to you leaning over the bridge is it very fat man. And you could give him a shove he would fall over the bridge onto the track right in the way of the trolley car he would die but he would spare the five. Now, how many would push the fat man over the bridge? Raise your hand. How many wouldn't? Most people wouldn't. Here's the obvious question, what became of the principle better to save five lives even if it means\nsacrificing one, what became of the principal that almost everyone endorsed in the first case I need to hear from someone who was in the\nmajority in both cases is how do you explain the difference between\nthe two? The second one I guess involves an\nactive choice of  pushing a person and down which I guess that that person himself would otherwise not \nhave been involved in the situation at all and so to choose on his behalf I guess to  involve him in something that he otherwise would\nhave this escaped is I guess more than what you have in the first case where the three parties, the driver and the two sets of workers are already I guess in this situation. but the guy working, the one on the track\noff to the side he didn't choose to sacrifice his life any\nmore than the fat guy did, did he? That's true, but he was on the tracks. this guy was on the bridge. Go ahead, you can come back if you want. Alright, it's a hard question but you did well you did very well it's a\nhard question. who else can find a way of reconciling the reaction of the majority in these two cases? Yes? Well I guess in the first case where you have the one worker and the five it's a  choice between those two, and you have to  make a certain choice and people are going to die \nbecause of the trolley car  not necessarily because of your direct actions. The trolley car is a runway, thing and you need to make in a split second choice whereas pushing the fat man over is an actual\nact of murder on your part you have control over that whereas you may not have control over the trolley car. So I think that it's a slightly different situation."}, {"content": "Alright who has a reply? Is that, who has a reply to that? no that was good, who has a way who wants to reply? Is that a way out of this? I don't think that's a very good reason because\nyou choose either way you have to choose who dies\nbecause you either choose to turn and kill a person which is an act of conscious thought to turn, or you choose to push the fat man  over which is also an active conscious action so either way you're making a choice. Do you want to reply? Well I'm not really sure that that's the case, it just still\nseems kind of different, the act of actually pushing someone over onto the tracks and killing them, you are actually killing him yourself, you're pushing\nhim with your own hands you're pushing and  that's different than steering something that is going to\ncause death into another...you know it doesn't really sound right saying it now when I'm up here. No that's good, what's your name?"}, {"content": "Andrew. Andrew and let me ask you this question Andrew, suppose standing on the bridge next to the fat man I didn't have to push him, suppose he was standing over a trap door that I could open by turning\na steering wheel like that would you turn it? For some reason that still just seems more  more wrong. I mean maybe if you just accidentally like leaned into\nthis steering wheel or something like that or but,  or say that the car is  hurdling towards a switch that will drop the trap then I could agree with that. Fair enough, it still seems  wrong in a way that it doesn't seem wrong in the\nfirst case to turn, you say An in another way, I mean in the first situation you're\ninvolved directly with the situation in the second one you're an onlooker as well. So you have the choice of becoming involved\nor not by pushing the fat man. Let's forget for the moment about this case, that's good, but let's imagine a different case. This time\nyour doctor in an emergency room and six patients come to you they've been in a terrible trolley car wreck five of them sustained moderate injuries one\nis severely injured you could spend all day caring for the one severely injured victim, but in that time the five would die, or you could\nlook after the five, restore them to health, but during that time the one severely injured person would die. How many would save  the five now as the doctor? How many would save the one? Very few people, just a handful of people. Same reason I assume, one life versus five. Now consider another doctor case this time you're a transplant surgeon and you have five patients each in desperate\nneed of an organ transplant in order to survive on needs a heart one a lung, one a kidney,  one a liver and the fifth a pancreas. And you have no organ donors you are about to see you them die and then it occurs to you that in the next room there's a healthy guy who came in for a checkup. and he is you like that and he's taking a nap you could go in very quietly yank out the five organs, that person would\ndie but you can save the five. How many would do it? Anyone? How many? Put your hands up if you would do it. Anyone in the balcony? You would? Be careful don't lean over too much How many wouldn't? All right. What do you say, speak up in the balcony, you\nwho would yank out the organs, why? I'd actually like to explore slightly alternate possibility of just taking the one of the five he needs an organ who dies first and using their four healthy organs to save the other\nfour That's a pretty good idea. That's a great idea except for the fact that you just wrecked the philosophical point. Let's step back from these stories and these arguments to notice a couple of things about the way the arguments have began to unfold. Certain moral principles have already begun to emerge from the discussions we've had and let's consider what those moral principles look like the first moral principle that emerged from the \ndiscussion said that the right thing to do the moral thing to do depends on the consequences that will result from your action at the end of the day better that five should live even if one must die. That's an example of consequentialist moral reasoning. consequentialist moral reasoning locates morality\nin the consequences of an act. In the state of the  world that will result  from the thing you do but then we went a little further, we considered\nthose other cases and people weren't so sure  about consequentialist moral reasoning when people hesitated to push the fat man over the bridge or to yank out the organs of the innocent patient people gestured towards reasons having to do with the intrinsic quality of the act itself. Consequences be what they may. People were reluctant people thought it was just wrong categorically wrong to kill a person an innocent person even for the sake of saving five lives, at least these people thought that in the second version of each story we reconsidered so this points a second categorical way of thinking about moral reasoning categorical moral reasoning locates morality\nin certain absolute moral requirements in certain categorical duties and rights regardless of the consequences. We're going to explore in the days and weeks to come the contrast\nbetween consequentialist and categorical moral principles. The most influential example of consequential moral reasoning is utilitarianism,\na doctrine invented by Jeremy Bentham, the eighteenth century English\npolitical philosopher. The most important philosopher of categorical moral reasoning is the eighteenth century German philosopher\nEmmanuel Kant. So we will look at those two different modes of moral reasoning assess them and also consider others. If you look at the syllabus, you'll notice\nthat we read a number of great and famous books. Books by Aristotle John Locke Emanuel Kant, John Stuart Mill, and others. You'll notice too from the syllabus that\nwe don't only read these books, we also all take up contemporary political and legal controversies\nthat raise philosophical questions. We will debate equality and inequality, affirmative action, free speech versus hate speech, same sex marriage, military conscription, a range of practical questions, why not just to enliven these abstract and distant\nbooks but to make clear to bring out what's at stake\nin our everyday lives including our political lives, for philosophy. So we will read these books and we will debate these issues and we'll see how each informs and\nilluminates the other. This may sound appealing enough but here I have to issue a warning, and the warning is this to read these books in this way, as an exercise in self-knowledge, to read them in this way carry certain risks risks that are both personal and political, risks that every student of political philosophy have known. These risks spring from that fact that philosophy teaches us and unsettles us by confronting us with what we already know. There's an irony the difficulty of this course consists in the\nfact that it teaches what you already know. It works by taking what we know from familiar unquestioned settings, and making it strange. That's how those examples worked worked the hypotheticals with which we began with their\nmix of playfulness and sobriety. it's also how these philosophical books work. Philosophy  estranges us from the familiar not by supplying new information but by inviting and provoking a new way of seeing but, and here's the risk, once the familiar turns strange, it's never quite the same again. Self-knowledge is like lost innocence, however unsettling you find it, it can never be unthought or unknown what makes this enterprise difficult but also riveting, is that moral and political philosophy is a story and you don't know where this story will lead\nbut what you do know is that the story is about you. Those are the personal risks, now what of the political risks. one way of introducing of course like this would be to promise you that by reading these books and debating these issues you will become a better more responsible\ncitizen. You will examine the presuppositions of\npublic policy, you will hone your political judgment you'll become a more effective participant\nin public affairs but this would be a partial and misleading promise political philosophy for the most part hasn't\nworked that way. You have to allow for the possibility that political philosophy may make you a worse\ncitizen rather than a better one or at least a worse citizen before it makes you a better one and that's because philosophy is a distancing even debilitating activity And you see this going back to Socrates there's a dialogue, the Gorgias in which one of Socrates\u2019 friends Calicles tries to talk him out of philosophizing. calicles tells Socrates philosophy is a pretty toy if one indulges in it with moderation at\nthe right time of life but if one pursues it further than one should\nit is absolute ruin. Take my advice calicles says, abandon argument learn the accomplishments of active\nlife, take for your models not those people who spend\ntheir time on these petty quibbles, but those who have a good livelihood and reputation and many other blessings. So Calicles is really saying to Socrates quit philosophizing, get real go to business school and calicles did have a point he had a point because philosophy distances us from conventions from established assumptions and from settled beliefs. those are the risks, personal and political and in the face of these risks there is a\ncharacteristic evasion, the name of the evasion is skepticism. It's\nthe idea well it goes something like this we didn't resolve, once and for all, either the cases or the principles we were\narguing when we began and if Aristotle and Locke and Kant and Mill haven't solved these questions\nafter all of these years who are we to think that we here in Sanders Theatre over the\ncourse a semester can resolve them and so maybe it's just a matter of each person having his or her own principles\nand there's nothing more to be said about it no way of reasoning that's the evasion. The evasion of skepticism  to which I would offer the following reply: it's true these questions have been debated for a very\nlong time but the very fact that they have reoccurred and persisted may suggest that though they're impossible in one sense their unavoidable in another and the reason they're unavoidable the reason they're inescapable is that we live\nsome answer to these questions every day. So skepticism, just throwing up their hands\nand giving up on moral reflection, is no solution Emanuel Kant described very well the problem with skepticism\nwhen he wrote skepticism is a resting place for human reason  where it can reflect upon its dogmatic wanderings but it is no dwelling place for permanent settlement. Simply to acquiesce in skepticism, Kant wrote, can never suffice to overcome the restless\nof reason. I've tried to suggest through theses stories\nand these arguments some sense of the risks and temptations of the perils and the possibilities I would\nsimply conclude by saying that the aim of this course is to awaken the restlessness of reason and to see where it might lead thank you very much. Like, in a situation that desperate, you have to do what you have to do to survive. You have to do what you have to do you? You've gotta do What you  gotta do. pretty much,  If you've been going nineteen days without any food someone has to take the sacrifice, someone has to make the sacrifice  \nand people can survive. Alright that's good, what's your name?"}, {"content": "Marcus."}, {"content": "Marcus, what do you say to Marcus? Last time we started out last time with some stores with some moral dilemmas about trolley cars and about doctors and healthy patients vulnerable to being victims of organ transplantation we noticed two things about the arguments we had one had to do with the way we were arguing it began with our judgments in particular cases we tried to articulate the reasons or the\nprinciples lying behind our judgments and then confronted with a new case we found ourselves re-examining those principles revising each in the light of the other and we noticed the built-in pressure to try\nto bring into alignment our judgments about particular cases and the principles we would endorse on reflection we also noticed something about the substance\nof the arguments that emerged from the discussion. We noticed that sometimes we were tempted\nto locate the morality of an act in the consequences in the results, in the state of the world that\nit brought about. We called is consequentialist moral reason. But we also noticed that in some cases we weren't swayed only  by the results sometimes, many of us felt, that not just consequences but also the intrinsic\nquality or character of the act matters morally. Some people argued that there are certain things\nthat are just categorically wrong even if they bring about a good result even if they save five people at the cost of one life. So we contrasted consequentialist moral principles with categorical ones. Today and in the next few days we will begin to examine one of the\nmost influential versions of consequentialist moral theory and that's the philosophy of utilitarianism. Jeremy Bentham, the eighteenth century English political philosopher gave first the first clear systematic expression to the utilitarian moral theory. And Bentham's idea, his essential idea is a very simple one with a lot of  morally intuitive appeal. Bentham's idea is the following the right thing to do the just thing to do it's to maximize utility. What did he mean by utility? He meant by utility the balance of pleasure over pain, happiness over suffering. Here's how we arrived  at the principle of maximizing utility. He started out by observing that all of us all human beings are governed by two sovereign masters, pain and pleasure. We human beings like pleasure and dislike pain and so we should base morality whether we are thinking of what to do in our own lives or whether as legislators or citizens we are thinking about what the law should be, the right thing to do individually or collectively is to maximize, act in a way that maximizes the overall level of happiness. Bentham's utilitarianism is sometimes summed\nup with the slogan the greatest good for the greatest number. With this basic principle of utility on hand, let's begin to test it and to examine it by turning to another case another story but this time not a hypothetical story, a real-life story the case of the Queen versus Dudley and Stephens. This was a nineteenth-century British law case that's famous and much debated in law schools. Here's what happened in the case I'll summarize the story and then I want to hear how you would rule imagining that you are the jury. A newspaper account of the time described the background: A sadder story of disaster at sea was never told than that of the survivors of the yacht Mignonette. The ship foundered in the south Atlantic thirteen hundred miles from the cape there were four in the crew, Dudley was the captain Stephens was the first mate Brooks was a sailor, all men of excellent character, or so the newspaper account tells us. The fourth crew member was the cabin boy, Richard Parker seventeen years old. He was an orphan he had no family and he was on his first long voyage at sea. He went, the news account tells us, rather against the advice of his friends. He went in the hopefulness of youthful ambition thinking the journey would make a man of him. Sadly it was not to be, the facts of the case were not in dispute, a wave hit the ship and the Mignonette went down. The four crew members escaped to a lifeboat the only food they had were two cans of preserved turnips no fresh water for the first three days they ate nothing on the fourth day that opened one of the cans of\nturnips and ate it. The next day they caught a turtle together with the other can of turnips  the turtle enabled them to subsist for the next few days and then for eight days they had nothing no food no water. Imagine yourself in a situation like that what would you do? Here's what they did by now the cabin boy Parker is lying at the\nbottom of the lifeboat in a corner because he had drunk sea water against the advice of the others and he had become ill and he appeared to be dying so on the nineteenth day Dudley, the captain, suggested that they should all have a lottery. That they should all draw lots to see who would die to save the rest. Brooks refused he didn't like the lottery idea we don't know whether this was because he didn't want to take that chance\nor because he believed in categorical moral principles but in any case no lots were drawn. The next day there was still no ship in sight so a Dudley told Brooks to avert his gaze  and he motioned to Stephens that the boy Parker had better be killed. Dudley offered a prayer he told a the boy his time had come and he killed him with a pen knife stabbing him in the jugular vein. Brooks emerged from his conscientious objection\nto share in the gruesome bounty. For four days the three of them fed on the body and blood\nof the cabin boy. True story. And then they were rescued. Dudley describes their rescue in his diary with staggering euphemism, quote: \"on the twenty fourth day as we were having our breakfast a ship appeared at last.\" The three survivors were picked up by a German ship. They were taken back to Falmouth in England where they were arrested and tried Brooks turned state's witness Dudley and Stephens went to trial. They didn't\ndispute the facts they claimed they had acted out of necessity that was their defense they argued in effect better that one should die so that three could survive the prosecutor wasn't swayed by that argument he said murder is murder and so the case went to trial. Now imagine\nyou are the jury and just to simplify the discussion put aside the question of law, and let's assume that you as the jury are charged with deciding whether what they did was morally permissible or not. How many would vote not guilty, that what they did was morally\npermissible? And how many would vote guilty what they did was morally wrong? A pretty sizable majority."}, {"content": "Now let's see what people's reasons are, and let me\nbegin with those who are in the minority. Let's hear first from the defense of Dudley and Stephens. Why would you morally exonerate them? What are your reasons? I think it's I think it is morally reprehensible but I think that there's a distinction between\nwhat's morally reprehensible what makes someone legally accountable in other words the night as the judge said\nwhat's  always moral isn't necessarily against the law and while I don't think that\nnecessity justifies theft or murder any illegal act,  at some point your degree of necessity does\nin fact exonerate you form any guilt. ok. other defenders, other voices for the defense? Moral justifications for what they did? yes, thank you I just feel like  in a situation that desperate you have to do\nwhat you have to do to survive. You have to do what you have to do ya, you gotta do what you gotta do, pretty much. If you've been going nineteen days without any food you know someone just has to take the sacrifice\nhas to make sacrifices and people can survive and furthermore from that let's say they survived and then they become productive\nmembers of society who go home and then start like a million charity organizations and this and that and this and that,\nI mean they benefit everybody in the end so I mean I don't know what they did afterwards, I mean\nthey might have gone on and killed more people but whatever. what? what if they were going home and turned out to be assassins? What if they were going home and turned out to be assassins? You would want to know who they assassinated. That's true too, that's fair I would wanna know who they assassinated. alright that's good, what's your name?"}, {"content": "Marcus. We've heard a defense a couple voices for the defense now we need to hear from the prosecution most people think what they did was wrong, why? One of the first things that I was thinking was, oh well if they  \nhaven't been eating for a really long time,  maybe then they're mentally affected that could be used for the defense,  a possible argument that oh, that they weren't in a proper state of mind, they were making decisions that they otherwise wouldn't be making, and if that's an  \nappealing argument that you have to be in an altered mindset to do something\nlike that it suggests that people who find that argument convincing do you think that they're acting immorally. But I want to know what you think you're defending you k\n0:37:41.249,0:37:45.549\nyou voted to convict right? yeah\n I don't think that they acted in morally  appropriate way. And why not? What do you say,\nHere's Marcus he just defended them, he said, you heard what he said, yes I did yes that you've got to do what you've got to do in a\ncase like that. What do you say to Marcus? They didn't, that there is no situation that would allow human\nbeings to take  the idea of fate or the other people's\nlives into their own hands that we don't have that kind of power. Good, okay thanks you, and what's your name?"}, {"content": "Britt? okay. who else? What do you say? Stand up I'm wondering if Dudley and Stephens had asked for Richard Parker's  \nconsent in, you know, dying,  if that would would that exonerate them from an act of murder, and if so is that still morally\njustifiable? That's interesting, alright consent, now hang on, what's your name? Kathleen."}, {"content": "Kathleen says suppose so what would that scenario look like? so in the story Dudley is there, pen knife in hand, but instead of the prayer or before the prayer, he says, Parker, would you mind we're desperately hungry, as Marcus empathizes with we're desperately hungry you're not going to last long anyhow, you can be a martyr, would you be a martyr how about it Parker? Then, then then what do you think, would\nbe morally justified then? Suppose Parker in his semi-stupor  says okay  I don't think it'll be morally justifiable but I'm wondering. Even then, even then it wouldn't be? No You don't think that even with consent it would be morally justified. Are there people who think who want to take up Kathleen's  consent idea and who think that that would make it\nmorally justified? Raise your hand if it would if you think it would. That's very interesting Why would consent  make a moral difference? Why would it? Well I just think that if he was making his own original\nidea and it was his idea to start with then that would be the only situation in which I\nwould see it being appropriate in anyway\n \n0:40:25.940,0:40:28.359\nbecause that way you couldn't make the argument\nthat he was pressured you know it\u2019s three to one or whatever the ratio was, and I think that if he was making a decision to give his life\nthen he took on the agency to sacrifice himself which some \npeople might see as admirable and other people  might disagree with that decision. So if he came up with the idea that's the only kind of consent we could have\nconfidence in morally, then it would be okay otherwise it would be kind of coerced consent under the circumstances you think. Is there anyone who thinks that the even the consent of Parker would not justify their killing him? Who thinks that? Yes, tell us why, stand up I think that Parker would be killed with the hope that the other crew members\nwould be rescued so there's no definite reason that he should\nbe killed because you don't know  when they're going to get rescued so if you kill him you're killing him  \nin vain do you keep killing a crew member until you're rescued and then you're  \nleft with no one? because someone's going to die eventually? Well the moral logic of the situation seems to\nbe that. That they would keep on picking off the weakest maybe, one by\none, until they were rescued and in this case luckily when three at least were still alive. Now if if Parker did give his consent would it be all right do you think or not? No, it still wouldn't be right."}, {"content": "Tell us why wouldn't be all right. First of all, cannibalism, I believe is morally incorrect so you shouldn\u2019t be eating a human anyway. So cannibalism is morally objectionable outside so then even in the scenario of waiting until someone died still it would be objectionable. Yes, to me personally I feel like of it all depends on one's personal morals, like we can't just, like this is just my opinion of course other people are going to disagree. Well let's see, let's hear what their disagreements\nare and then we'll see if they have reasons that can persuade you or not. Let's try that Let's now is there someone who can explain, those of you who are tempted\nby consent can you explain why consent makes such a moral difference, what about the lottery idea does that count as consent. Remember at\nthe beginning Dudley proposed a lottery suppose that they had agreed to a lottery then how many would then say it was all right. Say there was a lottery, cabin boy lost, and the rest of the story unfolded. How\nmany people would say it's morally permissible? So the numbers are rising if we add a lottery,\nlet's hear from one of you for whom the lottery would make a moral difference why would it? I think the essential element, in my mind that makes it a crime is the idea that they decided at some point that\ntheir lives were more important than his, and that I mean that's kind of the basis for really\nany crime right? It's like my needs, my desire is a more important than yours\nand mine take precedent and if they had done a lottery were everyone\nconsented that someone should die and it's sort of like they're all sacrificing \nthemselves, to save the rest, Then it would be all right? A little grotesque but, But morally permissible? Yes."}, {"content": "what's your name?"}, {"content": "Matt. so, Matt for you what bothers you is not the cannibalism, but the lack of due process. I guess you could say that And can someone who agrees with Matt say a little bit more about why  a lottery would make it, in your view, morally permissible. The way I understood it originally was that that was the\nwhole issue is that the cabin boy was never consulted about whether or not it something was going\nto happen to him even though with the original lottery whether or not he would be a part of that\nit was just decided that he was the one that was going to die. Yes that's what happened in the actual case but if there were a lottery and they all agreed\nto the procedure you think that would be okay? Right, because everyone knows that there's gonna be\na death whereas you know the cabin boy didn't know that this discussion was even happening there was no you know forewarning for him to know that hey, I may be the one\nthat's dying. Okay, now suppose the everyone agrees to the lottery they have the lottery the cabin\nboy loses any changes his mind. You've already decided, it's like a verbal contract, you can't go back  \non that. You've decided the decision was made you know if you know you're dying for the \n reason for at others to live, you would, you know if the someone else had died you know that you would consume them, so But then he could say I know, but I lost. I just think that that's the whole moral issue is that there was\nno consulting of the cabin boy and that that's what makes it the most horrible is that he had no idea what was even\ngoing on, that if he had known what was going on it would be a bit more understandable. Alright, good, now I want to hear so there's some who think it's morally permissible but only about twenty percent, led by Marcus, then there are some who say the real problem here is the lack of consent whether the lack of consent to a lottery to\na fair procedure or Kathleen's idea, lack of consent at the moment of death and if we add consent then more people are willing to consider the sacrifice morally justified. I want to hear now finally from those of you who think even with consent even with a lottery even with a final  murmur of consent from Parker at the very last moment it would still be wrong and why would it be wrong that's what I want to hear. well the whole time I've been leaning towards the categorical moral reasoning and I think that there's a possibility I'd be okay with the\nidea of the lottery and then loser taking into their own hands to kill themselves so there wouldn't be an act of murder but\nI still think that even that way it's coerced and also I don't\nthink that there's any remorse like in Dudley's diary we're getting our breakfast it seems as though he's just sort of like, oh, you know that whole idea of not valuing someone else's life so that makes me feel like I have to take the categorical stance. You want to throw the  \nbook at him. when he lacks remorse or a sense of having done\nanything wrong. Right. Alright, good so are there any other defenders who who say it's just categorically wrong, with or without consent, yes  \nstand up. Why? I think undoubtedly the way our society is shaped, murder\nis murder murder is murder and every way our society looks down at it in the same  \nlight and I don't think it's any different in any case. Good now let \nme ask you a question, there were three lives at stake versus one, the one, that the cabin boy, he  had no family he had no dependents, these other three had families back home\nin England they had dependents they had wives and children think back to Bentham, Bentham says we have to consider the welfare, the utility, the happiness of everybody. We have to add it all up so it's not just numbers three against one it's also all of those people at home in fact the London newspaper at the time and popular opinion sympathized with them Dudley in Stephens and the paper said if they weren't motivated by affection and concern for their loved ones at\nhome and dependents, surely they wouldn't have done this. Yeah, and how is that any different from people on the corner trying to having the same desire to feed their family,\nI don't think it's any different. I think in any case if I'm murdering you to advance my status, that's murder and I think  \nthat we should look at all of that in the same light. Instead of criminalizing certain activities and making certain things seem more\nviolent and savage when in that same case it's all the same act and mentality  that goes into the murder, a necessity\nto feed their families. Suppose there weren't three, supposed there were thirty, three hundred, one life to save three hundred or in more time, three thousand or suppose the stakes were even bigger."}, {"content": "Suppose the stakes were even bigger I think it's still the same deal. Do you think Bentham was wrong to say the right thing\nto do is to add up the collected happiness, you think he's\nwrong about that? I don't think he is wrong, but I think murder is murder in any case. Well then Bentham has to be wrong if you're right he's wrong. okay then he's wrong. Alright thank you, well done. Alright, let's step back from this discussion and notice how many objections have we heard to what they did. we heard some defenses of what they did the defense has had to do with  necessity the dire circumstance and, implicitly at least, the idea that numbers matter and not only numbers matter but the wider effects matter their families back home, their dependents Parker was an orphan, no one would miss him. so if you add up if you tried to calculate the balance of happiness and suffering you might have a case for  saying what they did was the right thing then we heard at least three different types\nof objections, we heard an objection that's said what they did was categorically wrong, right here at the end categorically wrong. Murder is murder it's always wrong even if it increases the overall happiness of society the categorical objection. But we still need to investigate why murder is categorically wrong. Is it because even cabin boys have certain fundamental rights? And if that's the reason where do those rights come from if not from\nsome idea of the larger welfare or utility or happiness? Question number one. Others said a lottery would make a difference a fair procedure, Matt said. And some people were swayed by that. That's not a categorical objection exactly it's saying everybody has to be counted as an equal even though, at the end of the day one can be sacrificed for the general welfare. That leaves us with another question to investigate, Why does agreement to certain procedure,  even a fair procedure, justify whatever result flows from the operation of that procedure? Question number two. and question number three the basic idea of consent. Kathleen got us on to this. If the cabin boy had agreed himself and not under duress as was added then it would be all right to take his life \nto save the rest. Even more people signed on to that idea but that raises a third philosophical question what is the moral work that consent does? Why does an act of consent make such a moral difference that an act that would be wrong, taking a life,\nwithout consent is morally permissible with consent? To investigate those three questions we're going to have to read some philosophers and starting next time we're going to read Bentham, and John Stuart Mill, utilitarian philosophers. Don't miss the chance to interact online with other viewers\nof Justice join the conversation, take a pop quiz, watch lectures you've missed, and a lot more. Visit  \nwww.justiceharvard.org. It's the right thing to do. Funding for the program is provided by  Additional funding provided by"}], "Naomi Oreskes: \"The Scientist as Sentinel\"": [{"content": " Hey, hi. Hi. Hey. It's really nice of you to come tonight. My name is Melissa Franklin. I'm a physicist. You can clap now. I'm really excited about this talk tonight. I just want to say that I invited Professor Eskies to give this talk before the election. So obviously I'm brilliant. And I'm so happy that she's giving this talk tonight because it's so important, so incredibly important. This topic, the scientist, is sentinel is probably the most important thing for a lot of us right now. What do we do now? How do we make a contribution? So Naomi Eskies is kind of an interesting person. I've known her for just a little while because she arrived at Harvard just a few years ago as a professor in the history of science department. And in the Earth and Planetary Sciences department. And she's very cool. But most cool is that she got her degree from the School of Mines. I love School of Mines at the Imperial College in England, the place where there is now Brexit. And she left early and came back and went to Stanford University to do her PhD. But she didn't want to just do a normal PhD. She had to make it up herself because it's not in just one department, which I also like. So I just like her a lot in many ways. But maybe possibly other than the incredible work she does right now, which is so important to us, is her work with the Pope. Sorry, I know that sounds funny. But she wrote an introduction to the encyclical on climate change and inequality with Pope Francis. I've never really worked with the Pope. But I hear he's a great guy. And I wish he'd asked me, and even though I don't know anything about anything really, about climate change. Also, she's just incredibly prolific. She wrote a book called 2014 called The Collapse of Western Civilization, which was a positive thing. And she's also given really good TED talks, if you like that kind of thing. And she's just an incredible historian, but also she knows science. And she also wrote some books that I don't even know. The words about, like, do we believe in climate change?"}, {"content": "That's really important. So sorry, I think the thing that I'm looking forward to tonight is hearing from a really sane person who's very creative. And she's going to tell us what to do. She's going to tell us what to do. So please welcome her. Well, thank you for that great introduction. One of the things I've always struggled with as a historian and philosopher of science, who studies science and thinks about what scientists should do, is in my experience, most scientists do not want historians to tell them what to do. So I'm really, really happy that we've come to this moment. It's not turned on. I thought I'd turn it on. Okay, we've come to this moment where a physicist wants me to tell her what to do. That's pretty special."}, {"content": "I also thank you for that generous introduction. When I was studying at the Royal School of Mines, which is part of Imperial College London, and people would ask me where I went to school, and I would say go to the Royal School of Mines, and they would say, the Royal School of Mines? Are you studying philosophy? So yes, the Royal School of Mines, I studied mining geology. Also, when you said I'm very interesting, my mother is the kindest, sweetest person on earth, and never, ever, ever likes to say anything bad about anyone. But if you put on an article of clothing, the she thinks is a little not so flattering, she'll say, oh, that's interesting. So my family being interesting is not necessarily a compliment, but I will take it as a compliment here tonight. What I want to talk to you tonight about is work I've actually been doing for a number of years about this question of what the role of scientists should really be in relation to talking about contested public policy issues. I developed this specific talk for AAAAS that was here in Boston last month. I had the honor to do a keynote speech talk there. So I developed this talk as a talk about science for scientists. So in this talk, I'm going to use the word we when I refer to scientists. For those of you who aren't scientists, you can just say they, but the message I think is relevant for all of us in thinking through what our role needs to be in terms of thinking about contested issues of public policy. So some of you may have, and I'm going to come out from behind this podium because I don't like podiums. I don't like to be separated from my people. So some of you may have seen this that ran in the New Yorker a few weeks ago. Those smug pilots have lost touch with regular passengers like us. Who thinks I should fly the plane? Now of course we all laugh at that because to us it's obvious that the public ignores expertise and it's peril. And as people associated with a great university like Harvard, we have an expectation that our expertise should be respected and in many cases acted on. But as we've seen of late, many of our political leaders not only don't act on our expertise, but they actually rejected. And in some cases they actively misrepresented. And that creates a very difficult situation for us. What should we do when our work, our expertise, is being rejected, dispatched, and in some cases misrepresented? Should we speak up in public? Should we be speaking up in public on tricky, sensitive topics like evolution or the safety of vaccinations or in my case climate change? And if so, how? And what exactly does it mean to say that we should respect expertise? What are the boundaries of expertise? And if we as experts embrace a public role, what does that entail? So I want to start by talking about a scientist that I've studied in some detail, a man by the name of Roger Revelle. How many of you have heard of Revelle before? Oh a lot."}, {"content": "Wow, this is a great audience."}, {"content": "And not just the Earth Science professors. So Revelle, as many of you know, is a very famous chemical oceanographer. He was also one of the directors of the Scripps Institution of Oceanography in San Diego. And he was also the mentor to Al Gore. It's because of Roger Revelle that Al Gore first learned about climate change and began to think of it as a serious issue. In 1965, Revelle was serving as part of the President's Science Advisory Committee, a group that was writing a report on the state of the environment. And this report had an appendix that was co-author with four of his prominent geoscientific colleagues, including Dave Killing. And the title of this appendix was atmospheric CO2, the invisible pollutant. And in this appendix, Revelle and his colleagues warned that in due course, not tomorrow, not next year, but in due course, increased atmospheric carbon dioxide would likely alter the climate in serious and adverse ways. They wrote, throughout his worldwide industrial civilization, men is unwittingly conducting a vast geophysical experiment. By the year 2000, the increase in atmospheric CO2 will be close to 25%. This may be sufficient to produce measurable and perhaps market changes in climate. And as we all know today now, that prediction came true. So we can say that these leading scientists of a previous generation were acting as sentinels. They were calling attention to an issue that was not yet publicly recognized, in fact not even recognized by most of their scientific colleagues at this time. And they were pointing out that while the issue of visible pollution was getting a lot of attention in the media and in halls of power, particularly smog and Los Angeles, or the terrible London fogs that had killed people in the 1950s, there were other invisible pollutants that were also important. And carbon dioxide was probably the most important of these invisible pollutants. So they reached out to political leaders, in this case to the president of the United States, about their scientific research that implied a societal threat. Now, in our research and the Revell archives, we've never found any evidence that Revell worried that this might somehow not be an appropriate thing to do. On the contrary, in the context of growing public concern about air pollution in the 1960s, Revell considered to be obviously appropriate to alert political leaders to this other form of pollution that was not as obvious but could also have serious long term effects. And we also have seen from our research that most of the political leaders to whom he reached out did seem to be at least cautiously receptive to being made aware of this issue. And in current research that I'm doing here with two of my graduate students here at Harvard in the history of science department, Hannah Conway and Colleen Linear-Kristenson, we've identified a number of political leaders in the 1960s who took an interest in this issue and who thanked Revell and his colleagues for sending these materials to them. In some cases, followed up and asked for more information."}, {"content": "So those were, it was a very different time. Now, today though, many of us are reluctant to be sentinels. Many of us are worried that if we speak up in public beyond the confines of scientific publications or scientific meetings and conferences, that this will lead to us being viewed as advocates or activists, and that we will politicize our science and will lose credibility. So, I think that if you have heard this in last few weeks, maybe some of you have been involved in conversations with your students or your colleagues here at Harvard about whether we should participate in the March for Science and whether or not that threatens to politicize science in an undesirable way. So this is a question that colleagues and I have been looking at actually for a few years. Well, before the March on Science or March for Science or whatever March on Science, no. Guess it's a March for Science Donald Trump is marching on science. Well, before the current political situation, my colleagues and I have been interested in this question of how scientists think about their role in relation to political questions. So I've been for several years now part of a project that's coming wrapping up now called assessing assessments. My colleagues on this are Dale Jamison who just talked at the law school today about other work and Michael Oppenheimer and several other postdocs and graduate students. So in our work, this was one of the things that we asked scientists about. We asked them how do you think about the relationship between science and policy and how do you think about your role in relation to that question. And what we found was a very consistent pattern where the vast majority of scientists that we asked expressed a great deal of reluctance to take on any role other than simply presenting factual information in the context of scientific assessments. So not expressing opinions on policy and not even presenting those facts in other contexts outside of the assessment framework. And when we probe this and ask why many expressed the idea that there is or needs to be a bright line between science and the one hand and policy on the other. And that speaking up in public, even about the facts threatens to dull that line. And therefore scientists said, scientists said things like, well, you should do your work, but you should not go public. Some said using imperative, you must not go public. You must not cross the line. And when we said, well, why?"}, {"content": "They said, well, because if you do, you'll lose your credibility. The IPCC, the Intergovernmental Panel on Climate Change has addressed this issue explicitly through its rubric of being policy relevant, but not policy prescriptive. IPCC leaders acknowledge that their work is not pure science. That climate science is linked to major public policy issues. And that the IPCC exists because of its link to governance through the United Nations Framework Convention on Climate Change. But nevertheless, even though they acknowledge that the IPCC itself is a kind of hybrid organization, most climate scientists working within the IPCC still say that they should refrain from taking any kind of public policy. And they say, well, they should not be taking any kind of public role and stay removed from discussion of policy implications and considerations. And one of the most common refrains that we've heard both in the context of this research and more broadly is scientists saying that we should let the facts speak for themselves. The climate science colleague, James Hanson, has also written about this issue, and he's referred to this as redisense. He said that scientists in general are reluctant to speak up about what they know about climate change, so not just about the policy things, but even to speak up in public about the facts. And again, because they believe that we need to let the facts speak for themselves. Now, those of you who saw the poster for this talk saw that one of the pictures on it is this picture of Jim Hanson getting arrested, protesting at the White House over the lack of action on climate change. Many scientists who have we have spoken to have invoked Jim Hanson as proof. Now, Hanson has become a major figure not only because he speaks up about the evidence of climate change, which he does, but also because he's become an advocate for particular solutions. So he is an advocate for a carbon price established through a fee and dividend system. And in support of this work, he has been arrested several times. But I want to ask the question tonight, what exactly is Hanson proof of other than maybe that scientists are uncomfortable with his position? And it's not just climate science. This is an issue that affects scientists in many, many areas. So people who work in re-biology and conservation, people who study lead or other toxic in the environment, endocrine disrupting chemicals, neo-nicotonoid pesticides, and their potential impacts on pollinators, genetically modified crops, vaccine safety, the harms of sugar in the American diet, deaths and injury from gun violence, all of these are issues on which scientists do scientific work collect data factual information, but they have policy implications, policy implications that may be highly contested. So scientists in many fields face this sentinel problem. And I want to argue that scientists in the past have faced this problem too. That although this moment we're living in may feel unique, it certainly feels distinctive and troubling in our lifetimes, but it isn't actually a unique moment in terms of scientists facing this question of what their roles should be. So we could think of this problem as having a kind of spectrum of choices. At one end we might say, as many scientists do say, that we should just do our science and leave it to others to communicate it, to explain why it matters, and we're needed to propose remedies. And we could call this the pure science ideal. At the other end we could think about Jim Hanson, who does speak out, does get involved, does propose remedies, and in his case does even get arrested, we could call that the activist ideal. So what I want to do tonight is to argue for a middle ground. Now as a person who works on climate change, I adamantly reject the idea that the truth is always in the middle, because it's often not, but tonight I do want to suggest that there is a kind of middle ground that we might think about occupying. And then in any case thinking about the spectrum helps us think about where we want to be on that spectrum, because at the end of the day there isn't actually a right or wrong answer to this. These are judgments, but historical experience, historical evidence can help us make those judgments. Oh, thank you very much, and help us judge where we want to be in relation to this question. So my argument tonight has three parts. The first is to talk about historical precedent. To look at an example of scientists in the past who did step up and act as sentinels, not just on the fact of the problem, but also on policy solutions, and to argue that this precedent refutes our worry or our anxiety that taking a public position on an urgent issue undermines the credibility of our science, and then to argue to conclude that I do believe we need to speak because facts don't in fact speak for themselves. So what is the historical precedent? Well, some of you may already know, maybe you've heard me talk before, but it's obviously it's the precedent of nuclear weaponry. Many climate scientists feel that we are today in an existential crisis. Scientists in 1945 knew that they were. There was no question among physicists who worked on the Manhattan Project, who worked on nuclear weapons, that the potential for an arms race meant the potential to eliminate life on Earth as we knew it. And in this context, particularly as 1945 became 1948 and the Soviet Union exploded its first hydrogen bomb, and then the, sorry, it's atomic bomb, and then the United States developed a hydrogen bomb, and then the Soviet Union developed a hydrogen bomb, and pretty soon we were in an arms race. Many leading physicists felt that they had a moral obligation to speak out against nuclear proliferation. As many of you know, they had played an essential role in creating these weapons, and these weapons now threatened human safety and potentially even human survival. And therefore they felt it was actually rather obvious that they had an obligation to raise an alarm and suggest means to control this threat. So, drawing on the framework of scientific rationality, they argued that it was rational to be alarmed and irrational to be complacent. And therefore they did have an obligation. But one of the interesting things about this discussion that takes place in the late 40s and 50s is that many of these physicists argue that their obligation is not just to speak up on the technical aspects of nuclear weaponry, to explain how they worked and how they killed people and why they were so damaging, but also to address the problem of what to do about them, about how to control them. Because to them it was eminently clear that these things did need to be controlled. There was no real plausible argument that an unrestricted arms race would be a good thing for the world. The most famous example as many of you know is Niels Bohr, who spoke out actually even before the end of World War II, and predicted the arms race that would follow if there were not some kind of international dream in between the US and the Soviet Union. And also Albert Einstein, who in later years famously became an advocate for international peace and arms control. But it wasn't just Bohr and Einstein. Sometimes people have said to me, well Bohr could afford it, he was so famous or Einstein could afford to speak out. But it wasn't just these incredibly conspicuously famous figures. Actually a wide range of physicists, Hans Betta, Leo Zalard, James Frong, Philip Morrison, who taught for many years in MIT, and many, many others. There are dozens of physicists that we could name who became involved in a variety of different ways, large and small, to help alert the world to the threat of nuclear weapons and engage people in a conversation about arms control and disarmament. These men spoke to the moral imperative to control nuclear weapons, and they became advocates. The core of their argument is something that I've labeled epistemic proximity. And I think it's an important argument to understand because it's not just about a kind of moral obligation that arose from having built the weapons, but it also has specifically to do with their role as they understand it as experts. So the argument goes like this. not just that they helped build these weapons, but it's that as physicists, they had a uniquely vivid appreciation and understanding of the damage that these weapons could wreak. That they could explain things that other people really didn't and maybe even couldn't fully understand about what an arms race and a large-scale nuclear exchange would really mean. So people had seen the destruction of Hiroshima and Nagasaki. John Hersey had written about it very beautifully in his book Hiroshima, but many people said, well, yes, it's true, lots of people died, but how is that worse or different than the fire bombing in Dresden or the fire bombing of Tokyo? So for many people, it wasn't actually clear why these weapons were substantially worse. Now, this seems hard to conceptualize today and previously the slide said today, all the world leaders understand what a major nuclear explain would do. Okay, well, when you do contemporary history, you have to stand your toes. But clearly, in the late 40s and 50s, that was not the case. Some of you may know that President Harry Truman when confronted with the announcement of the Soviet atomic bomb and when he was asked, well, does this mean there will be an arms race, he said yes, and we will win it. Douglas MacArthur during the Korean conflict requested permission to use nuclear weapons. So it's not the case that people understood that using nuclear weapons would necessarily lead to some kind of terrible conflagration. So these scientists felt that the need to understand fully and viscerally what a large-scale nuclear exchange would mean that this need was severe and that they had to step up to that challenge and many of them did so in both public and private ways. So I think a comparable argument can be made about climate change today. I've given something like 300 public talks on climate change and I've spoken, I'm going to South Dakota tomorrow, so I now will have been to 47 or 50 states and I've talked in Idaho and North Dakota and Texas and Oklahoma and California. And one of the things that I feel I've learned is that many people, including many people who do not deny climate change or do not disparage climate science, still don't really quite get why this issue really matters. They don't quite understand just how serious uncontrolled climate change will be. One of my colleagues, quite famous and distinguished political scientists, said to me once over lunch, so tell me why I should be worried about polar bears. And this is part of why Eric Conway and I wrote the collapse of Western civilization because we were trying to think about, what could we do, what kind of story could we tell that would convey to people why climate change really matters and why this is not just a question of polar bears as much as I personally do love and worry about polar bears. Okay, so part two, I want to argue that this historical press and offers evidence that refutes our worry that taking a public position on an urgent issue will undermine our scientific credibility. And this argument is actually extremely short. The evidence simply doesn't support this hypothesis. If we think about Albert Einstein, I know of no evidence and maybe someone can come up with it but I know of no evidence that the theory of relativity or his work on the photoelectric effect ever lost credibility because of his advocacy of arms control. Now, there were anti-Semites in Germany who didn't like Einstein, but that was not because of his public advocacy. That was because of a whole set of other complicated issues. Hans Betta, as many of you know, won the Nobel Prize for his work on nuclear fusion fusion. No one ever suggested revoking that prize or that somehow that work was doubted or dubious because he was an arms control advocate. And Bohr is such an interesting character. Some of you know, many of Bohr's colleagues were dubious about the Copenhagen interpretation of quantum mechanics. Bohr wrote a lot of philosophical essays that lots of his colleagues thought were completely off the rails. But it wasn't, but not his arms control advocacy. Millions of schoolchildren learned about the Bohr model of the atom. If you took high school physics or college physics, you learned about the liquid drop model of fish and you learned about his crucial scientific contributions. And no one ever said that those contributions were undermined because he advocated arms control. Now what about Robert Oppenheimer? Some people say to me, well, what about Oppenheimer? Oppenheimer is a famous case of a scientist who got discredited. But what exactly happened to Oppenheimer? He lost his security clearance because of his opposition to the hydrogen bomb. And also because Edward Teller held a personal grudge against him. And he saw this as an opportunity to undermine him. So Oppenheimer lost political standing inside the US government. He lost his clearance. He lost his capacity to give advice to political leaders on certain delicate issues. But losing political standing with politicians is a very different thing than losing scientific credibility. And in fact, Oppenheimer became a hero in many circles. I mean, many of you have heard of Oppenheimer because of this, more than because of his science. So among many people, he gained credibility. So when we talk about credibility, we actually have to be more precise. We have to ask, are we talking about credibility with political leaders? Are we talking about credibility with the public? Are we talking about credibility with our own colleagues? Or what?"}, {"content": "These are different things. There's no evidence that Oppenheimer lost scientific credibility with any of his colleagues because of this work. So I want to argue that the fear of losing scientific credibility is exactly that. It's a fear. It's an anxiety. And there's actually very little evidence to support the worry. And you can call me naive, but I do think we should be making our decisions based on evidence. Even though I know that's a very controversial claim to make in the stage. And some of you may have seen that there's actually a study that just came out a couple weeks ago."}, {"content": "And I apologize I haven't had a time to make a slide of it. But a recent study that actually interviewed people of the public and posed to them some scenarios where scientists spoke out in public, made policy recommendations, actually showed that there's very little evidence to suggest that this causes you to lose credibility with the public either. So it's actually an interesting question. And it will be a question for another talk about, actually, why we are so worried about this issue when there's actually so little evidence to support it. But I haven't done that work to answer that question yet. OK, so part three, facts don't speak for themselves. Well, why do I say that? Why don't facts speak for themselves? Well, the simple answer. And when I started saying this 7, 8, 9, 10 years ago, this was considered controversial. Now it seems obvious. So that's good. I can make the talk shorter. We live in a world where many people are trying to silence facts. And as you know, as many of us now understand, these arguments are not actually about the facts. They're not just about the facts. In most cases, they're not even actually about the facts at all. Many of the arguments that are being used to undermine climate science, to be able to undermine evolution, to undermine vaccine safety. These are arguments about political beliefs, about economic interests, and about values. You cannot refute a claim about values with facts, or claim about facts with values. That's what philosophers would call a category mistake. We're conflating two different domains. If people are rejecting climate science because of values, then we have to think about what those values are, and why those values are perceived to be in conflict with our facts. And this was essentially the conclusion of the work I did with my colleague Eric Conway when we wrote the book Merchants of Doub."}, {"content": "When we started this project, we were frankly mystified. We had come across a story of some very famous scientists that we actually knew, one of whom had been the director of the script's institution of oceanography, where I was working at the time. And these prominent distinguished famous scientists had become climate change deniers. And there was no possibility that this was scientific illiteracy, because these were some of the most brilliant and successful scientists of the 20th century. It included Frederick Sites, who was the president of the US National Academy of Sciences. So the question we had was, why would these people reject the scientific work of their own colleagues? How do you make sense of that? And what we found overwhelmingly, not just are these men that we studied in our book, but in general that the rejection of climate scientific facts was not about scientific illiteracy or lack of scientific understanding. But actually, it was driven by an anxiety, by fear, by a worry, the worry that addressing climate change would lead to bigger government, higher taxes, and loss of personal freedom. And those concerns and the solutions that people were proposing for climate science were seen as clustering with conservative values. And that's why we've seen this enormous politicization of the issue because of conservatives believing that the implications of climate science threaten their values. It's this clash of values, then, that explains the polarization, and also some of the otherwise strange alliances that we see on this issue. The perception that addressing climate change, sorry, that addressing climate change will require big government or higher taxes, explains a good deal of the political polarization around the issue because opponents of big government are ideologically motivated to reject climate scientific events. And they've made common cause, then, with industries that reject the science because it threatens their profit. So the obvious part of this story, the economic mode of this economic self-interest aligns with a more subtle and complicated story about people seeing climate science as clashing with and threatening their values. So just to sum this up, I want to show you a short clip from the film that we were made of our movie. And this is just kind of merchants of doubt. And in that shell, it shows you who these people are and the sorts of arguments that they make to try to undermine scientific findings. Bill O'Keefe is Executive Vice President of the American Petroleum Institute. He's also a board member of the Global Climate Coalition, made up of oil and electric companies, automakers, and others. We believe it was the war on the oil. They had an off-hoyle agenda. Climate change was part of that. I think that it's unfortunate that the science is so distorted and mistated. And without it... The science is complicated. There are lots of different factors. So you really have to understand the whole picture. There is a natural variability that has nothing to do with me. Climate is changing naturally. Has to do with sunspots and has to do with the wobble of the earth. And so it's not too difficult to persuade some of the public that we really don't know for sure. So maybe that's way to wild. We need to have more proof. We need more data. The science isn't there to make that determination. There is no need for us to rush to this kind of judgment to respond. Others put out ads saying more pollution is going to be good for us. A doubling of the CO2 content to the atmosphere will produce a tremendous greening of planet Earth. CO2 is a benefit to plant life. It's increasing the bounty and the productivity of the planet, our ability to feed populations in this world. What you're seeing here from the coal industry is perfectly analogous to what the tobacco industry is to do. They refuse to change, refuse to shift. And they're trying to convince us that it's actually good for us, the way they used to say, luckies make you healthy. OK, so in that clip, you see sort of the basic, you see the merchants have doubt doing their thing. But in that clip, you're seeing people, mostly who represent the fossil fuel industry. So Lee Raymond from Exxon Mobil, Don Blankenship from Massey, Energy, Bill O'Kee from the American Petroleum Institute. But again, as I already mentioned, part of what we were trying to do in our work was to understand why would scientists, why would other people who don't have stock and exome mobile, why would they make common cause with this sort of work? And again, as I've already suggested, it's this confluence of money and ideology that explains the pattern that we saw. And this also helps to explain why climate change now is so much more prevalent in the United States than elsewhere. Because of our powerful commitment to individualism in the United States, and the historic American skepticism about the federal government going back to the founding of the Republican skepticism about centralized decision making. And if you think about it, think about the articles of confederation, think about the separation of powers. This country was rooted in an idea that investing too much power in centralized government would undermine the individual freedoms of the states and the people in them. We have a deeply rooted belief in the United States that the government that governs best governs least. And that's a belief that informs a huge amount of climate change denial. So climate skeptics, contrairions, deniers, whatever you want to call them, they play on these cultural norms, insisting that addressing climate change will lead to an expansion of the government and a constriction of our freedoms. And you've seen this kind of ideology on display in the last few weeks. And this resonates very strongly with many ordinary Americans, particularly in what we've come to call the red states. And therefore, people resist accepting the findings of climate science, and they're open to the suggestion that our findings are exaggerated or even perhaps a hoax. So I want to just give one example of how this works in sort of one scientist that we study that I think is a particularly illustrative example of how this ideological piece comes together with rejecting the findings of science. So one of the people that we studied was a physicist named Fred Singer, who some of you may be are familiar with. Singer was not a climate scientist. He was a physicist. In fact, he was the proverbial rocket scientist. He was the first director of the US whether satellite service and was involved in the early years of the US rocketry and space programs. But in addition to his scientific work, he was also involved in campaigns to challenge the scientific evidence of acid rain, of the ozone hole, of climate change, and also of the harms of tobacco. Fred Singer, in the early 1990s, worked with the Philip Morris tobacco company to attack the US Environmental Protection Agency over the issue of secondhand smoke. Now some of you know that we have known since the 1980s that secondhand smoke can cause cancer. The same chemicals that are in primary smoke also occur in the exiled smoke or in smoke that comes from a cigarette that's just burning. And if you breathe that smoke, you also can be subject to cancer and many of the other diseases that affect smoking, smokeers. This was first stated unequivocally in 1986 in a report of the US surgeon general in which the surgeon general declared that involuntary smoking, that was the term he used, is a cause of disease including lung cancer and healthy non-smokers, especially children. And based on this, as well as an independent review of over 6,000 peer-reviewed scientific papers, in the early 1990s, the EPA declared secondhand smoke to be a class A or proven carcinogen. In 1994, working with the tobacco industry, Fred Singer wrote a report attacking these scientific findings. And one of the reasons, this is a copy of out of the tobacco legacy documents of the cover page for this report. And I made this slide because Fred Singer is still alive and he accused me of being a liar. He has said in public that he never worked with the tobacco industry. So, yeah, there it is. Okay, so I love documents."}, {"content": "I love my job. So, Singer had been working as a consultant to Philip Morris and he was hooked up, I'm not sure exactly how, but he became, he started working with a lawyer named Kent Jeffries, a lawyer who was affiliated with the Kato Institute and the competitive enterprise institute. And some of you know these are things that promote free market solutions to social policy problems. The report was actually published by the Alexis de Toekeville Institute but funded by the tobacco institute, which was the so-called research arm of the tobacco industry. So, this report illustrates in a nutshell how this works. There's a kind of shell game where the tobacco industry would give money to the tobacco institute claiming that that was for research. Tobacco Institute would hire lawyers out of thing tanks like the Kato Institute. They would then work with a scientist who would give the thing scientific credibility and produce a report that would be issued by yet another thing tank. In this case, the Alexis de Toekeville Institute whose name would conjure up notions of freedom and democracy in America. Now, the interest of the tobacco industry in attacking the EPA is obvious. And I think the interest of these thing tanks in opposing regulation is nearly as obvious. But why would a physicist attack the scientific evidence of the harms of tobacco? Well, I would assume that singer was paid for this work but that's not the key part of the story in my view. The key part, I think, comes from his own words. So, in the introduction to this report, he explains why it's important to push back against the EPA. And he says, quote, if we do not carefully delineate the government's role in regulating dangers, there's essentially no limit to how much government can ultimately control our lives. And in our book and in our films, we document many, many examples of this argument. If we allow the government to regulate X, then soon it will also regulate Y and Z and we will lose our freedom. Acid rain today, the Bill of Rights tomorrow. This argument comes directly from the work of the Chicago economist Milton Friedman. And it's an argument that I call the capitalism and freedom argument. And indeed, we've seen this argument being resurrected in the last few months. It was used by Ronald Reagan in the 1990s to justify lower taxation, less government and to foster a marketplace deregulation. And that set of arguments has really been guiding conservative thinking in the United States since the Reagan administration. The two key texts are Milton Friedman's book by that name, capitalism and freedom, which was inspired and turned by the work of the Austrian neoliberal economist Friedrich von Haack. So if you're interested in this problem, these are the two books that you need to read to understand the intellectual framework that is guiding this whole argument. But if you go back to von Haack, his basic argument is he's making this argument towards the end of World War II, he's saying if we allow Western democracies to have national health insurance or other forms of intervention in the marketplace, we're going to be on the road to surf dumb. And it's only a matter of time."}, {"content": "It's a kind of inevitable creeping of government controlled into different aspects of life. So this is the common thread of science denial. And in fact, the common thread between climate change, acid rain, tobacco, and other things that otherwise might seem to be totally different issues, that scientists discover a problem, typically inadvertently, I mean, the scientists who were working on acid rain didn't set out to discover acid rain. They were studying forest ecology and watershed hydrology. But scientists discover a problem. And the solution to this problem appears to require some sort of government action, whether it's putting a price on carbon or banning CFCs or limiting sulfur emissions from coal-fired power plants. And so people who don't want that government action, either for economic reasons or philosophical ones, question the science and attack the scientists. Sociologists call this implacatory denial. You deny something because you don't like its implications. And it's a common pattern in human life. It's not restricted to climate change denial. But if you deny the evidence that your partner is having an affair, maybe that's OK. Life goes on. If you deny the evidence that the climate is changing, the consequences are pretty serious. So science has been politicized to bring this now back to our question. Science has been politicized as a means to undermine it by groups and individuals who do not like what they interpret to be the political implications of our scientific findings. So here's the crucial point, though. When scientists were attacked, when the scientists who worked on tobacco or acid rain or the ozone hole were attacked, it wasn't because they had crossed the line into policy. Most of the scientists who were attacked in these stories that we wrote about had actually not played any role in public policy at all. They had done what most of us think we're supposed to do. Do important work, publish and period your journals. Talk about. out of the meetings, but their scientific research had revealed or affirmed serious problems like deaths from tobacco use or the threat to life on Earth from ozone depletion. So these scientists did not become targets because they had spoken out. They became targets because of the importance and significance of their work. So I think that many of us in the scientific community have actually misinterpreted cause and effect. The causal arrow is the reverse of what most of us think. Now it is true that today there are some climate scientists like Mike Mann and like Jim Hanson who have spoken out, who have become public figures, but even these men, even Mike Mann and Jim Hanson, both of whom I know, they became public figures after they were attacked. They were scientists doing science. They got attacked and being attacked made them decide that they needed to stand up and be counted. So it may help to understand this also to know that there is actually a very long history in the United States of claiming that government intervention in the marketplace threatens our freedom. And this is from some new work I am doing now trying to understand the deeper roots of this argument, which it turns out go back to at least the 1920s. And one of the ironies of this is that these arguments have actually often been used to protect products, products, profits. And in some cases actually to restrict competition even when people are claiming to support a free market society. So I just thought this is such a very interesting advertisement that I found. And I thought I would ask you, what do you think this might be an ad for? And my postdoc is not allowed to answer, but anybody else? Yeah, Barbara wire, that is the usual answer. Sometimes people say hats. The answer is privately generated electricity. This was part of an ad campaign run in the early 1960s by a group of private sector electrical utilities who did not want the federal government to generate electricity at the Hoover Dam or other places in the United States. Now we can argue about the benefits of government electricity, but the point is to see that the argument is not being made, it's not about the merits of different ways of generating electricity. The argument is this fear mongering campaign that if we allow the government to generate electricity, pretty soon we'll be living behind barbed wire. So what does all of this tell us about facts and values? Well, we see that in all these stories, the facts and values are conflated and complicated in difficult ways that are difficult to sort out. But the key insight I think is to understand that these arguments are not about the scientific facts and because they're not about facts, they can't be refuted with facts. But they can be addressed with political or historical evidence or with other arguments about values. So for example, one thing we can do is to challenge the assertion that addressing climate change necessarily requires bigger government or higher taxes. And a good example of this is the history of acid rain. So some people in the audience here I know remember that acid rain, which was a very serious problem, was addressed in the 1990s through amendments to the Clean Air Act that created emissions trading, a market-based mechanism that was supported by both Democrats and Republicans and signed into law by Republican President George H.W. Bush. This law did not lead to the expansion of the federal government, it did not lead to higher taxes, nor did it lead to a loss of liberty of people living in the Midwest where acid rain was a problem. And in fact, the price of electricity in the American Midwest fell. The second thing I think that's important for us to think about is that I think we shouldn't be afraid to address values. For two reasons, first of all, because these are values, questions we have to address the values involved. If we just keep throwing more facts on people, it won't address the worry, the concern that they have. So if our fellow citizens in South Dakota, Ohio, or Oklahoma are worried about bigger government or worried about values, then we have to be able to talk about that. And I think that preventing disruptive climate change actually lines up with the fundamental values that many of our fellow Americans share. In fact, I think it lines up with more values that many of our fellow Americans share than simply the value of constraining big government. So think about it. The value of fairness, which includes protecting innocent people from getting hurt. We controlled secondhand smoke in part because there was so much overwhelming evidence that secondhand smoke hurt children. And that was one of the very powerful arguments that was made for the right of the EPA to regulate secondhand smoke in order to prevent hurt or harm to innocent people like non-smokers and children who were not choosing to smoke. Or the value of accountability that the people who made a problem have an obligation to address it. Or the value of being realistic, of accepting that market failures are reality, and sometimes there is the need for the government to nudge the market in the right direction. We saw that during the housing crisis we've seen it in the financial crisis. People know that markets don't always work the way we want them to. Or the value of technological leadership and hard work of rolling up our sleeves and getting the job done, something that Americans have always done and prided themselves in doing. And of course, most importantly is the argument that there are values that the market does not protect like the basic inherent dignity of all people. And this is the central argument that Pope Francis makes in the encyclical on climate change and inequality, that there is an inherent dignity to all humans, and that that is something that we all have a right to defend and protect. Freedom is important, but so are many other things. And in the long run, climate change deniers are not actually protecting our freedom. In fact, they're threatening it. And this I think is one of the most powerful arguments we can make. That while the climate change deniers say freedom, freedom, freedom, which they do, in reality, the shoe is on that other foot. So I want to just show you one other clip from the film where I'm speaking and addressing this issue. As sea level rises and hurricanes become more intense, people get killed. Their houses and communities get destroyed. But think about heat waves and droughts that ruin agricultural communities. All of these are problems that it will require government intervention to address. The great irony of the story to me is that people who don't like big government are going to get more of it. And we're going to see more money being spent on dealing with the aftermath of these disasters. There will be billions of dollars in real estate losses, but more than that, people die. That's why it matters. That's why this is meaningful for us and not just for polar bears or people in Bangladesh. That's why so many people in the scientific community now are really starting to talk in very worried tones. Because there's, I think, a growing sense in scientific community that we're running out of time to prevent a train ride. So obviously my argument is that we do need to speak up because we need to counter the disinformation, the misinformation, and the false value arguments that are being made by other people. But I want to end with one more thought which gets back to this idea of epistemic proximity. So if we go back to Roger Revelle in the 1960s, Revelle spoke up because he knew about something, the possibility of disruptive climate change that few other people were aware of. And like Born Einstein and Beta before them, Revelle and killing and their colleagues, we're speaking from their proximate expertise. Meaning about something that they understood by virtue of their expertise as scientists that other people did not understand. But they also respected the expertise of others to propose and formulate the solutions. And Revelle worked on climate change his whole life. He studied it from many different angles. But he always was very cautious about any response to a question about what the right policy solution was. And I think this points to an important distinction. And it's not the fact value distinction as people have normally understood it, but it is related to it. It's about the limits of expertise. It's about who really knows how to fly that plane. So there are many things which is natural scientists or social scientists depending on what our expertise is. There are many things which we're not experts about and not particularly qualified to speak about. So in the case of climate change, if you're a climate scientist, if you do work on the physical science of climate change, you're not particularly knowledgeable in most cases about the social and economic aspects of impacts or the details of policy. Earth scientists cannot say, oh sorry, Earth scientists can say that if we're to prevent dangerous anthropogenic interference in the climate system, which is what the UN Framework Convention commits us to, then we must do something to control greenhouse gases because greenhouse gases and deforestation are the causes of this problem. Just as CFCs were the causes of ozone depletion and second-hand smoke was the cause of lung cancer and otherwise healthy non-smokers. And that means preventing the continued dumping of CO2 into the atmosphere. That is a conclusion that falls directly from our science. We could call it a direct deductive consequence. And therefore I want to argue we should not hesitate to say things that fall out directly from our scientific knowledge and understanding. Indeed, I would argue that it is our responsibility to do so because we are the people who have that epistemic proximity, who understand the problem best. And in my model for this, then I said in the beginning I was going to put forward an idea of the responsible scientist and I think we have a model of someone who played that role in the United States and that's Sherry Rowling, who won the Nobel Prize in the mid-1990s for his work accurately predicting that chlorinated fluorocarbons would deplete stratospheric ozone. In the 1980s Rowling and his colleagues realized that unless we control CFCs, then destruction of stratospheric ozone would threaten the future of life on Earth. And because of that he began to speak publicly and like his nuclear physics colleagues beforehand he became an advocate for action to control these chemicals that were the cause of ozone depletion. So for him policy was a deductive consequence. Rowling felt that he couldn't do this because the need to control CFCs was a direct consequence of the scientific work. So we can think of the argument as going like this, CFCs destroyed ozone. stratospheric ozone protects life on Earth. So if we want to protect life on Earth, if we want life to continue, we must somehow control CFCs. Now, this didn't involve an implicit value premise, but that value premise was the value of life on Earth. And really, who was going to challenge that? I mean, that's a debate that I'd be happy to take on with any climate change denier. But here's the important point. His advocacy didn't undermine his scientific credibility. He did his key work in the 70s and early 80s and he became an advocate for action in the middle to later 80s and played a role actually in the development of the Montrel Protocol to control the substances that deplete stratospheric ozone. But in 1995, he was awarded the Nobel Prize in chemistry for this work along with Paul Krutzen and Mario Molina. I think that if anything, there's an argument to say that his advocacy actually cemented his scientific legacy that we know more about Sherry Rowling's scientific work today in part because of what he did as a public, as a responsible scientist. So is there a comparable position for climate scientists? Yes, absolutely. Greenhouse gases are causing climate change. Climate change is dangerous. As I say in the film, it threatens people's lives, it threatens our homes, our well-being, our prosperity, and it also threatens polar bears and many other species, coral reefs and others. So if we want to protect humans and other species from the damages of disruptive climate change, then we must dramatically reduce and eventually phase out greenhouse gas emissions. And this is a deductive consequence that follows logically from our scientific work. But, however, as natural scientists, we don't have the expertise to answer questions such as, which is better a carbon tax or emissions trading system. If we go for an emissions trading system, should it be revenue neutral? Sorry, if we go for carbon tax, should it be revenue neutral? And if so, should we do it through fee and dividend or through cutting tariffs and taxes elsewhere? And how useful are fee and tariffs in stimulating renewable energy production? Should we focus on grid integration or energy storage? And what about nuclear power? These are all matters of social science, law, policy, and politics."}, {"content": "It doesn't mean we can't have an opinion on these questions, of course, as citizens we can. And Jim Hansen has a right to be an advocate for the fee and dividend system if he wants to as a citizen. But I do think it's important for us to be clear about the limits of our own expertise because let's face it, we aren't the people, unless you have a pilot's license, we aren't the people to fly that plane. So I want to argue that we should be reticent about areas outside our expertise. We should respect the expertise of others who are expert in those domains, whether they're economists, psychologists, lawyers, or other natural scientists. And if we want to address these topics because we've concluded that they're essential to the solution, then we should forge collaborations with colleagues in those domains. Put another way, if we expect people to respect our expertise, then we also have to respect theirs. That seems like common sense. But we should not be reticent about talking about the things we know and understand, the things we know to be true. And we should not be reticent about calling out others who say things that we know to be untrue. Experience of the past several decades has shown that when it comes to facts about the natural world, there are many people prepared to speak against the facts for many diverse reasons. And therefore, someone has to speak for the facts. And that's someone, I think, is us."}, {"content": "Thank you very much. So, who's going to the science march? That was wonderful. If there are any questions, please ask them."}, {"content": "Yes. Just call on people. Yeah."}, {"content": "Oh. Oh, here we go."}, {"content": "Thank you. Is this on? Yes. Oh, boy. It's very on. So I'm a great admirer of your work. And I found your chain of logic persuasive this evening, but loses me at one point. You suggest, but never quite say, that if more climate scientists spoke out about climate change, it would make a difference. So to put it in question form, do you believe that in spite of evidence that Peter Froomehoff speaks out James McCarthy writes letters to the globe last week, Somerville, et cetera, et cetera. Any climate scientists are speaking out. It's not clear to me what difference that's making. Yeah, that's a fair question. Of course, with anything evolving, social change, these are very hard things to judge, what makes a difference, what moves the needle. But I think that if you think about it, I mean, you named four or five people and I could probably name four or five more. But there are something like 10,000 climate scientists in America. And when we think about the people who have spoken out, the same dozen names comes to mind all the time. And I've served on committees that give out prizes for climate science communication. And I can tell you that the lists of people that get nominated are distressingly small. So I think that actually the numbers of people who have really been involved making the effort are small compared to the pool of people who are potentially available to do the work. And I think it's very important to have a larger pool for a couple of reasons. One is that we know we do have good evidence from social science research about the so-called trusted messenger problem. People respond to messages in part based on who's telling it. And that makes it really important that it isn't just a couple of people. It isn't just Jim Hansen and Richard Somerville and Peter Fremhoff, much as I love all those people. No, I don't love Jim Hansen. No, just kidding. Not just I honor and respect all three of those men. We need more different people. And everyone talks about Catherine Haleho now because she's this famous evangelical Christian scientist. And it's absolutely fabulous what Catherine is doing. But I mean, she is so incredibly lonely. And she's not the only climate scientist in America who is a religious believer. So why are we always talking about Catherine Hale? I mean, where are the other Christian scientists, not Christian scientists, but scientists who are Christians? I mean, where are they, right? And we know we do have evidence from the evolution debate that it does make a difference. When students hear about, and the person doesn't even have to come to class, there was a great talk at Trippoliest just last month or whatever we were there. I don't know."}, {"content": "Time has stood still since Donald Trump got elected. But there was this great talk about an experiment that faculty at ASU have done where in class, in a biology class, they talk about scientists, evolutionary biologists who are also Christians like Ken Miller, Brown. And it doesn't, you don't need Ken to come to the class. Just talking about it, just having the students read something that he wrote about how he personally recognizes his religious faith with his scientific work. That makes a difference in how students receive the scientific information that they're getting. So we have evidence to say that it does make a difference. And so, and again, though, it's not just about writing a letter to the editor. I think the evidence also tells us that, no, I don't think writing more letters to the Boston Globe is the key thing right now. Oh, forgive me, but that is how I feel. But I think getting out, talking to people in communities, explaining to people how you became a climate scientist, what you've learned, and why it matters. Why it matters to people in the places they live. I think we do have evidence that that can make a difference. We also know that most people, when they vote, you know, they're not voting about, people didn't vote for Donald Trump because he said climate change was a hoax, right? And part of the problem is that people don't see climate change as a problem that affects them and their lives. So we need to do more to talk about what the relationship is between climate change and jobs and storms that do affect you or that affect your crops. And that's a message that I think has not been articulated nearly as fully as it needs to be. Yeah."}, {"content": "Oh, sorry, well, that's right. Okay, so it's alternate sides. I don't know who I'm talking to. Oh, hi. Yeah. I wonder if you could clarify something and just didn't focus quite for me."}, {"content": "And the most thing the last part of you talk. You said you distinguish science from the values and you said the people who are the deniers are really motivated by values. They associate their position with freedom and so on. Now are you saying that the scientists should get in a debate with them about values? You quickly outlined a few values like fairness and accountability and so on. But the scientists aren't expert about values necessarily. And besides which the deniers, it's not like they're... are leading with their values, they're denying the science. And that's something the scientists do know. So how much do you want the scientists to be talking about values and why? Yeah."}, {"content": "OK, that's a great question."}, {"content": "I think you're right, because this is tricky. So the point is, though, when climate change denires attack the science, and scientists then try to answer it with science, it doesn't work. Because first of all, now you're in a debate about the science. And that just, one of the claims of climate change denial is that the science is unsettling. There's a big debate. So if you participate in a debate, now they have won, because you've demonstrated that there's a debate. And we learned this again in the evolution situation, too. If an evolutionary biologist debates a creationist, the evolutionary biologist always loses. Because what the audience hears is, oh, there's a big debate. We don't really know. So the debate framework, when you're debating science, just doesn't work. So then the question is, well, what are some of the alternatives? And so what I'm trying to get at, and it is a little subtle, so maybe I didn't explain clearly, is to expose, to say, look, I understand that there's a value premise here, right? And I understand that you don't want to see the expansion of big government. So let's talk about how we could solve this with small government solutions. So it's a way of shifting the argument. And then I think a scientist can talk about values if they are your own values. Because we all have values. You don't have to be an expert on values to talk about your values. So one of the things I do sometimes is I do talk about accountability. And I say, look, most of us believe in accountability. So let's talk about who made this problem, and who's got the responsibility to fix it. Because one of the strategies that we've also seen being used is the idea that this is the fault of China or India, right? And so it's a way of saying, well, let's talk about the US contribution to climate change. We made this problem, but we also can fix it."}, {"content": "And we can fix it through technologically innovation. And then we can move the conversation into a discussion about grid integration and energy storage. And that moves the conversation where I want it. Because I don't want to be debating whether or not climate change is happening. I want to be debating how do we get better grid integration and energy storage. And then I've got the conversation where I want it to be. And then I can talk to my audience about all this cool stuff that's going on in the technological domain and all the jobs that have been created in green energy in this country, which it turns out almost nobody knows about. Here's a good question."}, {"content": "Again, my postdoc is not allowed to answer. But how many jobs do you think there are in coal in the United States? OK, most estimates say between 20 and 30,000. So this is a highly educated audience. And even you think it's much more than it is. Most people will tell you a million or 500,000, right? There are actually incredibly few jobs in coal mining in this country, despite all the fusing and rhetoric that we've been hearing about the one coal, right? How many jobs do we have now today, not in the future, but today in renewable energy? Anybody care to guess? 500,000? Do I? OK, this is a ridiculously educated audience."}, {"content": "This doesn't work as well. The correct answer, though, even in this wildly educated Harvard audience, 3.5 million. Yeah, who knows that?"}, {"content": "So that's the point. We want to be take, if people are worried about jobs, let's talk about jobs, because actually we have really good evidence on that job front. So does that kind of clarify it a bit?"}, {"content": "Good."}, {"content": "Thank you. Hi."}, {"content": "And I need to add something over here. I don't want this to get into a debate about religion, but I do have a point to make a question to ask. I'm actually from Texas. Don't worry, I'm on your side. And I'm a science advocate among my people. I just mentioned that. My family is chocked full of very religious people. My dad's a Southern Baptist minister. Most of the people in my family are in the ministry. And when I talk to them about this stuff, they don't deny the science. They deny that we'll be here long enough for it to matter. They keep telling me that don't worry, Jesus is coming. And so I wonder where the argument is, what the point we can make is, and if there's hope for even any progress with these kind of people. Yeah."}, {"content": "I don't think there's anything you can do, honestly. No, really."}, {"content": "I mean, I'm teaching science and religion in America, and one of the things that's so interesting about religious belief in America is how incredibly diverse it is. But if people are Millenarians who believe that Jesus is coming, that's all going in pretty soon anyway, I'm not really sure there's much of an argument you can make that would change those people's minds. So it might be better to just move on and work on, you know, I mean, Texas already has a feed and tariff for wind power. But to think about, you know, things you could do to support renewable energy that people might support for other reasons, like it's economically sensible, or some other issue that they might like. I don't know."}, {"content": "I'm sorry, what? I'm sorry. I know you're going to be a little bit more confused. Sorry. What was that?"}, {"content": "What? Do you think that the problem with religious belief is that we care to care to care to be back? We think that. Right. But if you think it's all about to end, right? I mean, if you really, and especially, no, but I mean, this does raise an important issue. I mean, one of the challenges that scientists often face is that things that for us are clearly evidence to support our theories are not necessarily evidence of that for other people. So if you have an eschatological philosophy, and you do believe that the world is about to end, then the intensification of hurricanes, you know, heating of the ocean, depth of coral reefs, things that for us are clearly evidence of climate disruption, for those people could be evidence that we're really getting close. So you can't really win that argument, right?"}, {"content": "And so you might just decide, you know, you need to move on, right? But the fact is, let's face it. I mean, political change doesn't come because everyone agrees on everything. Political change comes because enough people agree on enough things. And so I think that's where again, there are people who are evangelical Christians who might disagree with me on a whole heck of a lot of things, but still might be willing to support solar power because they could see the way in which it might empower their communities, right? Or they might see the way it could bring jobs to their communities. So yeah, yeah."}, {"content": "Oh, sorry, you're next. Okay, go ahead."}, {"content": "Hi. About a month ago, I went to Denver for three days of training with Al Gore's group, the Climate Reality Project. And as part of this free training, I've got an obligation to participate or be involved or organized 10 events during the year. Oh, gosh. As I was thinking about how to meet my obligations, one thing that occurred to me was to turn to my work. I work for a national company in healthcare. It's a billion dollar company. So I sent an email to the owner of the company and he said, oh, sure, let's talk about it. I had a conversation with him yesterday in which he said, well, you know, this is a really good thing, but I concerned about the political angle. Some of our customers and employees might not agree with the message. So as we talked over about half an hour, I was able to pull him around to thinking about this. This is a green initiative. We're doing something good. It's going to benefit people. So we take it out of the political angle. Well, today, Michael Mann gets into it with a Lamar Smith and with Judith Curry at the House Science Committee meetings and it's going to be in New York Times tomorrow and my boss is going to read that. So we've got this political environment going on that's making it difficult for people to take positions because they're afraid of the polarization. You're out there speaking. Catherine Heyho is. Michael Mann is. But where are the rest of the scientists? Well, I mean, that was the question we had earlier, right? And that is part of my argument is that I think that more people and more diverse people need to be involved in this issue. And the more different kinds of people are out there talking, the less this would look like, it would just be a particular political point of view. I mean, the hearing today was an interesting when I got approached by the committee last week. And this is theater, right? So we have to borrow from our colleagues in the theater department and think about how do you respond to a theatrical performance? And so when I'd recommend it, it was too late because they'd already invited Mike to come. And that's okay, Mike's great. But I said don't invite a scientist to respond, right? This is a charade, you know, I mean, it's ridiculous."}, {"content": "You can find three people say almost anything anywhere if you look hard enough. I think what you should do is just fill the chair with all the reports, you know, all the government reports, all the IPCC reports, just stack them and have a giant pile and say that's our response, right? You know, so, you know, we could begin to think creatively."}, {"content": "We don't have to just be pulled in all the time to these, you know, frameworks that we know are wrong and misleading. But we do get sucked into them and it's hard not to, right? It's hard not to think. I mean, it was an interesting moment for me when I realized the House Science Committee, well, the minority is calling me. And I'm telling them don't get a scientist to testify, you know, just bring in a pile of reports, right? And after I wrote, you know, I said that. I hung up the phone and thought, wow, that's an interesting moment, you know, right? So, but, I mean, we do have to be creative. We have to think differently about this, I think, than we have in the past. Okay, thank you. I'm actually related to what you just addressed. But so I'm a PhD student at this point without much public visibility or platform. And I'm just curious what you think are positive and productive venues for scientists to express their expertise to affect real change. Go to a local school, go talk in your church. I mean, we're all part of organizations and networks. And I think what you said about your job and your work is a really important insight. Sometimes we think we don't have access because we don't get invited to Congress so we don't get invited. But, you know, public opinion isn't really being made on the front page of the New York Times as much as the editors and New York Times would like to think that it is. Public opinion is being made by people talking to their friends and their neighbors and in their churches and in their synagogues and in their mosques and in their community groups. So there are opportunities to reach out to people all the time, but we sometimes don't see them because we take for granted what's around us. So I would say, you know, wherever you live, maybe the local library has a lecture series that you could volunteer to speak in. Or give me your phone number and I will tell them to invite you. And I mean, seriously, I get so many more invitations and I could possibly handle. And I'm always looking for other colleagues who I can suggest. So if you want to do talks, let me know. And I'll send, you know, I mean, obviously not everything I get invited to do, you know, they'll take a substitute, but some of those they will. But the point is they're all kinds of opportunities. And, you know, I was thinking about this the other day about being a Harvard professor and I'm going to South Dakota State tomorrow. You know, I think if most of us get invited to speak at Princeton or Chicago or Caltech, of course we go because we see the obvious value for our professional stature to do that. But I think that a lot of colleagues if they got invited to go to South Dakota State would just say no thanks, I'm busy, right? And my thinking about that is completely inverted now. I mean, I should say myself, 15 years ago, I would have probably thought that."}, {"content": "Now it's the other way around. I get invited to go to Princeton, I say, you guys don't need me, South Dakota State, they need me, right? So where I'm going to speak, what I'm accepting now is very different than even five years ago, because I'm really thinking in terms of places where outreach could make more of a difference. So I'm not doing any more teachers and teachers in Cambridge, I'm already told people that. Okay, so anyone's thinking about it, because I don't think this is where we need the action. And I think that all of us should be thinking about what are our points of connection to places where it could make a difference. And that, for a lot of us, means not on the Harvard campus, but it might be in our communities because lots of us live in places where, as I said, people might not be climate deniers, but they may not be very engaged in understanding why this issue is important. So let's just have two more questions, is that okay?"}, {"content": "Yeah, that'd be great, thank you. And then I'm sure, I know we would be happy to. Do what, go home and have a good night's sleep."}, {"content": "Yeah, yeah, yeah, okay. There are other contexts in which scientists are asked to make moral choices. A couple of years ago, Steven Hawking refused to attend a major conference in Israel because he was asked to observe the academic boycott there. Should scientists take a moral stand on very pressing issues of the day, if it's not directly related to their field of inquiry, even though it entails moral obligation in which some of their activities may come in contact with the circumstances that are resulting in various forms of repression in human suffering. Look, I mean, I can't tell other people what to do."}, {"content": "I think those kinds of issues are quite difficult and vexed and people have to decide for themselves. But in general, my view would be, would be no. I do think that as climate scientists or as toxicologists, or as marine biologists, or whatever our field is, that we have a particular role to play as experts. I mean, that's why I use the cartoon about applying the plane. And that's what I'm really most interested in. Now, of course, as human beings, we're going to be making choices all the time about what we think are more or more, or we might make choices about our own safety. I mean, speaking of Texas, I've been to Texas more than once. But I do have reservations now about going back. Now that Texas and universities are allowing students to carry guns on campus. I mean, I find that very problematic and actually potentially a threat to my own personal safety. So I think that we might make choices. We might decide, in particular, situations that we might not go somewhere because it offends our personal sensibilities or because we think it threatens our safety. Or we might just feel that we just need to take a stand. But at the same time, there's always that question of who you're hurting, right? Because when I went to Houston, I guess it was a year or two ago, it was right around the time that the legislature was debating that thing about carrying guns on campus. And I actually almost didn't go. And of course, my colleagues there said the obvious thing, which is if you don't come, you'll hurt us, right? We want to hear you. We, our students need to hear you. So what good does it do if you don't come, right? So there's always that question about who are you impacting through your actions. So I think when you get out and when you talk to people and when you try to be, to listen and be empathetic and understand people's questions and answer them honestly and take people's questions seriously, that's almost always a good thing to do. When you say, I can't go here because of X or Y, I think that's much more complicated, much more fraught. I think that's a good thing."}, {"content": "All right, let's start. I'll turn it on. I can speak through the stuff. I know it's on the screen for you. No, I won't be able to go up. Listen to that now."}, {"content": "Okay, my name's Andrew Bergman. I'm a PhD student here in applied physics and a member of a new group called the Environmental Data and Governance Initiative. And this question is something I'm experiencing, but I'll frame it in terms of the conversation about climate change. I think that sometimes what I notice, and it's sort of related to this question, but more explicitly about expertise within a very specific field. Even when you say someone is a climate scientist, that is a lot of different types of scientists working on a totally disparate set of issues. And sometimes when you go testify in Congress, you're talking about the general science of climate. But the specificity with which people like Oppenheimer and others really understood the nuclear physics they were working on, doesn't necessarily apply in the same sort of general way to climate scientists. And I support scientists who have sort of peripheral or solid understanding advocating. But when we want the specific sort of expertise driven, sort of as you said, like somebody who really is an expert has a very different platform which they can stand and speak, should we as a scientific community be more discerning and actually say like, when you decide to speak out as an expert, be clear about the specificity of your expertise. And don't show up to schools or Congress and say you're an expert just because you have a degree in physics. I mean, I don't think when my friends ask me, tell me about climate science, I say I really don't know. I'm trusting other scientists. And that's the reality."}, {"content": "This is a really important point."}, {"content": "Thank you for raising it. So one of the reasons why I've been thinking about this whole issue of approximate expertise is because it's actually essential if we're going to be able to refute merchants of doubt type claims, right? Because one of the ways that the whole doubt-mongering strategy works is to recruit scientists to be part of this, right? And that was a key strategy that the tobacco industry invented, which was to find scientists to come and speak. Now, in many cases, those people were actually scientists of some kind. They had some kind of scientific training, but they weren't oncologists. They weren't public health officials. They weren't physicians. And Fred Singer, who I talked about, is a classic case in point. I mean, the man was a very intelligent, very highly educated person, but he knew nothing about cancer or bronchitis or emphysema or epidemiology or any of the disciplines that could have been potentially relevant to that case. And that's the key point about what you're saying. So when it comes to climate science, there are many disciplines that are relevant. And there are many angles from which you might enter into the topic with legitimate strong expertise. Just as with tobacco, you could have been a physician. You could have been an epidemiologist. You could have been an oncologist. Many different expertise could have been relevant there. And Singer had none of them. So that should have been a red flag. It should have been a red flag to his colleagues. It should have been a red flag to the media. But it's not, in part, because we don't discern. And that is why I think it's so important that we are discerning and that we also exercise a certain amount of restraint. Right? And I work hard, really hard on this, because I get asked about all kinds of things. And you know, I have, I mean, just the other day, I got asked this question. I went and talked to the EPS graduate students and someone wanted to talk about immigration. And the first thing I said is, I, I, I, I, you know, no, I'm like, I feel like I know a lot of things and immigration isn't one of them. I don't feel like, you know, and I just said, let's talk about other things, right? So being able to say, it's not my area. I really don't know. If I answer that question, I just be giving you my personal opinion. I think that's really important."}, {"content": "Now that said, though, there is one other nuance to add to this. Expertise is not absolute, it's relative. So depending on the context, it might be that even though you're a physicist, you might actually be pretty knowledgeable about evolutionary biology. Maybe you've even read the origin of species. And if somebody asked you a question about it, you might be able to say, well, look, I'm not an evolutionary biologist, but I do know certain things. Or something I sometimes say when I'm asked, I'm not an economist, you know, but some of my best friends are economists. But my economist, my economics colleagues, say that carbon pricing is one of the most effective things we could do to level playing field and pay the true cost, not the price, but the cost of carbon. So you can invoke the authority of other experts that you know or that you've read. And then that invites your audience to say, okay, so if we want to learn more about this, you know, next year we invite an economist to come and speak to you. And you can even give them the names. You could say, well, I am friends with Nick Stern and here's what Nick says about it, right? And that's, I think, legitimate because you've talked to the person you've played attention to their arguments. One time years ago, when I was talking to a group of grad students, a student, and I forget, she was studying choral reefs. It was something to do with marine biology. And she said that she was in a choral group, a singing group, in which one of the other people in the group had asked her about climate change. And it was some element of climate change that was quite far away from what she worked on. And she said she felt really awkward because she felt like she wasn't an expert. But I said to her, you know, that's true. You're not an expert on, you know, tropospheric warming. But in that group, you are the local expert. And people are turning to you because they know that you're getting a PhD. This was at UC Davis. They know you're at Davis. They know Davis is a good school. And by asking you, they're actually saying that they trust you. They want to know what you know about this. And in that situation, I think it is reasonable to say, well, it's not my specialty, but here's what I know. And I think that, so expertise is contextual and relative. And it's about, in a way, it's just about being thoughtful about where your limits of expertise are and being honest about that. And when it goes too far to say, okay, I really can't answer that. I'm sorry. So, and on that note, I can't answer that. On that note. Thank you. Thank you. Thank you."}], "Naomi Oreskes: Why we should trust scientists": [{"content": "Every day we face issues like climate change or the safety of vaccines where we have to answer questions whose answers rely heavily on scientific information. Scientists tell us that the world is warming. Scientists tell us that vaccines are safe. But how do we know if they are right? Why should be believe the science? The fact is, many of us actually\ndon't believe the science. Public opinion polls consistently show that significant proportions of the American people don't believe the climate is\nwarming due to human activities, don't think that there is\nevolution by natural selection, and aren't persuaded by the safety of vaccines. So why should we believe the science? Well, scientists don't like talking about \nscience as a matter of belief. In fact, they would contrast science with faith, and they would say belief is the domain of faith. And faith is a separate thing\napart and distinct from science. Indeed they would say religion is based on faith or maybe the calculus of Pascal's wager. Blaise Pascal was a 17th-century mathematician who tried to bring scientific\nreasoning to the question of whether or not he should believe in God, and his wager went like this: Well, if God doesn't exist but I decide to believe in him nothing much is really lost. Maybe a few hours on Sunday."}, {"content": "(Laughter) But if he does exist and I don't believe in him, then I'm in deep trouble. And so Pascal said, we'd better believe in God. Or as one of my college professors said, \"He clutched for the handrail of faith.\" He made that leap of faith leaving science and rationalism behind. Now the fact is though, for most of us, most scientific claims are a leap of faith. We can't really judge scientific\nclaims for ourselves in most cases. And indeed this is actually\ntrue for most scientists as well outside of their own specialties. So if you think about it, a geologist can't tell you whether a vaccine is safe. Most chemists are not experts in evolutionary theory. A physicist cannot tell you, despite the claims of some of them, whether or not tobacco causes cancer. So, if even scientists themselves have to make a leap of faith outside their own fields, then why do they accept the\nclaims of other scientists? Why do they believe each other's claims? And should we believe those claims? So what I'd like to argue is yes, we should, but not for the reason that most of us think. Most of us were taught in school\nthat the reason we should believe in science is because of the scientific method. We were taught that scientists follow a method and that this method guarantees the truth of their claims. The method that most of us were taught in school, we can call it the textbook method, is the hypothetical deductive method. According to the standard\nmodel, the textbook model, scientists develop hypotheses, they deduce the consequences of those hypotheses, and then they go out into the world and they say, \"Okay, well are those consequences true?\" Can we observe them taking\nplace in the natural world? And if they are true, then the scientists say, \"Great, we know the hypothesis is correct.\" So there are many famous examples in the history of science of scientists doing exactly this. One of the most famous examples comes from the work of Albert Einstein. When Einstein developed the\ntheory of general relativity, one of the consequences of his theory was that space-time wasn't just an empty void but that it actually had a fabric. And that that fabric was bent in the presence of massive objects like the sun. So if this theory were true then it meant that light as it passed the sun should actually be bent around it. That was a pretty startling prediction and it took a few years before scientists were able to test it but they did test it in 1919, and lo and behold it turned out to be true. Starlight actually does bend\nas it travels around the sun. This was a huge confirmation of the theory."}, {"content": "It was considered proof of the truth of this radical new idea, and it was written up in many newspapers around the globe. Now, sometimes this theory or this model is referred to as the deductive-nomological model, mainly because academics like \nto make things complicated. But also because in the ideal case, it's about laws. So nomological means having to do with laws. And in the ideal case, the hypothesis isn't just an idea: ideally, it is a law of nature. Why does it matter that it is a law of nature? Because if it is a law, it can't be broken. If it's a law then it will always be true in all times and all places no matter what the circumstances are. And all of you know of at least\none example of a famous law: Einstein's famous equation, E=MC2, which tells us what the relationship is between energy and mass. And that relationship is true no matter what."}, {"content": "Now, it turns out, though, that there \nare several problems with this model. The main problem is that it's wrong."}, {"content": "It's just not true. (Laughter) And I'm going to talk about\nthree reasons why it's wrong. So the first reason is a logical reason."}, {"content": "It's the problem of the fallacy\nof affirming the consequent. So that's another fancy, academic way of saying that false theories can make true predictions. So just because the prediction comes true doesn't actually logically\nprove that the theory is correct. And I have a good example of that too, \nagain from the history of science. This is a picture of the Ptolemaic universe with the Earth at the center of the universe and the sun and the planets going around it. The Ptolemaic model was believed by many very smart people for many centuries. Well, why? Well the answer is because it made \nlots of predictions that came true. The Ptolemaic system enabled astronomers to make accurate predictions\nof the motions of the planet, in fact more accurate predictions at first than the Copernican theory\nwhich we now would say is true. So that's one problem with the textbook model."}, {"content": "A second problem is a practical problem, and it's the problem of auxiliary hypotheses. Auxiliary hypotheses are assumptions that scientists are making that they may or may not even\nbe aware that they're making. So an important example of this comes from the Copernican model, which ultimately replaced the Ptolemaic system. So when Nicolaus Copernicus said, actually the Earth is not the center of the universe, the sun is the center of the solar system, the Earth moves around the sun. Scientists said, well okay, Nicolaus, if that's true we ought to be able to detect the motion of the Earth around the sun. And so this slide here illustrates a concept known as stellar parallax. And astronomers said, if the Earth is moving and we look at a prominent star, let's say, Sirius -- well I know I'm in Manhattan\nso you guys can't see the stars, but imagine you're out in the country, \nimagine you chose that rural life \u2014 and we look at a star in December, we see that star against the backdrop of distant stars. If we now make the same observation six months later when the Earth has moved to this position in June, we look at that same star and we \nsee it against a different backdrop. That difference, that angular\ndifference, is the stellar parallax. So this is a prediction that the Copernican model makes. Astronomers looked for the stellar parallax and they found nothing, nothing at all. And many people argued that this proved \nthat the Copernican model was false. So what happened? Well, in hindsight we can say \nthat astronomers were making two auxiliary hypotheses, both of which we would now say were incorrect. The first was an assumption \nabout the size of the Earth's orbit. Astronomers were assuming \nthat the Earth's orbit was large relative to the distance to the stars. Today we would draw the picture more like this, this comes from NASA, and you see the Earth's orbit is actually quite small. In fact, it's actually much\nsmaller even than shown here. The stellar parallax therefore, is very small and actually very hard to detect. And that leads to the second reason why the prediction didn't work, because scientists were also assuming that the telescopes they had were sensitive enough to detect the parallax. And that turned out not to be true. It wasn't until the 19th century that scientists were able to detect the stellar parallax. So, there's a third problem as well. The third problem is simply a factual problem, that a lot of science doesn't fit the textbook model. A lot of science isn't deductive at all, it's actually inductive. And by that we mean that scientists don't necessarily start with theories and hypotheses, often they just start with observations of stuff going on in the world. And the most famous example\nof that is one of the most famous scientists who ever lived, Charles Darwin. When Darwin went out as a young \nman on the voyage of the Beagle, he didn't have a hypothesis, he didn't have a theory. He just knew that he wanted\nto have a career as a scientist and he started to collect data. Mainly he knew that he hated medicine because the sight of blood made him sick so he had to have an alternative career path. So he started collecting data. And he collected many things, \nincluding his famous finches. When he collected these finches,\nhe threw them in a bag and he had no idea what they meant. Many years later back in London, Darwin looked at his data again and began to develop an explanation, and that explanation was the\ntheory of natural selection. Besides inductive science, scientists also often participate in modeling. One of the things scientists want to do in life is to explain the causes of things. And how do we do that? Well, one way you can do it is to build a model that tests an idea. So this is a picture of Henry Cadell, who was a Scottish geologist in the 19th century."}, {"content": "You can tell he's Scottish because he's wearing a deerstalker cap and Wellington boots. (Laughter) And Cadell wanted to answer the question, how are mountains formed? And one of the things he had observed is that if you look at mountains\nlike the Appalachians, you often find that the rocks in them are folded, and they're folded in a particular way, which suggested to him that they were actually being\ncompressed from the side. And this idea would later play a major role in discussions of continental drift. So he built this model, this crazy contraption with levers and wood, and here's his wheelbarrow, buckets, a big sledgehammer. I don't know why he's got the Wellington boots. Maybe it's going to rain. And he created this physical model in order to demonstrate that you could, in fact, create patterns in rocks, or at least, in this case, in mud, that looked a lot like mountains if you compressed them from the side. So it was an argument about\nthe cause of mountains. Nowadays, most scientists prefer to work inside, so they don't build physical models so much as to make computer simulations. But a computer simulation is a kind of a model. It's a model that's made with mathematics, and like the physical models of the 19th century, it's very important for thinking about causes. So one of the big questions\nto do with climate change, we have tremendous amounts of evidence that the Earth is warming up. This slide here, the black line shows the measurements that scientists have taken for the last 150 years showing that the Earth's temperature has steadily increased, and you can see in particular\nthat in the last 50 years there's been this dramatic increase of nearly one degree centigrade, or almost two degrees Fahrenheit. So what, though, is driving that change? How can we know what's causing the observed warming? Well, scientists can model it using a computer simulation. So this diagram illustrates a computer simulation that has looked at all the different factors that we know can influence the Earth's climate, so sulfate particles from air pollution, volcanic dust from volcanic eruptions, changes in solar radiation, and, of course, greenhouse gases. And they asked the question, what set of variables put into a model will reproduce what we actually see in real life? So here is the real life in black. Here's the model in this light gray, and the answer is a model that includes, it's the answer E on that SAT, all of the above. The only way you can reproduce the observed temperature measurements is with all of these things put together, including greenhouse gases, and in particular you can see that the increase in greenhouse gases tracks this very dramatic increase in temperature over the last 50 years. And so this is why climate scientists say it's not just that we know that\nclimate change is happening, we know that greenhouse gases are a major part of the reason why. So now because there all these different things that scientists do, the philosopher Paul Feyerabend famously said, \"The only principle in science that doesn't inhibit progress is: anything goes.\" Now this quotation has often\nbeen taken out of context, because Feyerabend was not actually saying that in science anything goes. What he was saying was, actually the full quotation is, \"If you press me to say what is the method of science, I would have to say: anything goes.\" What he was trying to say is that scientists do a lot of different things. Scientists are creative. But then this pushes the question back: If scientists don't use a single method, then how do they decide what's right and what's wrong? And who judges? And the answer is, scientists judge, and they judge by judging evidence. Scientists collect evidence in many different ways, but however they collect it, they have to subject it to scrutiny. And this led the sociologist Robert Merton to focus on this question of how scientists scrutinize data and evidence, and he said they do it in a way he called \"organized skepticism.\" And by that he meant it's organized because they do it collectively, they do it as a group, and skepticism, because they do it from a position of distrust. That is to say, the burden of proof is on the person with a novel claim. And in this sense, science\nis intrinsically conservative. It's quite hard to persuade the scientific community to say, \"Yes, we know something, this is true.\" So despite the popularity of the concept of paradigm shifts, what we find is that actually, really major changes in scientific thinking are relatively rare in the history of science. So finally that brings us to one more idea: If scientists judge evidence collectively, this has led historians to focus on the question of consensus, and to say that at the end of the day, what science is, what scientific knowledge is, is the consensus of the scientific experts who through this process of organized scrutiny, collective scrutiny, have judged the evidence and come to a conclusion about it, either yea or nay. So we can think of scientific knowledge as a consensus of experts. We can also think of science as being a kind of a jury, except it's a very special kind of jury. It's not a jury of your peers, it's a jury of geeks. It's a jury of men and women with Ph.D.s, and unlike a conventional jury, which has only two choices, guilty or not guilty, the scientific jury actually has a number of choices. Scientists can say yes, something's true. Scientists can say no, it's false. Or, they can say, well it might be true but we need to work more\nand collect more evidence. Or, they can say it might be true, but we don't know how to answer the question and we're going to put it aside and maybe we'll come back to it later. That's what scientists call \"intractable.\" But this leads us to one final problem: If science is what scientists say it is, then isn't that just an appeal to authority? And weren't we all taught in school that the appeal to authority is a logical fallacy? Well, here's the paradox of modern science, the paradox of the conclusion I think historians and philosophers and sociologists have come to, that actually science is the appeal to authority, but it's not the authority of the individual, no matter how smart that individual is, like Plato or Socrates or Einstein. It's the authority of the collective community. You can think of it is a kind of wisdom of the crowd, but a very special kind of crowd. Science does appeal to authority, but it's not based on any individual, no matter how smart that individual may be. It's based on the collective wisdom, the collective knowledge, the collective work, of all of the scientists who have worked on a particular problem. Scientists have a kind of culture of collective distrust, this \"show me\" culture, illustrated by this nice woman here showing her colleagues her evidence. Of course, these people don't\nreally look like scientists, because they're much too happy. (Laughter) Okay, so that brings me to my final point."}, {"content": "Most of us get up in the morning. Most of us trust our cars. Well, see, now I'm thinking, I'm in Manhattan, this is a bad analogy, but most Americans who don't live in Manhattan get up in the morning and get in their cars and turn on that ignition, and their cars work, and they work incredibly well. The modern automobile hardly ever breaks down. So why is that? Why do cars work so well? It's not because of the genius of Henry Ford or Karl Benz or even Elon Musk. It's because the modern automobile is the product of more than 100 years of work by hundreds and thousands and tens of thousands of people. The modern automobile is the product of the collected work and wisdom and experience of every man and woman who has ever worked on a car, and the reliability of the technology is the result of that accumulated effort. We benefit not just from the genius of Benz and Ford and Musk but from the collective intelligence and hard work of all of the people who have worked on the modern car. And the same is true of science, only science is even older. Our basis for trust in science is actually the same as our basis in trust in technology, and the same as our basis for trust in anything, namely, experience. But it shouldn't be blind trust any more than we would have blind trust in anything. Our trust in science, like science itself, should be based on evidence, and that means that scientists have to become better communicators. They have to explain to us not just what they know but how they know it, and it means that we have\nto become better listeners. Thank you very much."}, {"content": "(Applause)"}], "1. Introduction and Supply & Demand": [{"content": "[SQUEAKING] [RUSTLING] [CLICKING] JONATHAN GRUBER: This is 14.01. I'm John Gruber, and\nthis is microeconomics. Today, I want to\ncover three things. I want to talk about\nthe course details. I want to talk about\nwhat is microeconomics. And then I'll start the\nsubstance of the course by talking about\nsupply and demand. Couple of the points about\nthe course-- the course will have a distinct sort\nof policy angle to it. I sort of do economic policy,\ngovernment policy is my thing. So I think it's what\nmakes economics exciting and it sort of offers, I\nthink, an interesting angle to understand why we're\nlearning what we're learning. I think sometimes\nin an intro class, it's sort of hard to\nunderstand why the heck you're doing things. However, that's just\nsort of a slight flavor. If you're really more\ninterested in this, I teach a whole\ncourse called 1441. I'm not teaching it\nthis year, but it will be taught by a visitor\nin the spring, Kristin Butcher from Wellesley."}, {"content": "And I'll be teaching next year. That dives much more\ninto these policy issues. So I'm going to use government\npolicy as sort of an organizing theme, but it won't be the\ndominant theme of the class. Finally, three points\nabout my teaching style. I don't write\neverything on the board. We're not in high\nschool anymore. You're actually responsible for\nwhat I say, not what I write. Partly that's because my\nhandwriting is brutal, as you can tell already. So what that means is,\nplease, please do not be afraid to ask me what the\nhell I just wrote on the board. There's no shame in that. Don't just lean to your\nneighbors, and say, what the hell did he\njust write in the board. Ask, me, because if\nyou can't read it, I'm sure someone else can't\nread it, so feel free to ask. And in general, please\nfeel free to engage with questions in this class. The other point of my teaching\nstyle is I talk way too fast. And the longer I go-- there's\na mathematical function, which is the longer I\ngo without interruption, the faster I speak,\nuntil I just spin off. So basically, please\nask questions. If anything is not\nclear, or you just want to ask questions\nabout some related tangent or whatever, please\nfeel free to do so. You might think, how would\nthat work in a class this big? There's always way too\nfew questions, even a class this big. So never be afraid that it\nwill slow me down or whatever. Ask me questions. We have plenty of\ntime on the class. And you'll be doing\nyour classmates a favor, because it'll slow me down. Finally, last point, I\nhave this terrible tendency to use the term \"guys\"\nin a gender neutral way. So this class, I\nlike to see, looks like it's a fairly\nhealthy representation both males and females. When I say \"guys,\"\nI don't mean men. I mean people. I mean people. So women, don't\ntake it personally. \"Guys\" means economic agent. It means people. It doesn't mean men. Just the way-- just\na bad tendency. It drives my wife crazy,\nbut I've decided better to just apologize up front\nthan try to fix it throughout, which is impossible. So let's talk about\nwhat is microeconomics. So fundamentally,\nmicroeconomics-- how people took AP high school Econ? How many people--\nfor how many people was it taught really well? That's about right. That's why I did my high\nschool online class."}, {"content": "That's the answer\nI wanted to hear. So tell your friends\nstill in high school who are taking high school\nEcon, if your high school teacher isn't great,\ntell them to go on EdX and take the class. And help out your friends\nstill in high school. So what is microeconomics? Microeconomics is\nthe study of how individuals and\nfirms make decisions in a world of scarcity. Scarcity is what\ndrives microeconomics. Basically, what\nmicroeconomics is is a series of constrained\noptimization exercises, where economic agents, be\nthey firms or individuals, try to make themselves\nas well off as possible given their constraints. Yeah. AUDIENCE: Will this\ncover irrationality? JONATHAN GRUBER: I will,\nbut not as much as I should. Essentially, we\nhave another course in the department called 1413,\nBehavioral Economics, which gets into that much more. I will sprinkle it\nthroughout, but not as much as I actually\nbelieve in it. In other words, the way\nwe think about economics is it's best to sort\nof get the basics down before you start worrying\nabout the deviations. Find it's better\nto climb the tree before you start going\nout in the branches. So basically, what this\ncourse is then about is it's about trade-offs. It's about given that\nyou're constrained, how do you trade off things\nto make yourself as well off as possible? And behind this notion of\ntrade-offs is going to be-- I'll say about 100 times this\nis the most important thing in the course, so\njust ignore that. But this is one of the\nmost important things. I'll say \"one of the\nmost important\" things in the course, is the\nnotion of opportunity cost. Opportunity cost is a\nvery important concept that we teach, sort of the\nfirst concept we teach, which is that every\naction or every inaction has a cost in that you\ncould've been doing something else instead. So if you buy a shirt, you\ncould have bought pants. If you stayed at\nhome and watched TV, you could have been out working. Everything you do has\na next best alternative you could have done instead. And that is called the\n\"opportunity cost.\" And that's a critical\nconcept in economics, and that is why,\nin some sense, we are referred to casually\nas the \"dismal science.\" Economics is referred to\nas the dismal science. First of all, I'm flattered\nwe're considered a science. But it's called the\n\"dismal science\" because our whole point\nis that nothing is free. There is always a trade-off."}, {"content": "There's always an\nopportunity cost. Anything you do, you could be\ndoing something else instead. And your constrained\noptimization means you're going to\nhave to pass up one thing to do another. Now, some may call it\n\"dismal,\" but as a former MIT undergraduate, I call it \"fun.\" And this is why I think\nMIT is the perfect place to be teaching economics,\nbecause MIT engineering is all about constrained\noptimization. That's what engineering is. And economics is\njust the engine. It's just the principles\nyou learn in engineering applied in different contexts. So if we think about the\n2.007 contests-- that still exist with the robots, 2.007? Yeah, the 2.007 contests,\nthose, as you know, are contests where you're given\na limited set of materials. And you have to build a\nrobot that does some task, like pushing ping-pong balls off\na table or something like that. That's just constraint\noptimization. It's got nothing to\ndo with economics, but it's constrained\noptimization. So just think of microeconomics\nas like engineering, but actually interesting. So think of microeconomics\nas engineering, but instead of\nbuilding something to push a ping-pong ball\noff tables, you actually build people's lives,\nand businesses, and understand the decisions\nthat drive our economy. So same principles\nyou could think of for your engineering classes,\nbut applied to people's lives. And that's why, in fact, modern\neconomics was born in this room, this room or 26.100 by\nPaul Samuelson in the 1940s and '50s, who wrote the\nfundamental textbook that gave birth to modern economics. Because he was here and\napplied the kind of engineering principles of MIT\nto actually develop the field of modern economics. What we'll learn today\nwas developed at MIT, so it's a great place\nto be learning it. Now, with that as background--\nany questions about that, about what is microeconomics? With that as\nbackground, let's turn to our first model we'll talk\nabout this semester, which is the supply and demand model. Supply and demand--\nnow, the way we're going to proceed in this course\nis going to drive you crazy, because we're going to\nproceed by teaching, as the very first\nquestion pointed out, by teaching very\nsimplified models. We're going to essentially--\nwhat is a model? A model is technically\na description between any two or more economic\nvariables or any two or more variables. But unlike the models used\nin all your other classes, these aren't laws, by and\nlarge, they're models. So we don't have a relation\nbetween energy and mass which you can write down. It's a law and you're done."}, {"content": "We have models which are\nnever 100% true, but always pretty true, \"pretty\"\nbeing somewhere between 10% and 95% true. So basically, the idea\nis to make a trade-off. We want to write\ndown in our models a set of simplifying\nassumptions that allow us, with a relatively\nsmall set of steps, to capture relatively\nbroad phenomena. So it's essentially a trade-off. On the one hand,\nwe'd like a model that captures as well as\npossible the phenomena in the real world, like\nE equals Mc squared. But we want to do so in the\nmost tractable possible way so that we can teach it\nfrom first principles, and don't need an arrow to teach\nevery single insight we have. So basically in\neconomics, we tend to resolve that by erring\non the side of tractability. That is why I can teach\nyou the entire field of microeconomics--\nwhich is really sort of-- macro is kind of\na fun application. Micro is really economics. I can teach you the entire\nfield of microeconomics in the semester,\nbecause I'm going to make a whole huge set\nof simplifying assumptions to make things tractable. But the key thing\nis that you will be amazed at what these\nmodels will be able to do. With a fairly simple\nset of models, we will be able to offer\ninsights and explain a whole huge variety\nof phenomena, never perfectly, but\nalways pretty well, generally pretty well. And so that is\nessentially the trade-off we're going to try\nto do this semester. So the line I like\nis the statistician George Box said that all models\nare wrong, but some are useful. Now obviously, it doesn't apply\nto models in the hard sciences, but in the social\nsciences, that's true. And basically, I'm\ngoing to write down a set of models like that. Now, with every model I write\ndown, I'm going to try-- my goal is to have you\nunderstand it at three levels. The first and most\nimportant level is the intuitive\nlevel, the level which you sort of understand. I call it \"passing\nthe Mom Test.\" You can go home and explain\nit to your mom at Thanksgiving or at the end of semester. No offense to dads, just\ncalled it \"the Mom Test.\" So basically, that's\nthe intuitive level. You really understand it in a\nway that you could explain it."}, {"content": "The second is graphical. We were going to do--\nmost of our models here were developed in a\ngraphical framework using x/y graphs that really in\neconomics, we think delivers a lot of shorthand power. And the third is mathematical. The mathematical is probably\nthe least important, but it's the easiest\nto test you on."}, {"content": "So we're going to need to know\nthings mathematically as well. So let's start by considering\nthe supply and demand model by using\nthe famous example brought up by Adam Smith. Adam Smith is sort of considered\nthe father of economics. If Paul Samuelson is the\nfather of modern economics, Adam Smith is the\nfather of all economics. His 1776 book, The\nWealth of Nations did an incredible job\nof actually laying out the entire core of\nthe economics field-- no math, just words,\nbut he just nailed it. And one of his most\nfamous examples was the water diamond paradox. He said, think about\nwater and diamonds. He said, start with water. Nothing is more important\nfor life than water. It's the building\nblock of all of life. Even when we look for\nlife on other planets, we always start by\nlooking for water. Now think of diamonds, one\nof the more frivolous things you can buy, certainly\nirrelevant to leading a successful or happy or\nproductive life, or any life. Yet for most of us,\nwater's free and diamonds are super expensive. How can this be,\nAdam Smith asked. Well, the answer he posed is\nthat what I first described was just demand. That is, we demand\nlots of water. We demand fewer diamonds. But we have to match that\nwith the concept of supply. And the supply of water\nis almost infinite, while the supply of diamonds--\nmaybe not naturally, maybe it's through decisions\nof various businesses-- but it's somewhat limited. So basically what\nhe developed is what we call the \"supply\nand demand scissors\"-- that you can't just think of\nsupply or demand in isolation. You have to put\nthem together if you want to explain the real world\nphenomena we see, like the fact that water is cheap and\ndiamonds are expensive. So let's just about an example. So there's one graph\nthat was handed out in the back, which\nis, let's talk about the market for roses. So in the market for roses,\nwe have a demand curve and a supply curve. So what we have here-- this\nis the kind of x/y graph we're going to look at all\nthroughout the semester. On the x-axis is the\nquantity of roses. On the y-axis is\nthe price of roses. The blue, downward-sloping\nline is the demand curve. Now, what I'm going to do here,\nI'm just giving you a overview. We are going, over the next\nfive or six lectures, dive into where this demand\ncurve comes from. We'll go to first principles\nand build it back up. But for now, what\nwe know of a demand curve is it simply\nrepresents the relationship between the price of a good\nand how much people want it. Therefore, we assume\nit is downward sloping. At higher prices, people\nwant less of the good. And we'll derive where\nthat comes from shortly, starting next lecture. But for now, I think\nit's pretty intuitive that if the price\nof roses is higher, people want fewer of them. And that's why it's\ndownward sloping. Basically, as the\nprice of roses goes up, people want fewer roses. The yellow curve is\nthe supply curve. Now, after we've derived\nthe demand curve, we'll then go and\nspend about 12 lectures deriving the supply curve. That's a bit harder. But once again, we'll\nstart from first principles and build it up. For now, you just need to\nknow that's how much firms are willing to supply,\ngiven the price. So basically, as\nthe price goes up, firms want to\nproduce more roses. The higher price means\nyou make more money, so you want to\nproduce more of them. This is slightly less\nintuitive than demand, but we'll derive it and\nexplain how it can be. But for now, just go\nwith the basic intuition that if you're making\nsomething, and you can sell it in the market for\na higher price, you're going to want\nto make more of it. And that leads to the\nupward sloping supply curve. Where the points meet is\nthe market equilibrium. Where supply and demand meets\nis the market equilibrium. And that is the point where\nboth consumers and producers are happy to make a transaction. Consumers are happy because\non their demand curve is the $3 and 600 roses. That is, they are willing\nto buy 600 roses at $3. Producers are happy,\nbecause on their supply curve is the same point. They are willing to\nsupply 600 roses at $3. That is the one point\nwhere consumers are happy and producers are happy. Therefore, it's\nthe equilibrium-- highly non-technical, but\nthat's the basic intuition. The point at which they're\nboth willing to make that transaction, the\npoint at which they're both satisfied with\nthat transaction, is the equilibrium, which\nin this case is $3 per rose and 600 roses. Now, this raises\nlots of questions. Where did the curves come from? How does equilibrium\nget achieved? Why the heck do we give roses? These are a bunch of questions."}, {"content": "We will come to\nall these questions over the next set of lectures. But the basic thing\nis to understand this intuition of Adam Smith's\nsupply and demand model. Questions about that? Now, this model also raises\nanother important distinction that we'll focus\non this semester and is easy to get mixed up. So I want you to, if\nyou're ever unclear, I want you to ask me about it. And that's the distinction\nbetween positive versus normative analyses--\npositive versus normative. Positive analysis is the\nstudy of the way things are, while normative\nanalyses is the study of the way things should be. A positive analysis is the\nstudy of the way things are, while normative\nanalysis is the study of the way things should be. Let me give you a great\nexample, which is eBay auctions."}, {"content": "Auctions are a terrific example. They're like the\ntextbook example of a competitive market. You can see it in your head-- demand comes as a bunch of\npeople going on and bidding. People who want\nit more bid more, so you actually\nget a demand curve. The higher the price, the fewer\npeople you're getting to bid. Supply is how many units\nof it are for sale on eBay. You bid until those two meet. And then you have a\nmarket equilibrium at that bidded price. Now, one example\nof an eBay auction that got a lot of attention\na number of years ago, early in the days\nof eBay, was someone offered their\nkidney for auction. They said, look,\nI got two kidneys."}, {"content": "You only need one to live. There are people out\nthere who need a kidney. I'm putting my kidney\non eBay for auction. And what happened,\nbidding went nuts. It started at $25,000. It climbed to $5 million before\nthe auction was shut down, and eBay decided\nthey wouldn't allow you to sell your body on\neBay, bodily parts on eBay. So this raises two questions. The first is the\npositive question, why did the price go so high? So what's the answer to that? What's the answer to\nthe positive question? AUDIENCE: Somebody\nwanted a kidney. JONATHAN GRUBER: Good\nanswer, but let's raise hands and give answers. That's part of it."}, {"content": "Yeah. AUDIENCE: Low\nsupply, high demand. JONATHAN GRUBER: Low\nsupply, high demand. Demand is incredibly high,\nbecause I'd die without it. Supply is low, because\nlike not a lot of us are willing to\nsell their kidneys on eBay So low supply, high\ndemand led to a high price-- Adam Smith at work. That's the positive analysis. But then there's the\nnormative question, which is, should you be allowed to\nsell your kidneys on eBay? That's the normative question."}, {"content": "The positive question is,\nwhat happens if you do? The normative question\nis, should you? Now, the standard\neconomics answer to start would be, of course you should. We're in a world where\nthousands of people die every year because there's\na waiting list for a kidney transplant. and these are people who would\nhappily pay a lot of money to stay alive, I presume. Meanwhile, there's\nhundreds of millions of people walking around with\ntwo kidneys who only need one. And many of these\npeople are poor. And lives could be changed\nby being paid $1 million for their kidney, and might be\nhappy to take the risk that one kidney will be fine, as\nit is for most everyone for most of their\nlife, in return for having a life-changing\npayment from a stranger. So economists say, look-- here's a transaction that\nmakes both parties better off. The person who gets the\nkidney gets to stay alive, and they are willing to\npay a huge amount for that. The person who sells the\nkidney in most probability is fine, because\nalmost all of us can make it through life\nfine with one kidney, and create a life-changing\namount of money that could allow them to pursue\ntheir dreams in various ways. So that's the standard\nargument, would be, yeah, you should be able to\nsell your kidneys on eBay. So the question is, why not?"}, {"content": "Why would we want to\nstop this transaction? What are the\ncounter-arguments to that?"}, {"content": "Let's raise our hands."}, {"content": "Yeah. AUDIENCE: Potentially, I\nthink maybe the issue is because on eBay, there's\nno way to regulate it or you don't necessarily know. People could be like selling\nfake kidneys, per se. JONATHAN GRUBER: Right. So the first type\nof problem comes out of the category we\ncall \"market failures.\" Market failures are reasons\nwhy the market doesn't work in the wonderful\nway economists like to think it should. So for example,\nthis answer puts up there could be the\nproblem of fraud. People might not be\nable to tell if they're getting a legit kidney or not. There could be the example\nof imperfect information. Do you know what the\nodds are that you can spend the rest of your\nlife with only one kidney? I don't either. We ought to know that before\nwe start selling our kidneys. There could be\nimperfect information."}, {"content": "This is one type of problem,\nwhich is the market, maybe the market may fail. Yeah. AUDIENCE: Well, the\ncurrent system also holds people who are poor\nand have a failed kidney-- and which are people who would\nbe completely screwed otherwise in the [INAUDIBLE] system. JONATHAN GRUBER:\nA second problem is what we call\n\"equity\" or \"fairness.\" Equity or fairness, which is\nwe would end up with a world where only rich people\nwould get kidneys. Currently, there's a bunch of\nvoluntary donors and people who are in accidents who\nhave kidneys left over. And those go to people\non the basis of where they are on a waiting list. It's actually a\nprioritized waiting list. It's kind of a cool-- one of my colleagues, Nikhil\nAgarwal, if you think about-- I'll talk a lot this semester\nabout the imperialistic view of economics, all the\ncool things we can study. So he actually uses\neconomic models to study the optimal way to\nallocate organs to individuals. now it's just done\nbased on a waiting list, but it may be that someone\nfurther down the waiting list needs it more than someone\nhigher up the waiting list because they're more\ncritical or whatever. So there's various\noptimal ways to allocate. But certainly, the\noptimal way to allocate wouldn't be the rich\nguy gets it first. That would be unlikely to be\nwhat society would necessarily want. So there's an equity\nconcern with that. What else?"}, {"content": "What other-- yeah. AUDIENCE: In that\nsituation, since you know you can make money\noff of selling kidneys, and you take advantage\nof people, it's very bad, the black market for kidneys. JONATHAN GRUBER: Right, so\nthere's sort of a third-- it's related to\nfraud, but there's sort of a third\nclass of failures that gets into the question\nabout behavioral economics that was raised earlier, which\nwe could just call behavioral-- it's called\n\"behavioral economics,\" for want of a better term,\nwhich is essentially, people don't always\nmake decisions in the perfectly rational,\nlogical way we will model them as doing so this semester. People make mistakes. That's a word we hate\nusing in economics. We hate saying \"mistakes.\" Ooh, boo, mistakes--\nnobody makes mistakes. We're all perfectly\neconomic beings. But we know that's not true. Increasingly over the\npast several decades, economists have started\nincorporating insights from psychology into our\nmodels, to not just say people make mistakes,\nthat their lackadaisical, but to rigorously model the\nnature of those mistakes and understand how\nmistakes can actually happen due to various cognitive\nbiases and other things. In this world, you can imagine\npeople could make mistakes. They could not really\nsit down and quite understand what\nthey're doing, and they could have sold their\nkidney when it's really not in their own long-term interest. Yeah. AUDIENCE: Would\nanother example be if there's a family that\nis in extreme poverty, even though they\nonly have one kidney, they might sell the other\none, just to get more money for the family, per se? JONATHAN GRUBER: Well, in\nsome sense that would be, once again-- if we took this factor out,\nif the market works well with its behavioral\neffects, we'd say, you know, that's their decision. If they otherwise they\nstarve, who are you to say? But once you choose\nthis, say, wait a second, maybe they're not evaluating\nthe trade-offs correctly. Even if there's no fraud, even\nif there's perfect information, they may not know how to process\nthat information correctly. But that is not\nstandard economics."}, {"content": "That's not what we'll spend a\nlot of time on in the semester, but it's obviously realistic. So those are a bunch of good\ncomments, great comments. And yeah. AUDIENCE: Also, in\ninelastic demand, such that people\nalways need kidneys-- JONATHAN GRUBER: That won't\nturn out to be a problem. That doesn't turn\nout to be a problem. We'll come back--\nthat's a great comeback that we talk about the\nshape of demand curves. We want to return to that\nquestion in a few lectures, but that doesn't\nactually cause a problem. It's just that's more of\na positive thing about why the price is so high, but it's\nnot a normative issue about whether you should\nallow it or not. So basically,\nthese are exactly-- to me, honestly, I spend\nmy life thinking a lot about these things. I think these are really\ninteresting issues. But you can't get to\nthe normative issues without the positive analysis. You do the positive\nanalysis to understand the economic framework\nbefore you start jumping to drawing conclusions. That's no fun. We all want to jump\nto draw conclusions, saying this should happen,\nthis shouldn't happen. You can't do that."}, {"content": "We have to be disciplined. We have to start with the\nfundamental economic framework. And basically, the bottom line-- I said I'll teach this\ncourse with a policy bent, but you have to recognize\nthat economics at its core is a right-wing science. Economics at its\ncore is all about how the market knows best, and that\nbasically governments only mess things up. That's sort of the\nbasic, a lot of what we'll learn this semester. As the semester\ngoes on, we'll talk about what's wrong\nwith that view and how governments\ncan improve things. Indeed, I teach a whole\ncourse about the proper role of government the economy. But the standard of economics\nis, \"the market knows best.\" And that leads us to the last\nthing I want to talk about, which is basically, how freely\nshould an economy function? Let's step back to\nthe giant picture. Let's step back from\na market for roses to the entire economy. How freely should a market,\nshould an economy function? We have what's known as\na \"capitalistic economy.\" In a capitalistic economy,\nfirms and individuals decide what to\nproduce and consume, maybe subject to some rules of\nthe road set by the government. There's some minimum\nrules of the road to try to avoid fraud\nor misinformation, but otherwise, we\nlet the dice roll. Firms let consumers\ndecide sort of what to do. Now, this has led to\ntremendous growth. America was not\na wealthy nation, was not a very wealthy\nnation 100 years ago, or 150 years ago. Led to tremendous\ngrowth, where we are now the most powerful, still the\nmost powerful and wealthiest nation the world, largely driven\nby the capitalistic nature of our economy. On the other hand,\nwe are a nation with tremendous inequality. We are by far the most unequal\nmajor nation in the world. The top 1% of Americans has a\nmuch higher share of our income than in any other large\ncountry in the world, any other large developed\ncountry in the world. The bottom 99% has less of\nour income corresponding with anywhere else. So it's led to major inequality. And it's led to other problems. It turns out that the\ngovernment can't appropriately set the rules of the road to\navoid things like fraud, as we saw with Enron, if you\nremember back to that, or a lot of what happened\nin the financial meltdown. It turns out it's\nhard to get people perfect information, et cetera. So we've seen the problems. We've grown very\nwealthy as a nation. We've introduced a whole set of\nproblems through this system. Now, the other extreme is what's\ncalled the \"command economy.\" Rather than a\ncapitalist economy, it's what's called\na \"command economy.\" In this case, the government\nmakes all the production and consumption decisions. The government doesn't just\nset the rules of the road, the government owns the road. The government says, we're\ngoing to use this many cars this year. And people can get\nthem in some way. It could be a lottery,\ncould be waiting in line. How do we decide how\nto allocate them? We're not going to let\nthe market allocate them. We, the government,\nwill allocate them. We'll allocate how many get\nproduced and who gets them. And this was the model\nof the Soviet Union that I grew up with. This was the pre-1989\nSoviet Union. The government decided how many\nshirts, cars, TVs, everything. It's sort of bizarre\nto think that literally everything the government\ndecided how much to produce. And by and large, the government\ndecided who got it partly through corruption-- that\nis, the party members, party leaders got it first-- and often just through waiting\nin line for the remaining application. Now in theory,\nthis ensured equity by making sure that\neverybody had shot at things. In practice, it didn't\nwork well at all and actually was\nwhat dragged down the collapse of the\nold Soviet economy, was that the command\nmodel simply doesn't work. Partly there's just too many\nopportunities for corruption. When the government\ncontrols everything, that means there's no checks\nand balances on the opportunity for enormous corruption. The capitalist economy puts\nsome natural checks and balances on that. And partly because it\nturns out that it's hard to control human nature. And Adam Smith had it right. Adam Smith talks about\nthe \"invisible hand\" of the capitalist economy. The invisible hand is\nbasically the notion that the capitalist\neconomy will manage to distribute things roughly in\nproportion to what people want. And that's where\nfolks want to be."}, {"content": "Folks who want a\ncertain kind of car are going to want to\nget to that kind of car, and if the government\nhas it wrong, they're going to get upset. And it's going to lead to\na less functional economy. So basically, Adam\nSmith's view is that-- the invisible hand view is that\nconsumers and firms serving their own best interest will\ndo what is best for society. So the fundamental core\nof the capitalistic view is that consumers and firms\nserving their own best interest will do what ends up\nbeing best for society. And that's essentially\nthe model we'll learn to start in this course. Yeah. AUDIENCE: In that\ndefinition, are we defining the best for\nsociety as in everybody has the most money? Or everyone has the best health\nor the best standard of living? What is the best [INAUDIBLE]? JONATHAN GRUBER: Great question."}, {"content": "We're going to spend a lot\nof the semester talking about that. For now, we're going to\ndefine \"best for society\" as the most stuff gets\nproduced and consumed. That's how we're\ngoing to find it-- obviously raises a set\nof issues about what about pollution, what\nabout health, et cetera. We're going to come to those,\nbut for the first two-thirds of the course \"best\nfor society\" means what we're going to call\n\"maximum surplus,\" which is the most stuff gets\nproduced that people value. So that's how we're\ngoing to do it. And in his view, the\ninvisible hand does that. And by and large, it's a very\nhelpful framework to turn to. However, at least it\ncan lead to outcomes that are not very fair. So the way we're going\nto proceed in this course is we're going to start\nby talking about how Adam Smith's magic works. How does the magic happen? How does individuals\nand firms acting in their own self-interest,\nwithout caring about anybody else, end up yielding\nthe largest possible productive economy? How does that happen?"}, {"content": "And we're going to\ntalk about that. We'll start with\ndemand, which is how do consumers\ndecide what they want given their resources. We'll talk about the principle\nof utility maximization, the idea that I have\na utility function that I can mathematically\nwrite down what I want. I'll have a budget constraint,\nwhich is the resources I have, and those two\nconstrain optimization. We'll say given what I want\nand the resource I have, what decisions do I make? Boom, we get the demand curve. Then we'll turn to supply, and\nwe'll talk about how do firms decide what to produce. That's much more\ncomplicated, because firms have to decide\nwhat inputs to use and what outputs to produce. And we'll talk about\nhow firms can operate in very different markets. There is a competitive market\nthat Adam Smith envisioned, but that doesn't always work. Sometimes we get\nmonopoly markets, where one firm dominates. And you can actually\nhave outcomes which aren't the best\npossible outcome, even with the invisible hand. So we'll talk about\ndifferent kinds of markets. Then we'll put it together\nto get market equilibrium, and talk about\nSmith's principles. And then from there, we'll\ntalk about how it breaks down in reality, different\nchange in reality, how there are various\nmarket failures that can get in the way, why we\nhave to care about equity and what implications that has,\nabout behavioral economics, about a set of other factors. So that's basically how we're\ngoing to proceed this semester. As I said, the\nlectures are important, but the recitations are as well. Once we're sort of\nin steady state, the recitations will be about\nhalf new material and half working through problems\nto help you prepare for that next problem set. So the way the problem\nsets are going to work is the problem set\nthat's assigned will cover material that's\ntaught up to that date. So for example, problem\nset one is going to be assigned next Friday. That will cover everything\nyou've learned up through next Wednesday. Therefore, in section\non next Friday, we'll do a practice problem\nwhich you should understand because it'll cover things\nthat were taught in class, and help prepare you\nfor the problems. And we'll do that every week. That's about half the section. The other half of the\nsection will be new material. This Friday, the section on\nFriday is all new material. What we do on Friday is\ncover the mathematics. I don't like doing math. I always get it wrong. So I leave math for the TAs,\nwho are smarter than I am. So this Friday, we'll be doing\nthe mathematics of supply and demand, and how you\ntake the intuition here and the simple\ngraphics, and actually turn it into mathematical\nrepresentations, which is what you need for the problem sets. That's this Friday. Then we'll come back\non Monday and start talking about what's\nunderneath the demand curve. All right, any other questions? I'll see you on Monday."}], "1. Course Introduction and Newtonian Mechanics": [{"content": "Professor Ramamurti\nShankar: This is a first part of the year-long course\nintroducing you to all the major ideas in physics,\nstarting from Galileo and Newton right up to the big\nrevolutions of the last century, which was on relativity and\nquantum mechanics. The target audience for this\ncourse is really very broad. In fact, I've always been\nsurprised at how broad the representation is. I don't know what your major is; I don't know what you are going\nto do later so I picked the topics that all of us in physics\nfind fascinating. Some may or may not be useful,\nbut you just don't know. Some of you are probably going\nto be doctors and you don't know why I'm going to do special\nrelativity or quantum mechanics, but you don't know when it will\ncome in handy. If you're a doctor and you've\ngot a patient who's running away from you at the speed of light,\nyou'll know what to do. Or, if you're a pediatrician\nwith a really small patient who will not sit still,\nit's because the laws of quantum mechanics don't allow an\nobject to have a definite position and momentum. So these are all things you just don't know when they will\ncome in handy, and I teach them because these\nare the things that turn me on and got me going into physics\nand whether or not you go into physics,\nyou should certainly learn about the biggest and most\ninteresting revolutions right up to present day physics. All right. So that's what the subject\nmatter's going to be, and I'm going to tell you a\nlittle bit about how the course is organized. First thing is, this year it's going to be\ntaped. You can see some people in the\nback with cameras as part of an experimental pilot program\nfunded by the Hewlett Foundation and at some point they will\ndecide what they will do with these lectures. Most probably they'll post them somewhere so people elsewhere\ncan have the benefit of what you have sitting in the classroom. So I've been told that from now on we just ignore the camera and\ndo business as usual. Nothing's going to be changed. I tried to negotiate a laugh track so that if the jokes don't\nwork we can superimpose some laughter. I was told \"no.\" I just got to deal with it as\nit happens. So it's going to be--it's like\none of the reality shows where things are going to be as they\nare and hopefully after a while we'll learn to act and behave\nnormally and not worry about its presence. Then, coming to the rest of the details of the course. By the way, there are more details on the website that I\nposted, that was given to me by the university,\nif you want to know more about what all this is about. The course organization is fairly simple. We're going to meet Monday and Wednesday in this room,\n11:30-12:45. I will give you some problems\nto do on Wednesday and I'll post them on the website. You guys should get used to going to the class' website. I'm really, really dependent on that now. I finally learned how to use it. I will use that to post\ninformation, maybe once in a while send e-mail to the whole\nclass. If you want to get those\ne-mails, you got to sign up for the course because I push a\nbutton and it goes to anybody who's signed up there. The homework will be given on Wednesday and it's due before\nclass the following Wednesday. Let me introduce you to our\nhead TA, Mara Daniel, who's recently Mara Baraban. So Mara's going to be the\nperson who will see you after class and she will take the\nproblem sets that you have submitted before class and\nshe'll give you the graded ones after class. Okay? That will be sorted up,\nit'll be up there. So you should drop the homework\nbefore you come into class, rather than furiously work on\nit during class, and the solutions will be\nposted the same afternoon. So there is not much point in\ngiving homework that's late. But once in a while,\nyou know, you will come up with a reason that I just cannot\nargue with. You got married,\nyou're getting a transplant, whatever it is. That's fine. You got a transplant,\nI want to see the old body part. You got married, I want to see your spouse. If something happened to a grandparent, I'm counting. Up to four I don't get suspicious. Go five, six, seven, eight,\nI will have to look into the family tree."}, {"content": "But, you know, any reasonable excuse will be\nentertained. Relative importance given to\nthese different things, there's 20% for your homework,\n30% for the Midterm, which will be sometime in\nOctober, and 50% for the Final. That'll be the weighted average. But I have another plan called the \"Amnesty Plan\" in which I\nalso compare just your final grade,\nwhat you did on the Final exam, and whichever is higher of the\ntwo is what I will take to determine your overall course\ngrade. This is something I used to\nannounce near the end but then some people felt that it's not\nfair not to know this from the beginning. So, I'm telling you from the beginning, but don't dream and\nthink that somehow the Final's going to be so much different\nfrom your regular day-to-day performance,\nbut to give you some reason to live after the Midterm. So, you feel there is hope. I can change everything\novernight; it does happen. I put that in for a reason because sometimes some of you\nhave not taken a physics course and you don't know how to do\nwell in physics and slowly you catch on and by the time it's\nFinal exam you crack the code; you know how to do well. As far as I'm concerned, that's just fine. If at the end of the semester you take a three-hour exam in a\nclosed environment and you answer everything,\nI don't care what you did in your homework or your Midterm. That's not relevant."}, {"content": "So that's how the grading will\nbe done. We have Mara's group of TAs. She is the head TA and she's the one you should write to\nwhenever you have a problem. Then we also have two faculty\nmembers. One is a Postdoctoral Fellow,\nMark Caprio. So he will have a discussion\nsection on Tuesdays between 1:00-2:00 in Sloane Lab. And Steve Furlanetto--I don't know if Steve is here or not. There's Steve, our new Assistant Professor. He will have his section on Tuesday night in Dunham Lab,\nin Room 220. Tuesday night is the night when\nyou people realize homework is due on Wednesday. So we know that, so he will be there to comfort\nyou and give you whatever help you need. All right. My own office hours I've not\ndetermined yet. I will have to find out when it\nis good for you. You know, I live and work out\nof Sloane Lab up on the hill and it was easy to have office hours\nbefore or after class but now you have to make a special trip. So, just give me a little bit of time to find out maybe by\nsoliciting e-mail responses from you what would be a good time\nfor my office hours. But for any procedural things,\nlike, you know, this problem set was not graded\nproperly, and so on, there's no point\ne-mailing me because I'm going to send it to Mara anyway. So directly deal with the powers that be. Okay, finally I want to give you some tips on how to do well\nin this course and what attitude you should have. First, I advise that you should come to the lectures. It's not self-serving; it's not so much for my benefit. I think there is something useful about hearing the subject\npresented once orally. Secondly, the book,\nyou can see, one of you had a book here,\nit's about 1,100 pages and when I learned physics it was,\nlike 300 pages. Now, I look around this room,\nI don't see anybody whose head is three times bigger than mine,\nso I know that you cannot digest everything the books\nhave. So I have to take out what I\nthink is the really essential part and cover them in the\nlecture. So, you come to class to find\nout what's in and what's not in. If you don't do that,\nthere's a danger you will learn something you don't have to,\nand we don't want that. Okay, so that's why you come to\nclass. Second thing,\nmost important thing for doing well in physics,\nis to do the homework. The 20% given to the homework\nis not a real measure of how important it is. Homework is when you really figure out how much you know and\ndon't know. If you watch me do the thing on\nthe blackboard, it looks very reasonable."}, {"content": "It looks like you can do it but the only way you're going to\nfind out is when you actually deal with the problem. That's the only time you're going to find out. So, I ask you to do the problems as and when they're\nposted. So if I post it on Wednesday to\ncover the material for that week, then you should attempt it\nas quickly as possible because I'm going to assume you have\ndone the problems when you come for the next few lectures. And in doing the homework, it is perfectly okay to work in\ngroups. You don't have to do it by\nyourself. That's not how physics is done. I am now writing a paper with two other people. They are my experimental colleagues who write papers with\n400 other people, maybe even 1,000 other people. When they do the big collider experiments in Geneva or\nFermilab, collaborations can run into hundreds. So, it's perfectly okay to be part of a collaboration,\nbut you've got to make sure that you're pulling your weight. You've got to make sure that if you explain to others how to do\nthis problem, then somebody else contributes\nto something else, but you know what everybody\ncontributed in the end. So the game is not just to\nsomehow or other get the solution to the problem set but\nto fully understand how it's done,\nand the TAs will be there to help you. Every day there's going to be a TA in the undergraduate lounge. I would urge you to use that. That's a beautiful new lounge\nthat the Provost's Office allowed us to build for\nphysicists and chemists, or whoever happens to be in the\nbuilding. If you go there on the third\nfloor of Sloane, you may run into other people\nlike you who are trying to work on problems. You may run into upper-class students, students who are more\nadvanced, you will run into your TA. So that's a good climate. There are coffee machines and\nthere are lounge sofas and everything else. There are computers, there are printers,\nso it's a good lounge, and I think if you go there one\nday a week to do your problem sets,\nmore often that's a good meeting place,\nI recommend that. The final piece of advice,\nthis is very important so please pay attention to this,\nwhich is, I ask you not to talk to your neighbors during\nlecture. Now, this looks like a very\ninnocuous thing, but you will find out,\nit is the only thing that really gets my back up. Most of the time I don't really care. I'm really liberal, but this disturbs me because I\nam looking at you, I'm trying to see from your\nreaction how much of my lecture you are following,\nand then it's very distracting when people are talking. So please don't do that. If you talk,\nI am going to assume you are talking about me. If you laugh, I'm going to assume you are\nlaughing at me. That's not really what I think,\nbut that's how disturbing it is when people talk,\nand very nice students who do not realize this often disrupt\nmy line of thinking. So I ask you to keep that to a\nminimum. Once in a while you'll have to\ntalk to your neighbor and say, \"Can you please pass me my\npacemaker that fell down?\" That's fine. Then you go back to your business."}, {"content": "But don't do too much of that. Finally, there is this ancient\nissue about sleeping in class. Now, my view is,\nit's just fine, okay. I know you guys need the rest and interestingly,\nthe best sleepers are in the first couple of rows. I haven't met you guys. It's not personal. I have found some people really have to come to the first and\nsecond row because they claim that if they don't hear me they\ncannot really go to sleep. Now, that was true in Sloane\nbut I think Luce has got very good acoustics so you can\nstretch out in the back. But my only criterion is if you\ntalk in your sleep, now that's not allowed because\ntalking is not allowed. Next, if you're going to sleep,\nI ask you to sit between two non-sleepers because sometimes\nwhat happens, the whole row will topple over. We don't want the domino effect. Now, it's going to be captured\non tape and that's going to be really bad for my reputation,\nso spread yourself around other people. All right. So that's it in terms of class,\nyou know, logistics and everything. I'm going to start going into the physics proper. I will try to finish every lecture on time,\nbut sometimes if I'm in the middle of a sentence or the\nmiddle of a derivation, I may have to go over by a\ncouple of minutes; there's no need to shuffle your\nfeet and move stuff around. I know what time it is. I also want to get out like you guys, but let me finish\nsomething. Other days I may finish a few\nminutes before time. That's because the ideas of\nphysics don't fall into 75-minute segments and sometimes\nthey spill over a little bit. Also, I'm used to teaching this\ncourse three times a week and now it's suddenly twice a week,\nand so things that fell into nice 50-minute units are now\nbeing snipped up different ways so it's pretty difficult. So, even for me, some of it will be new and the\ntiming may not be just right. I should tell you first of all\nthat in this class, the taping is not going to\naffect you because the camera is going to be behind your head. I mentioned to you in the website that this is not the big\nopportunity you've been looking for to be a star. Only the back of your head will be seen. In some cases, the back of the head could be\nmore expressive than the front, in which case this is your\nopportunity and I wish you luck. But otherwise,\njust don't worry about it because you will be only heard."}, {"content": "You may not even be heard. So, I've been asked that if a\nquestion is not very clear, I should repeat it so that\npeople listening to it later will know what the question was. Let me make one thing very\nclear. That is, I'm not in favor of\nyour talking to each other because you're distracting. Your stopping me at any time is just fine. I welcome that because I've seen this subject for God knows\nhow many years. The only thing that makes it\ndifferent for me is the questions that you people have. You can stop me any time and you should not feel somehow you\nare stopping the progress of the class. There is no fixed syllabus. We can move things around and\nit's far more exciting for me to answer your questions than to\nhave a monologue."}, {"content": "So, don't worry about that. So stop me anytime you don't follow something,\nand don't assume that you're not following something because\nthere's something wrong with your level of comprehension. Quite often, you guys come up with questions\nthat never cross my mind, so it's very interesting."}, {"content": "And things we've been repeating year after year after year,\nbecause they sound so reasonable,\nsuddenly sound unreasonable when some of you point out some\naspect of it that you didn't follow. So, it could be very interesting for all of us to\nhave issues to discuss in class, and quite often some questions\nare very common and your classmates will be grateful to\nyou that you brought it up. Otherwise, you know,\nTAs get ten e-mails, all with the same question. Okay."}, {"content": "So I'm going to start now. Anybody have any questions about class? The format? The Midterm? The exams? All right."}, {"content": "Yes? Student:\nYou said there's going to be two hours to be announced. How do we wait for [inaudible] Professor Ramamurti\nShankar: Oh, you mean my office hours? Student: No. I thought there was an\n[inaudible] Professor Ramamurti\nShankar: No, the discussion sections are\nTuesday afternoon from 1:00-2:00,\nand Tuesday night from 8:00-10:00, and the website has\ngot all the details on when and where. Yes? Student:\nSo the lab times will still be [inaudible]\nProfessor Ramamurti Shankar: Yeah. There are many, many lab times and you have to\ngo to the website for the lab. And, by the way,\nthat reminds me. I've got here lots of flyers\ngiven to me by the director of the laboratories which will tell\nyou which lab is the right lab for you,\nand they're offered many times a week. Yes? Student:\nAs far as knowing the material, just from your class,\nhow important is taking a lab concurrent with this class? Professor Ramamurti Shankar: I think it's a good\nidea to take the lab, particularly in this particular\nclass because I don't have any demonstrations. They're all in the other building. So, this will remind you that physics is, after all,\nan experimental science and you will be able to see where all\nthe laws of physics come from. So, if you're going to take it,\nyou should take it at the same time. Yes? Student:\nCould you please talk about when you expect [inaudible]\nProfessor Ramamurti Shankar: Ah,\nvery good. This is a calculus-based class\nand I expect everyone to know at least the rudiments of\ndifferential calculus. What's a function,\nwhat's a derivative, what's a second derivative,\nhow to take derivatives of elementary functions,\nhow to do elementary integrals. Sometime later,\nI will deal with functions of more than one variable,\nwhich I will briefly introduce to you,\nbecause that may not be a prerequisite but certainly\nsomething you will learn and you may use on and off. But there are different ways of doing physics. Mine is to demonstrate over and over how little mathematics you\nneed to get the job done. There are others who like to\nshow you how much mathematics you could somehow insinuate into\nthe process, okay. There are different ways of\nplaying the game, and some of us find great pride\nin finding the most simple way to understand something. That's certainly my trademark; that's how I do my research\nalso. So, if you feel there's not\nenough math used, I guarantee you that I\ncertainly know enough eventually to snow the whole class,\nbut that's not the point. I will use it in moderation and\nuse it to the best effect possible rather than use it\nbecause it is there. Okay. So I don't know your mathematical background,\nbut the textbook has an appendix, which is a reasonable\nmeasure of how much math you should know. You've got to know your trigonometry,\nyou've got to know what's a sine and what's a cosine. You cannot say, \"I will look it up.\"\nYour birthday and social security number is what you look\nup. Trigonometry functions you know\nall the time. Okay."}, {"content": "I will ask you, and you do."}, {"content": "All right. And of course, there's trigonometric\nidentities you know from high school. Pages and pages of them, so no one expects you to know\nall those identities, but there are a few popular\nones we will use. All right."}, {"content": "Anything else?"}, {"content": "Yes? Student: This may be a bit early,\nbut when will we be having our Midterm? Professor Ramamurti Shankar: Yeah. Midterm will be sometime around 20th of October. I have to find out exactly the right time. We have 24 lectures for this class and the first 12 roughly\nwill be part of the Midterm, but after the 12th lecture I\nmay wait a week so that you have time to do the problems and get\nthe solutions. Then I will give you the\nMidterm. Yes? Student: If wanting one of the two lab\ncourses, which one do you recommend? Professor Ramamurti Shankar: Yeah,\nthis tells you in detail. This flyer answers exactly that. Okay, there was one more question from somebody?"}, {"content": "Yes? Student:\nA few people I've talked to have recommended that we start\ntaking the lab second semester instead of first. Would that be advisable or should we take both\nconcurrently? Professor Ramamurti\nShankar: I don't have a strong view. I think you should take the lab sometime but I don't know how\nmany semesters that you have to take. But I would say the advice of your predecessors is very\nimportant. If they tell you this is what\nworks, that's better than what somebody like me can tell you. Also, you should talk to Stephen Irons,\nwho is the director of the labs. He has seen every possible situation."}, {"content": "He will give you good advice. Let's start now."}, {"content": "Okay. So we are going to be studying\nin the beginning what's called Newtonian mechanics. It's pretty remarkable that the whole edifice is set up by just\none person \u2013 Newton -- and he sent us on the road to\nunderstanding all the natural phenomena until the year\n18-hundred-and-something when Maxwell invented the laws of\nelectromagnetism and wrote down the famous Maxwell equations. Except for electromagnetism, the basics of mechanics,\nwhich is the motion of billiard balls and trucks and marbles and\nwhatnot, was set up by Newton. So that's what we are going to\nfocus on, and you will find out that the laws of physics for\nthis entire semester certainly can be written on one of those\nblackboards or even half of those blackboards. And the purpose of this course is to show you over and over and\nover again that starting with those one or two laws,\nyou can deduce everything, and I would encourage you to\nthink the same way. In fact, I would encourage you\nto think the way physicists do, even if you don't plan to be a\nphysicist, because that's the easiest way\nto do this subject, and that is to follow the\nreasoning behind everything I give you. And my purpose will be not to say something as a postulate,\nbut to show you where everything comes from,\nand it's best for you if you try to follow the logic. That way, you don't have to store too many things in your\nhead. In the early days when there\nare four or five formulas, you could memorize all of them\nand you can try each one of them until something works,\nbut after a couple of weeks you will have a hundred formulas and\nyou cannot memorize all of them. You cannot resort to trial and\nerror. So you have to know the logic. So the logical way is not just the way the physicists do it,\nit's the easier way to do it. If there is another way that it\nwill work for non-physicists, I won't hesitate to teach it to\nyou that way if that turns out to be the best way. So try to follow the logic of everything. Okay. So, Newtonian mechanics is our\nfirst topic. So, Newtonian mechanics has two\nparts. All of physics is a two-part\nprogram. The plan, every time,\nis to predict the future given the present. That's what we always do. When we do that right,\nwe are satisfied. So the question is,\n\"What do you mean by \u2018predict the future?'\"\nWhat do you mean by the future? What do you mean by the present? By \"present,\" we mean--we will pick some part of the universe\nwe want to study and we will ask,\n\"What information do I need to know for that system at the\ninitial time, like,\nright now, in order to be able to predict the future?\"\nSo, for example, if you were trying to study the\nmotion of some object, here is one example. [throws a piece of candy for someone to catch]\nProfessor Ramamurti Shankar: See,\nthat's an example of Newtonian mechanics."}, {"content": "I'll give you one more demonstration. Let's see who can catch this one. [throws another piece] Professor Ramamurti\nShankar: That's a good example. So, that was Newtonian mechanics at work,\nbecause what did I do? I released a piece of candy,\nthrew it from my hand, and the initial conditions have\nto do with where did I release it and with what velocity. That's what he sees with his eyes. Then that's all you really need to know. Then he knows it's going to go up, it's going to curve,\nfollow some kind of parabola, then his hands go there to\nreceive it. That is verification of a\nprediction. His prediction was,\nthe candy's going to land here, then he put his hand there. He also knew where the candy was going to land,\nbut he couldn't get his hand there in time. But we can always make predictions. But this is a good example of what you need to know. What is it you have to know about this object that was\nthrown, I claim, is the initial location of the\nobject and the initial velocity. The fact that it was blue or\nred is not relevant, and if I threw a gorilla at him\nit doesn't matter what the color of the gorilla is,\nwhat mood it is in. These are things we don't deal\nwith in physics. There is a tall building,\na standard physics problem. An object falls off a tall\nbuilding. Object could be a person. So we don't ask why is this guy ending it all today? We don't know, and we cannot deal with that. So we don't answer everything. We just want to know when he's\ngoing to hit the pavement, and with what speed. So we ask very limited questions, which is why we brag\nabout how accurately we can predict the future. So, we only ask limited goals and we are really successful in\nsatisfying them. So, we are basically dealing\nwith inanimate objects. So the product of Newtonian\nmechanics of predicting the future given the present,\nhas got two parts, and one is called kinematics\nand the other is called dynamics. So, kinematics is a complete description of the present. It's a list of what you have to know about a system right now. For example, if you're talking about the\nchalk--if I throw the chalk, you will have to know where it\nis and how fast it's moving. Dynamics then tells you why the\nobject goes up, why the object goes down and\nwhy is it pulled down and so on. That's dynamics. The reason it comes down is gravity is pulling it. In kinematics, you don't ask the reason behind\nanything. You simply want to describe\nthings the way they are and then dynamics tells you how they\nchanged and why they changed. So, I'm going to illustrate the\nidea of kinematics by taking the simplest possible example. That's going to be the way I'm going to do everything in this\ncourse. I'm going to start with the\nsimplest example and slowly add on bells and whistles and make\nit more and more complicated. So, some of you might say,\n\"Well, I've seen this before,\" so maybe there is nothing new\nhere."}, {"content": "That may well be. I don't know how much you've seen, but quite often the way\nyou learned physics earlier on in high school is probably\ndifferent from the way professional physicists think\nabout it. The sense of values we have,\nthe things that we get excited about are different,\nand the problems may be more difficult. But I want to start in every example, in every situation that\nI explain to you, with the simplest example,\nand slowly add on things. So, what we are going to study\nnow is a non-living object and we're going to pick it to be a\nmathematical point. So the object is a mathematical\npoint. It has no size. If you rotate it, you won't know. It's not like a potato."}, {"content": "You take a potato,\nyou turn it around, it looks different. So, it's not enough to say the potato is here. You've got to say which way the nose is pointing and so on. So, we don't want to deal with that now. That comes later when we study what we call \"rigid bodies\". Right now, we want to study an entity which has no spatial\nextent. So just a dot,\nand the dot can move around all over space. So we're going to simplify that too. We're going to take an entity that lives along the x\naxis. [draws a line with integrals]\nIt moves along a line. So you can imagine a bead with\na wire going through it and the bead can only slide back and\nforth. So, this is about the simplest\nthing. I cannot reduce the number of\ndimensions. One is the lowest dimension. I cannot make the object simpler than being just a\nmathematical point. Then, you've got to say,\n\"What do I have to know about this object at the initial time? What constitutes the present, or what constitutes maximal\ninformation about the present?\" So what we do is we pick an\norigin, call it zero, we put some markers there to\nmeasure distance, and we say this guy is sitting\nat 1,2, 3,4, 5. He is sitting at x = 5. Now, of course, we've got to have units and the\nunits for lengths are going to be meters. The unit for time will be a second, and time will be\nmeasured in seconds. Then we'll come to other units. Right now, in kinematics, this is all you need. Now, there are some tricky problems in the book. Sometimes they give you the speed in miles per hour,\nkilometers per year, pounds per square foot,\nwhatever it is. You've got to learn to\ntransform them, but I won't do them."}, {"content": "I think that's pretty elementary stuff. But sometimes I might not write the units but I've earned the\nright to do that and you guys haven't so you'll have to keep\ntrack of your units. Everything's got to be in the\nright units. If you don't have the units,\nthen if you say the answer is 19, then we don't know what it\nmeans. Okay."}, {"content": "So here's an object. At a given instant,\nit's got a location. So what we would like to do is\nto describe what the object does by drawing a graph of time\nversus space and the graph would be something like this. You've got to learn how to read this graph. I'm assuming everyone knows how to read it. [draws a graph of x versus t]\nThis doesn't mean the object is bobbing up and down. I hope you realize that. Even though the graph is going\nup and down, the object is moving from left to right. So, for example, when it does this,\nit's crossed the origin and is going to the left of the origin. Now, at the left of the origin, it turns around and starts\ncoming to the origin and going to the right. That is x versus t. So, in the language of calculus, x is a function\nof time and this is a particular function. This function doesn't have a name. There are other functions which have a name. For example, this is x = t,\nx = t^(2), you're going to have x = sin\nt and cos t and log t. So some functions have a name, some functions don't have a\nname. What a particle tries to do\ngenerally is some crazy thing which doesn't have a name,\nbut it's a function x (t). So you should know when you look at a graph like this what\nit's doing. So, the two most elementary\nideas you learn are what is the average velocity of an object,\nas then ordered by the symbol v-bar. So, the average is found by taking two instants in time,\nsay t_1 and later t_2,\nand you find out where it was at t_2 minus\nwhere it was at t_1 and divide\nby the time. So, the average velocity may\nnot tell you the whole story. For example,\nif you started here and you did all this and you came back here,\nthe average velocity would be zero, because you start and end\nat the same value of x, you get something;\n0 over time will still be 0. So you cannot tell from the\naverage everything that happened because another way to get the\nsame 0 is to just not move at all. So the average is what it is. It's an average,\nit doesn't give you enough detail. So it's useful to have the average velocity. It's useful to have the average acceleration,\nwhich you can find by taking similar differences of\nvelocities. But before you even do that,\nI want to define for you an important concept,\nwhich is the velocity at a given time, v (t). So this is the central idea of calculus, right? I am hoping that if you learned your calculus,\nyou learned about derivatives and so on by looking at x\nversus t. So, I will remind you,\nagain, this is not a course in calculus. I don't have to do it in any detail. I will draw the famous picture of some particle moving and it's\nhere at t of some value of x. A little later, which is t + \u0394t. So \u0394t is going to stand always for a small finite\nintegral of time; infinitesimal interval of time\nnot yet 0. So, during that time,\nthe particle has gone from here to there, that is x +\n\u0394x, and the average velocity in that interval is\n\u0394 x/ \u0394t. Graphically,\nthis guy is \u0394 x and this guy is \u0394t,\nand \u0394x over \u0394t is a ratio. So in calculus,\nwhat you want to do is to get the notion of the velocity right\nnow. We all have an intuitive notion\nof velocity right now. When you're driving in your\ncar, there's a needle and the needle says 60;\nthat's your velocity at this instant. It's very interesting because velocity seems to require two\ndifferent times to define it -- the initial time and the final\ntime. And yet, you want to talk about\nthe velocity right now. That is the whole triumph of\ncalculus is to know that by looking at the position now,\nthe position slightly later and taking the ratio and bringing\nlater as close as possible to right now,\nwe define a quantity that we can say is the velocity at this\ninstant. So v of t,\nv(t) is the limit, \u0394t goes to 0 of\n\u0394x over \u0394t and we use the symbol dx/dt\nfor velocity. So technically,\nif you ask what does the velocity stand for--Let me draw\na general situation. If a particle goes from here to\nhere, \u0394x over \u0394t, I don't know how\nwell you can see it in this figure here,\nis the slope of a straight line connecting these two points,\nand as the points come closer and closer,\nthe straight line would become tangent to the curve. So the velocity at any part of the curve is tangent to the\ncurve at that point. The tangent of,\nthis angle, this \u03b8, is then \u0394x over \u0394t. Okay, once you can take one derivative,\nyou can take any number of derivatives and the derivative\nof the velocity is called the acceleration,\nand we write it as the second derivative of position. So I'm hoping you guys are comfortable with the notion of\ntaking one or two or any number of derivatives. Interestingly, the first two derivatives have\na name. The first one is velocity,\nthe second one is acceleration. The third derivative,\nunfortunately, was never given a name,\nand I don't know why. I think the main reason is that\nthere are no equations that involve the third derivative\nexplicitly. F = ma. The a is this fellow here, and nothing else is given\nan independent name. Of course, you can take a\nfunction and take derivatives any number of times. So you are supposed to know, for example,\nif x(t) is t^(n), you're supposed to know\ndx/dt is nt^(n-1). Then you're supposed to know\nderivatives of simple functions like sines and cosines. So if you don't know that then, of course, you have to work\nharder than other people. If you know that,\nthat may be enough for quite some time. Okay, so what I've said so far\nis, a particle moving in time from point to point can be\nrepresented by a graph, x versus t. At any point on the graph you can take the derivative,\nwhich will be tangent to the curve at each point,\nand its numerical value will be what you can call the\ninstantaneous velocity of that point and you can take the\nderivative over the derivative and call it the acceleration. So, we are going to specialize to a very limited class of\nproblems in the rest of this class. A limited class of problems is one in which the acceleration is\njust a constant. Now, that is not the most\ngeneral thing, but I'm sure you guys have some\nidea of why we are interested in that. Does anybody know why so much time is spent on that? Yes? Student:\n[inaudible] Professor Ramamurti\nShankar: Pardon me?"}, {"content": "Student:\n[inaudible] Professor Ramamurti\nShankar: Right. The most famous example is that\nwhen things fall near the surface of the Earth,\nthey all have the same acceleration,\nand the acceleration that's constant is called g,\nand that's 9.8 meters/second^(2). So that's a very typical problem. When you're falling to the surface of the Earth,\nyou are describing a problem of constant acceleration. That's why there's a lot of emphasis on sharpening your\nteeth by doing this class of problems. So, the question we are going to ask is the following,\n\"If I tell you that a particle has a constant acceleration\na, can you tell me what the\nposition x is?\" Normally, I will give you a\nfunction and tell you to take any number of derivatives. That's very easy. This is the backwards problem. You're only given the particle has acceleration a,\nand you are asked to find out what is x? In other words, your job is to guess a function\nwhose second derivative is a,\nand this is called integration, which is the opposite of\ndifferentiation, and integration is just\nguessing. Integration is not an\nalgorithmic process like differentiation. If I give you a function, you know how to take the\nderivative. Change the independent\nvariable, find the change in the function, take the ratio and\nthat's the derivative. The opposite is being asked\nhere."}, {"content": "I tell you something about the\nsecond derivative of a function and ask you what is the\nfunction. The way we do that is we guess,\nand the guessing has been going on for 300 years,\nso we sort of know how to guess. So, let me think aloud and ask how I will guess in this\nproblem. I would say,\nokay, this guy wants me to find a function which reduces to the\nnumber a when I take two derivatives,\nand I know somewhere here, this result,\nwhich says that when I take a derivative,\nI lose a power of t. In the end, I don't want any\npowers of t. It's very clear I've got to\nstart with a function that looks like t^(2). This way when I take two derivatives, there will be no\nt left. Well, unfortunately,\nwe know this is not the right answer, because if you take the\nfirst derivative, I get 2t. If I take the second derivative I get 2, but I want to get\na and not 2. Then it's very clear the way\nyou patch it up is you multiply it by this constant and now\nwe're all set. This function will have the\nright second derivative. So, this certainly describes a\nparticle whose acceleration is a. The a is not dependent on time. But the question is, is this the most general\nanswer, or is it just one answer, and I think you all know\nthat this is not the most general answer. It is one answer. But I can add to this some\nnumber, like 96, that'll still have the property\nthat if you take two derivatives,\nyou're going to get the same acceleration. So 96 now is a typical constant, so I'm going to give\nthe name c to that constant. Everyone knows from calculus that if you're trying to find a\nfunction about which you know only the derivative,\nyou can always add a constant to one person's answer without\nchanging anything. But I think here,\nyou know you can do more, right? You can add something else to the answer without invalidating\nit, and that is anything with one power of t in it, because if you take one\nderivative it'll survive, but if you take two\nderivatives, it'll get wiped out. Now, it's not obvious but it is true that you cannot add to this\nanymore. The basic idea in solving these\nequations and integrating is you find one answer,\nso then when you take enough derivatives, the function does\nwhat it's supposed to do. But then having found one\nanswer, you can add to it anything that gets killed by the\nact of taking derivatives. If you're taking only one\nderivative you can add a constant. If you're taking two derivatives you can add a\nconstant and something linear in t.. If you knew only the third derivative of the function,\nyou can have something quadratic in t without\nchanging the outcome. So, this is the most general\nposition for a particle of constant acceleration,\na. Now, you must remember that this\ndescribes a particle going side to side. I can also describe a particle going up and down. If I do that, I would like to call the\ncoordinate y, then I will write the same\nthing. You've got to realize that in\ncalculus, the symbols that you call x and y are\ncompletely arbitrary. If you know the second\nderivative of y to be a, then the answer looks\nlike this. If you knew the second\nderivative of x, the answer looks like that. Now, we have to ask what are these numbers,\nb and c. So let me go back now to this\nexpression, x(t) = at^(2)/ 2 + c +\nbt. It is true mathematically,\nyou can add two numbers, but you've got to ask yourself,\n\"What am I doing as a physicist when I add these two numbers?\"\nWhat am I supposed to do with a and b? I mean, with the b and c?"}, {"content": "What value should I pick? The answer is that simply\nknowing the particle has an acceleration is not enough to\ntell you where the particle will be. For example, let's take the case where the\nparticle is falling under gravity. Then you guys know, you just told me,\nacceleration is -9.8, my g is -9.8. We call it \"minus\" because it's accelerating down and up was\ntaken to be the positive direction. In that case, y(t) will be\n-1/2gt^(2) + c + bt. So, the point is,\nevery object falling under gravity is given by the same\nformula, but there are many, many objects that can have many\nhistories, all falling under gravity, and what's different\nfrom one object and the other object is,\nwhen was it dropped, from what height,\nand with what initial speed. That's what these numbers are\ngoing to tell us and we can verify that as follows. If you want to know what the number c is,\nyou say, let's put time t = 0. In fact,\nlet me go back to this equation here. You'll put time t = 0, x(0) doesn't\nhave this term, doesn't have this term,\nand it is c. So I realize that the constant,\nc, is the initial location of the object,\nand it's very common to denote that by x_0. So\nthe meaning of the constant c is where was the object at the\ninitial time? It could've been anywhere. Simply knowing the acceleration is not enough to tell you where\nit was at the initial time. You get to pick where it was at\nthe initial time. Then, to find the meaning of\nb, we take one derivative of this, dx/dt,\nthat's velocity as a function of time, and if you took the\nderivative of this guy, you will find as at + b. That's the velocity of the object. Then, you can then understand that v(0) is what\nb is, which we write as v_0. Okay,\nso the final answer is that x(t) looks like\nx_0 + v_0 t + 1/2 at^(2). Okay. So what I'm saying here is we are specializing to a limited\nclass of motion where the particle has a definite\nacceleration, a. Then, in every situation where the body has an acceleration\na, the location has to have this form,\nwhere this number (x_0) is where\nit was initially, this (v_0 )\nwas the initial velocity of the object. So, when I threw that thing up and you caught it,\nwhat you are doing mentally was immediately figuring out where\nit started and at what speed. That was your initial data. Then in your mind, without realizing it,\nyou found the trajectory at all future times. Now, there is one other celebrated formula that goes\nwith this. I'm going to find that,\nthen I'll give you an example. Now, I'm fully aware that this\nis not the flashiest example in physics, but I'm not worried\nabout that right now. You'll see enough things that\nwill confound you, but right now I want to\ndemonstrate a simple paradigm of what it means to know the\npresent and what it means to say this is what the future behavior\nwill be. We want to do that in the\nsimplest context, then we can make the example\nmore and more complicated, but the phenomenon will be the\nsame."}, {"content": "So, what we have found out so\nfar, I'm purposely going from x to y because I\nwant you to know that the unknown variable can be called\nan x or can be called a y. It doesn't matter, as long as the second\nderivative is a; that's the answer. Now there's a second formula one derives from this."}, {"content": "You guys probably know that too from your days at the daycare,\nbut I want to derive the formula and put it up,\nthen we'll see how to use it. Second formula tries to relate\nthe final velocity of some time, t, to the initial\nvelocity and the distance traveled with no reference to\ntime. So the trick is to eliminate\ntime from this equation. So let's see how we can\neliminate time. You know that if you took a\nderivative of this, you will find v(t) is\nv_0 + at. What that means is,\nif you know the velocity of the given time and you know the\ninitial velocity, you know what time it is. The time, in fact, is v - v_0\nover a. If I don't show you any\nargument for v, it means v at time\nt and the subscript of 0 means t is zero. So what this says is, you can measure time by having\nyour own clock. A clock tells you what time it\nis, but you can also say what time it is by seeing how fast\nthe particle is moving because you know it started with some\nspeed. It's gaining speed at some rate\na. So, if the speed was so and so\nnow, then the time had to be this. So time can be indirectly inferred from these quantities. Then you take that formula here (t) and you put it here,\n(y(t)) to see a times t,\nyou put this expression. So what will you get? We'll get an expression in which there is no t;\nt has been banished in favor of v. So, I'm not going to waste your time by asking what happens if\nyou put it in. I will just tell you want\nhappens. What happens is,\nyou will find that v^(2) = v_o^(2) + 2a times\n(y- y_0). [Note: The Professor said x\nwhen he meant y] How many people have seen this\nthing before?"}, {"content": "Okay."}, {"content": "That's a lot. Look, I know you've seen this. At the moment, I have to go through some of\nthe more standard material before we go to the more\nnon-standard material. If this part's very easy for\nyou, there's not much I can do right now. So let me draw a box. Drawing a box to you guys means\nimportant. These are the two important\nthings."}, {"content": "Remember, I want you to\nunderstand one thing. How much of this should you\nmemorize? Suppose you've never seen this\nin high school. How much are you supposed to\nmemorize? I would say,\nkeep that to a minimum, because what the first formula\ntells you should be so intuitive that you don't have to cram\nthis. We are talking about particles\nof constant acceleration. That means, when I take two\nderivatives, I want to get a, then you should know\nenough calculus to know it has to be something like\nat^(2), and half comes from taking two\nderivatives. The other two you know are\nstuff you can add, and you know where you're\nadding those things, because the particle has a head\nstart. It's got an initial position. Even at = 0, and it has an initial velocity,\nso even without any acceleration,\nit will be moving from y^(0) to y^(0) + vt. The acceleration gives you an extra stuff, quadratic in time. Once you've got that, one derivative will give you\nthe velocity, then in a crunch you can\neliminate t and put it into this formula. But most people end up memorizing these two because you\nuse it so many times. It eventually sticks in you but\nyou shouldn't try to memorize everything. So, we are now going to do one standard problem where we will\nconvince ourselves we can apply this formulae and predict the\nfuture given the present. So the problem I want to\ndo--there are many things you could do but I just picked one,\nand this is the one with round numbers so I can do it without a\ncalculator. Here's the problem. There is this building and it's going to be 15 meters high,\nand I'm going to throw something and it's going to go\nup and come down. It's something I throw up has\nan initial speed of 10 meters per second. So we have to ask now,\nnow that my claim is, you can ask me any question you\nwant about this particle and I can answer you. You can ask me where it will be nine seconds from now,\neight seconds from now, how fast will it be moving. I can answer anything at all. But what I needed to do this\nproblem was to find these two unknowns."}, {"content": "So, you've got to get used to the notion of what will be given\nin general and what is tailor-made to the occasion. So, we know in this example the initial height should be 15\nmeters and the initial velocity should be 10,\nand for acceleration, I'm going to use -g and\nto keep life simple, I'm going to call it -10. As you know, the correct answer is 9.8,\nbut we don't want to use the calculator now so we'll call it\n-10. Consequently,\nfor this object the position y, at any time t\nis known to be 15 + 10t - 5t^(2). That is the full\nstory of this object. Of course, you've got to be a\nlittle careful when you use it. For example,\nlet's put t equal to 10,000 years. What are you going to get? When t is equal to\n10,000 years or 10000 seconds, you're going to find y\nis some huge negative number. You know, that's not right,\nwhat's wrong with that reasoning? Student: [inaudible]\nProfessor Ramamurti Shankar: So you cannot use\nthe formula once it hits the ground because once it hits the\nground, the fundamental premise that\na was a constant of -9.8 or -10 is wrong. So that's another thing to remember. Once you get a formula, you've got to always remember\nthe terms under which the formula was derived."}, {"content": "If you blindly use it beyond its validity,\nyou will get results which don't make any sense. Conversely, if you get an answer and it doesn't seem to\nmake sense, then you've got to go back and ask,\nam I violating some of the assumptions, and here you will\nfind the assumption that the particle had that acceleration\na is true as long it's freely falling under gravity but\nnot when you hit the ground. Now, if you dug a hole here\nuntil there, and of course it may work until that happens,\nokay. But you've got them every time. This is so obvious in this problem, but when you see more\ncomplicated formula, you may not know all the\nassumptions that went into the derivation and quite often you\nwill be using it when you shouldn't. All right. See, this you agree,\nis a complete solution to this miniature, tiny,\nMickey-Mouse problem. You give me the time and I'll\ntell you where it is. If you want to know how fast\nit's moving at a given time, if you want to know the\nvelocity, I just take the derivative of\nthis answer, which is 10 - 10t. So let me pick a couple of trivial questions one can ask."}, {"content": "One can ask the following question. How high does it go? How high will it rise? To what height will it rise? So, we know it's going to go up\nand turn around and come down. We're trying to see how high\nthat is. So, that is a tricky problem to\nbegin with because if you take this formula here,\nit tells you y if you know t,\nbut no, we're not saying that. We don't know the time and we\ndon't know how high it's rising so you can ask,\n\"How am I supposed to deal with this problem?\"\nThen you put something else that you know in your mind,\nwhich is that the highest point is the point when it's neither\ngoing up nor coming down. If it's going up,\nthat's not the highest point. If it's coming down,\nthat's not the highest point. So at the highest point it\ncannot go up and it cannot go down. That's the point where velocity is 0. If you do that, let's call the particular time\nt*, then 10t* - 10 = 0, or t*\nis 1 second. So we know that it'll go up for\none second then it will turn around and come back. Now, we are done because now we can ask how high does it go,\nand you go back to your, and y (1) is 15 + 10 -\n5, which is what? Twenty meters. By the way, you will find that I make quite a lot of mistakes\non the blackboard. You're going to find out,\nyou know, one of these years when you start teaching that\nwhen you get really close to a blackboard, you just cannot\nthink. There's definitely some inverse\ncorrelation between your level of thinking and the proximity to\nthe blackboard. So if you find me making a\nmistake, you've got to stop me. Why do you stop me? For two reasons. First of all,\nI'm very pleased when this happens, because I'm pretty\nconfident that I can do this under duress,\nbut I may not do it right every time. But if my students can catch me making a mistake,\nit means they are following it and they are not hesitating to\ntell me. Secondly, as we go to the more\nadvanced part of the course, we'll take a result from this\npart of the blackboard, stick it into the second part\nand keep manipulating, so if I screwed up in the\nbeginning and you guys keep quiet,\nwe'll have to do the whole thing again. I would ask you when you follow this thing to do it actively."}, {"content": "Try to be one step ahead of me. For example,\nif I'm struck by lightning, can you do anything? Can you guess what I'm going to say next? Do you have any idea where this is going?"}, {"content": "You should have a clue. If I die and you stop,\nthat's not a good sign, okay. You've got to keep going a little further because you\nshould follow the logic. So, for example,\nyou know, I'm going to calculate next when it hits the\nground. You should have some idea of\nhow I'll do it. But this is not a spectator\nsport. If you just watch me,\nyou're going to learn nothing. It's like watching the U.S. Open and thinking you're some kind of a player. You will have to shed the tears and you've got to bang your head\non the wall and go through your own private struggle. I cannot do that for you. I cannot even make it look hard\nbecause I have memorized this problem from childhood,\nso there is no way I can make this look difficult. That's your job."}, {"content": "All right. So, we know this point at one second is 20 meters,\nso let's just ask one other question and we'll stop. One other question may be, \"When does it hit the ground\nand at what speed?\" -- a typical physics question. So when does it hit the ground? Well, I think you must know now\nhow to formulate that question. \"When does it hit the ground\"\nis \"When is y = 0\"? By the way, I didn't tell you this but I think you know that I\npicked my origin to be here and measured y positively to\nbe upwards and I called that 15 meters. You can call that your origin. If you call that your origin,\nyour y_0 will be 0, but ground will be called\n-15. So, in the end,\nthe physics is the same but the numbers describing it can be\ndifferent. We have to interpret the data\ndifferently. But the standard origin for\neverybody is the foot of the building. You can pick your origin here, some crazy spot. It doesn't matter. But some origins are more equal\nthan others because there is some natural landmark there. Here, the foot of the building is what I call the origin. So, in that notation, I want to ask,\nwhen is y = 0? I ask when y = 0,\nthen I say 0 = 15 + 10t - 5t^(2). Or I'm canceling the 5 everywhere and changing the sign\nhere I get t^(2) - 2t - 3 = 0. That's when it hits the ground. So let's find out what the time\nis. So t is then 2 + or -\nor + 12 over 2, which is 2 + or - 4 over\n2, which is -1 or 3. Okay, so you get two answers\nwhen it hits the ground. So it's clear that we should\npick 3. But you can ask,\n\"Why is it giving me a second solution?\"\nAnybody have an idea why? Student:\nBecause there was an entire parabola [inaudible]\nProfessor Ramamurti Shankar: That's correct. So her answer was, if it was a full parabola,\nthen we know it would've been at the ground before I set my\nclock to 0. First of all,\nnegative time should not bother anybody;\nt = 0 is when I set the clock, I measured time forward,\nbut yesterday would be t = -1 day,\nright? So we don't have any trouble\nwith negative times. So the point is,\nthis equation, it does not know about the\nbuilding. Doesn't know the whole song and\ndance that you went to a building and you threw up a rock\nor anything. What does the mathematics know? It knows that this particle happened to have a height of 15,\na time 0, and a velocity of 10, a time 0, and it is falling\nunder gravity with an acceleration of -10. That's all it knows. If that's all it knows,\nthen in that scenario there is no building or anything else;\nit continues a trajectory both forward in time and backward in\ntime, and it says that whatever seconds,\none second before you set your clock to 0, it would've been on\nthe ground. What it means is if you'd\nrelease a rock at that location one second before with a certain\nspeed that we can calculate, it would've ended up here with\nprecisely the position and velocity it had at the beginning\nof our experiment. So sometimes the extra solution\nis very interesting and you should always listen to the\nmathematics when you get extra solutions. In fact, when a very famous physicist, Paul Dirac,\nwas looking for the energy of a particle in relativistic quantum\nmechanics, he found the energy of a\nparticle is connected to its momentum, this p is what\nwe call momentum, and its mass by this relation. It's a particle of mass m and momentum p\nhas this energy so you solve for the energy, you get two answers. Now, your temptation is to keep\nthe first answer because you know energy is not going to be\nnegative. Particle's moving,\nit's got some energy and that's it. But the mathematicians told Dirac, \"You cannot ignore the\nnegative energy solution because it tells you there's a second\nsolution and you cannot throw them out,\"\nand it turns out the second solution, with negative energy,\nwas when the theory is telling you,\nhey, there are particles and there are anti-particles,\nand the negative energy when properly interpreted will\ndescribe anti-particles. So the equations are very smart. The way the physics works is you will find some laws of\nmotion in mathematical form, you put in the initial\nconditions of whatever, you solve the equations,\nand the answer that comes, you have no choice. You have to accept the answer, but there are new answers\nbesides the one you were looking for. You've got to think about what they mean, and that's one of the\nbest things about physics because here's a person who is\nnot looking for anti-particles. He was trying to describe\nelectrons, but the theory said there are two roots in the\nquadratic equation and the second root is mathematically as\ninteresting as the first one. It has to be part of a theory,\nand then trying to adjust it so it can be incorporated,\nyou discover anti-particles. So always amazing to us how we\ngo into the problem, our eye or mind can see one\nclass of solutions, but the math will tell you\nsometimes there are new solutions and you've got to\nrespect it and understand and interpret the unwanted\nsolutions, and this is a simple example\nwhere you can follow what the meaning of the second solution\nis. It means that to the problem\nyou pose, there's more than the answers that you could imagine. Here it meant particle that was released from the ground\nearlier. There it meant something much\nmore interesting, mainly anti-particles\naccompanying particles. They are going to accompany\nparticles surely as every quadratic equation has two\nsolutions. All right, so now in this\nproblem, we can do something slightly different,\nand let's use this expression here,\nand I will do that, then I'll stop for today. If you were asking questions, like, how high does it go,\nbut you don't ask when does it go to the highest point,\nthen you don't have to go through the whole process of\nfinding the time at which it turned around. I don't know where that is, that disappeared on the\nblackboard, then putting the time equal to 1 second into this\nformula. If the question of time is not\nexplicitly brought up, then you should know that you\nhave to use this formula. So how do we get it here? Well, we say at the top of the loop, when it goes up and comes\ndown the velocity is 0. Therefore, you say 0^(2)\n= initial velocity^(2) + 2 times -g,\nthat's my acceleration, times y - y_0. If you solve for that, you find y - y_0 =\nv_0^(2) over 2g,\nand if you put in the v_0 I gave you,\nwhich was what, 10? That's 100 over 20, which is 5 meters. So y = y_0 + 5 meters, and that was the height\nto which it rises. I think we got it somewhere\nelse. We found the maximum height to\nbe 20 meters. Another thing you can do is you\ncan find the speed here. If you want to find the speed\nthere, you put the equation v^(2) = v_0^(2)\n+ 2 times -g (y - y_0). What is y - y_0? The final y is 0, the initial y is\n15. You solve for that equation and you will find the\nfinal velocity. So, if time is not involved,\nyou can do it that way. I want to derive the last\nresult in another way, then I will stop,\nand that's pretty interesting because it tells you the use and\nabuse of calculus. So I'm going to find for you\nthis result using calculus in a different way. So, from the calculus we know dv/dt = a. Now, multiply both sides by v. Now you have to know from\nelementary calculus that v times dv/dt is\nreally d by dt of v^(2) over 2. Now, I hope you guys know that much calculus,\nthat when you take a derivative of a function of a function,\nnamely v^(2) over 2 is a function of v,\nand v itself is a function of t,\nthen the rule for taking the derivative is first take the\nv derivative of this object,\nthen take the d by dt of t,\nwhich is this one. On the right-hand side,\nI'm going to write as a dx/dt. This much is standard. I'm going to do something which\nsomehow we are told never, ever to do, which is to just\ncancel the dts. You all know that when you do\ndy/dx, you're not supposed to cancel\nthat d. That's actually correct. You don't want to cancel the d in the derivative. But this happens to be completely legitimate,\nso I'm going to assume it's true and I'll maybe take a\nsecond and explain why it's legitimate. What this really means is in a given time, \u0394t,\nthe change in this quantity is a times the change in\nthis quantity. Therefore, you can multiply\nboth sides by the \u0394t, but the only thing you should\nunderstand is \u0394t, as long as it's small and\nfinite, will lead to some small infinite errors in the formula,\nbecause the formula is really the limit in which \u0394x\nand \u0394t both go to 0. So what you have to do is\nmultiply both sides by \u0394t, but remember it's\ngot to be in the end made to be vanishingly small. As long as we understand that, we can do this cancellation and\nthis says on the left-hand side the change in the quantity\nv^(2) over 2 is a times the change in the quantity\nx. So add up all the changes or what you mean by\nintegral. Same thing. Add up all the changes. The change in v^(2) over\n2 will be the final v^(2) over 2 - the initial\nv^(2) over 2 and the other side will be a\ntimes change in x; x - x^(0) and that's the\nformula I wrote for you: v^(2) is\nv_0^(2) + 2a (x - x^(0)). So,\nthe point is whenever you have derivatives with something over\ndt, do not hesitate to cancel the dts and think\nof them as \u0394v^(2) over 2 is equal to a times\n\u0394 of x. This will be actually true as long as both\nquantities are vanishingly small. They will become more and more true as \u0394x and\n\u0394v^(2) become vanishingly small,\nin the limit in which they are approaching 0,\nthe two will be, in fact, equal. If \u0394x is a finite amount, like 1 second,\nthis will not be true because in the starting equation,\n\u0394x and \u0394t and \u0394v^(2) were all assumed\nto be infinitesimal. So don't hesitate to do\nmanipulations of this type, and I will do them quite often. So you've got to understand when it's okay and when it's not\nokay. What this means is,\nin a time \u0394t, this quantity changes by some\namount, and in the same time,\n\u0394t, that quantity changes by some amount,\nthen keeping the \u0394t equal to some number we may\nequate the changes of the two quantities,\nprovided it is understood that \u0394v^(2) over 2 is a\nchange in v^(2) over 2 in the same time in which the\nparticle moved a distance, \u0394x. Adding the differences, we eliminate time and we get\nthis final result."}, {"content": "All right. So if you go to your website today, you will find I've\nassigned some problems and you should try to do them. They apply to this chapter. Then next week we'll do more\ncomplicated problems that involve motion in higher\ndimensions, how to go to two dimensions or three dimensions. "}], "Giving you guys a chance....": [{"content": "you guys know what this is It's a nose Swizzle check this [Applause] out sorry it was in the background it distracted me book review I can't clap cuz it will sound horrible at last book review will return last year I uploaded a video I bet you didn't watch it so I'm giving you a second chance I want to help you guys out it's was talking about habits how to take bad habits and replace them with good ones I know this seems like some sort of U productivity idity 2025 Sigma bigma I was trying to be go beyond that and use ideas from philosophy to incorporate these ideas and for me it really worked I picked up drawing I've been sketching every day since but that's just one of many things that I I think at least I've learned from reading reading is something that I enjoyed as a kid something I had also abandoned kind of like drawing actually something I enjoyed as a kid but then I picked it up again and realized just how much I loved it and uh it was the best thing I've ever done for myself by far reading completely changed my whole perspective in life I think it's just something incredibly valuable that I think everyone should experience and that's why I've announced book review 2025 cuz I want you guys to have the same Journey or if not Inspire to go on your own all these books that we're going to read I've already read but I will re read them with you I don't want to blame having a kid but I have not read nearly as much as I used to this past couple years and I want to get back into it as well so I can't make any excuses therefore you can't make any excuses now I haven't really figured out how I'm going to do this but I realize the best way to make anyone do anything is through shame therefore we will have have a shame list for anyone that fails book review 2025 and also everyone that do complete book review 2025 will get a sense of ah beyond the infinite wisdom also a sense of ah now let's get into the books the book list in January we're going to start off we're going to work our way this way we're going to start off super simple baby level T teing this is ancient Chinese wisdom it's only 100 pages but you could very easily finish it in a in one day but we're not supposed to do that we're going to take our time with it hey you can do whatever you want who cares it's hard to hold a book and a microphone the reason I H picked this book is because it's a really good soft start if for Rusty readers out there I think it's very simple it breaks down ancient wisdom in a nice simple way I kept seeing simple as you see I I've labored things in this because I think there are some passages that are just brilliant and that I keep coming back to that's sort of a theme throughout all these books that I've picked is it's books that really resonated with me I've learned stuff from them that I've Incorporated in my life and that I like to revisit and rethink about and remind myself and I think u t teing is just a great start of that and if you want to do a head start maybe let's say you finished this and you're like okay I want to move on obviously you can do that sorry to interrupt I actually wanted to review a book really quickly it's one that speaks highly to my heart and my Soul it's the book of the best vpn's on the planet starting n VPN only have you guys heard about this have you heard about nordvpn this book draws heavily for my own personal life the other day for example I was downloading legal Minecraft mods and oh I felt like I was being watched I don't know why so I launched nordvpn and my internet activity go po FBI agent go where'd he go and I could continue downloading my legal Minecraft mods safe and sound knowing that nor VPN has a strict no log's policy my internet business is my internet business as it should be another example from this great book that correlates with my personal life is when I was watching legal Minecraft anime online and all of a sudden not alloud in your region be nearly fainted was crying luckily I can always save the day with nordvpn Bing bam boom connect to anywhere in the world get rid of any region blocks the internet is open and free as it should be thank you nor VPN not just for being an amazing service but for being an amazing sponsor to this channel for so [Music] long this is what be does now he there's nothing to do with ad he thinks hello and by is blowing a kiss I just realized I did the same if you use my link in the description you get four bonus month and a huge discount the best offer you can get on nor VPN and if you're not convinced try it out for 30 days money back guaranteed I promise you will like it give it a shot I've been using it for how many years now like I know they're paying me but I'm actually paying to use it myself I'm happy to sponsor products that I actually use myself so thank you nordvpn check out nordvpn.com PewDiePie the link is the description let's continue with the book reviews the second book is in in the Buddhist words I've talked about this book so much because it's an incredible book we S I hate using the word Awakening for lack of better one that's all I'm going to use because it really was an Awakening experience for me to read this book I never examined myself in in such a way and it's funny the book seems almost aware of this itself where most of us throughout our lives go through our days and it maybe in our entire existence for many existences as Buddhism believes where we're drawn to Sensational Pleasures we're drawn to all these distractions without fully examining our lives and and our purpose and what we're doing and when you read it you get that understanding and you look at yourself obviously first you look at yourself but then you look at the whole world and you realize you want everyone to sort of stop and go no wait let's what are we doing here let's rethink this uh what is our purpose come on to me there's just something so tragic about living your life without examining it it taught me so much about Sensational pleasure especially I'm Mr dopamine okay I'm sure a lot of people can relate to this but there's almost this romanticized idea of of a sad man at a bar hunched over with a drink like there's something cool about it smoking a cigar these things are not cool that's sad all of that is sad and not a sad in that wow so sad kind of way just bad don't romanticize these things you know what I mean being drawn these Sensational Pleasures you can Free Yourself completely from them I'm speaking way more about this than I thought I would but here we are you just binding yourself to different things that you don't need and by uprooting these uh issues they will you'll completely be free from them I used to think about drinking alcohol every single day now it has zero control over me there's so much in Buddhism that ties into what we're going to go into later as well but moving on January February March March you can pick whatever you want I I think every third month I will let you guys decide I think the point of reading is not just to blindly follow what someone laid in front of you it's about finding your own Curiosities okay you read this huh I'm interested in maybe expanding on this or do reading something different maybe I hated it and this is not for me that's part of the your own Discovery I will announce which books I will read but I see them sort of outside of the list they might be maybe too advanced for some of you sounds so demeaning like you you dummies I'm going to read maybe I'll do uh uh meditations of first Philosophy by Renee Descartes I never read Renee there's a couple philosophers that I just never touched and I want to I I just feel like I should read them so that's what I pick for March moving on with the book list January February March April I don't know the months unless I count them in order another book I've talked about so much we're going to read epic Titus discourses and selected writings if you for some reason haven't read this with the amount of yapping I've done about this book you got to do it now I think if I had to pick one book out of all of these one book to rule the moral I would pick the inidian literally means handbook it is the handbook of stoicism it is the simplest way to ever explain stoicism people mostly associate stoicism with Marcus aurelus or maybe senica and aurelus obviously is an incredible story I think his life is more interesting and obviously he talks about stoicism which is amazing and he lived sism which is amazing but uh just take the inidian it puts it so beautifully and so simply and I think everyone can benefit from these ideas they are similar to Buddhism in the sense of it's in a way of avoiding suffering I've been very fortunate and and privileged I there hasn't been much times where I even feel like I need stoicism but reading stoicism it almost feels like I I put on this armor like I'm ready for anything whatever life is going to give me I'll be ready for it and that's how stoicism changes your perspective instead of fearing Misfortune you you sort of look at it as this gives me a chance to show show my virtue and and what I've learned I remember there was a book I read about American soldier that crashed his plane over Vietnam during the war and he had started sism and as he was shut down and falling in his parachute he he thought to himself this is my chance to practice what I've learned from stoicism because he knew how they would torture soldiers and uh they did okay I'm going to have to speed up cuz be is going to na soon next up we have hey play to The Republic oh oh if you think I've yed a lot about epic tittis I've actually realized I Y way more about play The Republic I I love I made a 40-minute video talking about this book I love it so much I think it is also really good introduction to philosophy because it covers so so many areas the whole premise of the book is uh Socrates is discussing with Glon they're trying to find the definition of justice since it's something that they're trying to discover together it makes you feel part of this philosophical discussion like you're transported into this uh the same room as as the greatest mind of all time and you get to experience and join this discussion what other medium could do that but literature it's just so incredible I feel so fortunate to even be reading this book it examines the human soul but then goes into a macro level and they try and examine okay so to answer this question we need to examine what is the ideal way of running a society yeah it goes into all these amazing and interesting ideas some are a little bit weird actually now I think about it I feel like I shouldn't say I enjoy this book too much big disclaimer I do not agree with every single idea that this author has a necessary 2019 disclosure everyone and then in the end it ties it all together in this beautiful like depiction of the afterlife and and rebirth kind of like Buddhism as well and what sort of Life they want to live and it answers that question in the end and I just think about it all the time I absolutely love it it's an an amazing book I'm just so excited for you for you guys defitely H anyway third month free I'm going to read I'm going to read content critique of pure reason I feel like I have to recount I tried a while ago I think I got a third in and uh I stopped for whatever reason it is extremely dense literature like uh I do not necessarily recommend casual readers to pick this one up I uh read a bit about Kant and I find him to be more interesting than his work he was just Mega autistic he's basically Sheldon Cooper which makes for a lot of interesting and sort of silly things about his life he had the famous philosopher walk every day at the exact same time but he refused to speak to anyone so if anyone tried to he would Resort he didn't want his lips to open cuz that would constitute us talking so he would resort to making noises with his mouth like mhm it's just I don't know it's funny to me I obviously know some ideas around him and his moral principles are quite famous I'm curious to read if his reasoning can sort of bring me over or not I don't know interjecting I done a little bit more research about the books that I picked cuz obviously I haven't read them and I think it might be a little difficult to do K in just a month so I was thinking okay maybe I'll do two much and or maybe I'll I was trying to think the best way to do it but then I thought you know what I'm just going to try my best if I fail that's okay I can I can pick it up at some other time or or keep going so I'm just saying this if you're deciding to read the same book which I don't encourage to be honest but if you are to be honest I I want you guys to pick your own interest uh but yeah just wanted to explain my reasoning right so next up guess what another book I've talked about a ton Aristotle nikan ethics that sounds so Advanced but it's really simple do not be turned turned off from this it is talking about happiness how to be happy what what does it mean to be a good person uh the Greek had this word for happiness which is a beautiful word called udonia and it sort of translates vaguely to flourishing as an individual and this is something I think about a lot and I can sort of make sense out of it I used this book a lot my previous video that I talked about last year about the ideas of habit cuz the Greeks understood habit and how it can lead to good virtues while bad habit obviously leads to bad virtues and but it's not just about becoming this imulation I almost Ed the tough word there but I I gave up amalgamation sort of this new Millennial idea of how you should be the more you achieve the more you show people that you're doing that is not the answer if anything that just leads to another attachment but Aristotle talk about the golden mean and this sort of balance instead of your life I'm looking forward to refresh my ideas on it I think it's just incredibly incredibly useful and valuable for people to read next is more for me you don't have to read this I I'll say you have to read a classic uh I think so many people don't want to read classic literature but that is just a huge mistake I can never say it right donkey shot the cter mon the crystal Moby Dick actually I don't I wasn't huge M day I read cash 22 recently too which is another classic I just think that was hilarious I love it actually but a classic literature maybe seems a bit dull but they are classic for a reason every time I read one it's just an amazing experience and my favorite I I don't know if it even counts as a classic but my favorite is The Iliad I love the ilad so much I've had such a weird experience with it because uh when I was a kid I think around 11 or 12 I was so into the Troy War history I don't know why I think I read it about it somewhere like a Comic version and I wanted to know everything there was about it how tall were the walls and I wanted to remember all the years and then you find out about things that you know may or may not have happened you know with the Achilles fighting Hector and all these things I just thought it was so cool I couldn't believe not everyone was talking about it and I thought it was the coolest thing ever it was basically my Marvel of the time you know I was like the gods are helping all in all these battles that's the coolest thing ever and then finally in my 30s I read the actual Iliad and I thought the gods are helping in the war that's the coolest thing ever I I I absolutely love it most people know the Iliad or associate The Iliad with the troyan horse everyone knows about it but surprisingly it isn't in The Iliad and the Iliad ends completely differently and to me the ending of Iliad was so beautiful it really struck me deeply I remember when I read it and I think most Classics do they really just hit you differently I don't know how to explain it yeah I just thought it was so incredible and I I love reading it and I want to reread it so uh that's why I picked it but I just encouraged maybe picking a a classic that you're interested in it will be worth it so next month it's free you can read whatever you want I will read Arthur schopenhauer I think actually you can read it as well it sort of makes sense to read before nche I will read the world as will and representation I think I've already read it but I think I definitely need a refresher chaen was the first as far as I understand the Western uh Buddhist he wanted to escape suffering and N which will read later really looked up to shophow but also critiqued him heavily and I want to sort of better understand schopenhauer and why cuz I I don't remember much I don't even have the copy here I left it in UK so I look forward to read open hour and then we have October I know like I'm the one who made the least so obviously I'm going to like all the books but I'm like this is so sick I love this all right so next up we're going to go into n thus so spoke sarra I talked about this book in the video I made last year a little bit there are these ideas of Will To Power the Uber MCH uh you you heard God is dead this is all in this book sarra n is such a interesting character he is the most misunderstood philosopher by far and even amongst people that have a better idea of n everyone seems to disagree as well but I I understand I understand n okay I got it I don't all these books that we read by the way so far not all of them but most of the philosophers n has up he hat he hated everyone he had this hit hit list of philosophers and he would attack them all the time it's kind of funny but I also saw it as a you know you criticize things that you like in a sense as much as I love sism and we will understand sism once we got to nche he did criticize stoicism and I think his criticism is actually fair you know nche is famously misunderstood as the poster boy for nihilism when in reality as you probably know now is he fought for the opposite he thought about this life his life affirming philosophy to me I think is so important I don't remember if s sistra had that much of that idea in it actually I think it's more in the gay science I know it's a hilarious title of a book I still think sister if I had to pick one each it's it's a really good one to start on well I don't know it's a really good one to pick apparently I don't know if it's true I think it is he was high on opioids when he wrote it so it is vastly different from other books I read from him but that also makes it very fun I think it's really fun to read and I can't wait for you guys to experience it as well like all of these books next up we have we're going to finish with sidara by Heron has maybe it doesn't make sense to end on this maybe it does make sense to end on this I think I'll know once I read reread it it is the the story of Buddha there are different depictions of Buddha amongst Buddhism but the more modern version as far as I understand is that he was a person that lived went through difficulties and suffering just as we did but then overcame it and became enlightened and became the Buddha and that is the story of suhara so I think it's a beautiful beautiful book that I would highly recommend to anyone it really surprised me caught me out of nowhere I had this horrible version of it from Amazon doesn't even have a proper spine uh if anyone have a better version if you want to give it to me I would love it and then finally you're free December I'm going to read phenomenology of Spirit by hego cuz I never read hego and I feel like I should he just have such a boring face I just assume he is equally boring I think any of these FL K looks boring yeah I'll say it if he has a dumb face probably his ideas are dumb too it's like I learned nothing from all this that is the 2025 book review imagine if you all you have to do is spend let's say the average of these book are 300 Pages they are probably a little bit more but probably more or less if you read 20 Pages a day for 30 days 2 minutes per page probably less maybe you know instead of going on your phone the first thing in the morning or um before bed replace that with the reading and it will have no change in your productivity or what you're doing but it will just vastly improve your life and I guarantee it rep a bad habit with a good one let's do it I'm so excited for you guys I'm so excited for me this is going to be great that's it hug come here"}], "3. Budget Constraints and Constrained Choice": [{"content": "[SQUEAKING] [RUSTLING] [CLICKING] JONATHAN GRUBER:\nToday, we're going to continue our discussion\nof consumer choice. And we're going to\ntalk now about what happens when we take that\nunconstrained choice we talked about on Monday and\nimpose budget constraints. We'll talk about what\nbudget constraints are. We'll then come to talking\nabout how consumers make constrained choices. And then we'll end with\nan example of food stamps. So let's start by talking\nabout budget constraints. And we'll start by talking\nabout their construction, the construction of\nbudget constraints. So, basically, last\ntime, we talked about the fundamental\naxiom of consumer choice that more is better. So what stops people from\njust bingeing on everything? It's their budget constraint. It's their limited resources. Now, for most of\nthis course, we're going to make a simplifying\nassumption that your budget-- that is what you spend-- equals your income. That is what you earn, OK? That is there won't be any\nsavings or borrowing, OK? Now that is a\nsimplifying assumption. And, indeed, we'll\nspend a couple lectures at the end of the semester\ntalking about what happens when people can save or borrow. That said, this is not\na terrible description of most Americans. The median American household\nhas $400 in the bank. So this is not kind of\na terrible description of the way most people live\ntheir lives in America, which is what they\nearn each week is what they spend each week. So that's what we'll do. It also might not be sort\nof a terrible description of your life. I presume, in college, you're\nnot doing a lot of savings. You maybe do a little\nborrowing, but not a lot of savings or borrowing. So what we're going\nto do is we're going to assume that's\ntrue for you as well. We're going to assume your\nparents have given you some amount of money to spend. We'll call it Y. Your income\nY is the amount of money your parents have given\nyou to spend for say the semester or the month. And, once again, let's say\nall you spend your money on is pizza and cookies, OK? That's all you want to\nspend your money on. We write the budget\nconstraint as saying that your resources,\nyour income Y, can be spent on either\npizza or cookies. And the constraint is\nthat you could spend it-- that budget has to be divided\nbetween pizza, where there's the price per slice of pizza\ntimes the number of slice of pizza, or cookies. We have the price per cookie\ntimes the number of cookies. So p sub p is the price\nper slice of pizza. p sub c is the price per cookie. P is the number of pizzas, and\nC is the number of cookies. That's your budget constraint. You can essentially\ndevote your income to some combination\nof pizza and cookies, but you have to consider\nhow much they actually cost in doing that. I find this easier\nto see graphically."}, {"content": "So let's turn to figure 3-1. Figure 3-1 shows a\nbudget constraint. So how does the budget\nconstraint look? Well, the x-axis is\nyour income divided by the price of cookies. That is, if you decide to devote\nall your income to cookies, then how many\ncookies can you have? Y over pc. If your income is $100,\nand cookies are $10-- that means you're going\nto Insomnia Cookies-- then you can only have\n10 cookies, et cetera. Likewise, the\ny-intercept is the income divided by the price of pizza. That's how many\npizzas you can have. The budget constraint\nrepresents-- the budget constraint, the\nslope of the budget constraint, is the price ratio, the\nnegative of the price ratio because it's a downward-sloping\nline, pc over pp. That is every extra\ncookie that you buy, holding your\nincome constant, lowers the amount of pizza\nyou can have by p sub p, OK? So let's consider an example. Suppose that Y is $96,\nthat the price of pizza-- it's an expensive\npizza place-- is $12, and the price of a\ncookie is $6, OK? $12 for pizza, this is like\ndowntown San Francisco or New York. $96 income, $12 for a slice\nof pizza, $6 for a cookie, OK? I'm sorry. Y is-- I wanted to\nmake Y 72, my bad. So Y is 72. Your income is $72, OK? And you can spend it\non pizza and cookies, and those are the prices. Now what that means is,\nif you wanted just pizza, you could get six pizzas. If you wanted just cookies,\nyou can get 12 cookies. And, generally,\nthe rate at which you can trade off pizza for\ncookies is minus 1/2, OK? That is every additional\ncookie would require giving up half a slice of pizza, OK? Every additional cookie requires\ngiving half a slice of pizza. That's why the slope\nwould be negative 1/2, OK? So, basically, we're going to\ncall the slope of the budget constraint-- the slope, we are going\nto call the Marginal Rate of Transformation, the MRT. Last time, we did the MRS, the\nMarginal Rate of Substitution. Now we're going to have\nMRT, the marginal rate of transformation, which is\nequal to minus pc over pp. Or the slope of the\nbudget constraint, OK? That is the marginal\nrate of transformation. Now this class is not alchemy. We are not literally\ntransforming pizza into cookies. That would be kind of cool,\nbut we're not doing that. That's somewhere\nelse at MIT, OK? But it's effectively\ndoing the same thing. What we're doing is, given that\nwe have a fixed amount of money and given that we're\ngoing to spend it all, the more you spend on pizza,\nthe less you spend on cookies. So you're effectively\ntransforming pizza into cookies and vice\nversa because you're going to spend all your money. You've got to spend\nit on something. So, the more you spend on one,\nthe less you get of another. So, through the\nbudget constraint, we are effectively transforming\none good to the other. By having more of one, we're\ngetting less of the other. So that's the sense\nin which we call it the marginal rate\nof transportation-- of transformation. So, basically, this comes\nback to the key concept we talked about in the very\nfirst lecture, opportunity cost. The opportunity cost of a\nslice of pizza is two cookies. Remember, opportunity\ncost is the value of the next best\nalternative, OK? The opportunity cost is\nthe next best alternative. Well, here you only have\ntwo alternatives, pizza and cookies. So the opportunity cost of a\nslice of pizza is two cookies. And that's the sense\nin which you're transforming pizza into cookies\nor cookies into pizza, OK? Now this seems kind of\nabstract, but let's actually think of an organization\nwhich has taken this principle to heart to develop the\nbest method of weight loss in America, which is Weight\nWatchers, OK, Weight Watchers. Now it turns out that\ndieting is super hard and basically doesn't work, OK? There's a large\nliterature, which says that people go\non diets all the time. Then they stop them, and\nthey gain the weight back. OK, dieting is incredibly hard\nand basically doesn't work, OK? But a much more\nsuccessful approach has been established\nby Weight Watchers. It's not the only\napproach, but it's been proven much\nmore successful, OK? And, essentially, what\ndoes Weight Watchers do? They set up a budget constraint\nand ask you to follow it. So, for example,\nthey essentially assign point values to every\ngood you might consume. You go on the website,\nand everything in the world you might want\nto eat has a point value. They then ask, well, what\nweight are you today? What's your age and gender? That stuff matters\nfor weight loss. And what weight do\nyou want achieve? And they say, if you\nwant to achieve a weight loss of x over y\ndays, then you've got to limit\nyourself to z points. So, essentially, your\ngoal is to lose weight. So we're going to give\nyou the budget constraint. We're not going to\ntell you what to eat. That's why it's\nbetter than dieting because, once again,\nAdam Smith was right. People like to have choices."}, {"content": "They like to let\nchoice drive things. But we are going to\ntell you a total budget. So, for example, vegetables\nare like zero points. Snickers bars are like\nsix points, et cetera. They have various\npoint systems, OK? So, for example, suppose your\nbudget is 30 points, which would be pretty typical, OK? Suppose you go to\nMcDonald's for lunch, and you get a number one. The number one at\nMcDonald's is a Big Mac, which has 14 points, fries,\nwhich have 10 points, and a Coke, which\nhas six points. That's 30 points, and\nit's only lunch, OK? You've blown your whole\nbudget for the day on lunch. Now you could just get\ndepressed and say screw it. I'll just be fat. But, clearly, looking\naround the room, you guys have not\nmade that choice. Or you could look at\nthe budget constraint and say, well, what\nelse can I get. Well, it turns out you can\nget a 10-piece nugget, which is 12 points, apple\nslices, which is one point, and a Diet Coke,\nwhich is zero points, for a total of only 13 points. Now you have 13 points and\nplenty of room for dinner. Now, to be honest,\nanyone who tells you that second lunch is as good as\nthat first lunch is a liar, OK? I'd much rather a Big Mac and\nfries and a Coke than nuggets and apple slice and Diet Coke. Give me a break."}, {"content": "But I'd also much\nrather have dinner, OK? So, basically, this lets\nyou make the trade-off by imposing a budget\nconstraint, by setting relative prices across goods. The points are like utils. They're not meaningful."}, {"content": "They're only\nmeaningful relatively. It lets you set relative\nprices across goods and then it lets\nyou, essentially, optimize across those various-- across those various goods. So budget constraints,\nessentially, by setting up this marginal\nrate of transformation, can help with a lot of\nkind of decisions in life. OK, questions about that? OK, now what happens if we\nshock the budget constraint? So we talked about\nconstructing them. What about shocking\nthe budget constraint? We're going to do a lot\nin this class of what we call comparative statics,\nwhich is, essentially, making changes in\none thing or another and seeing what it\ndoes to the system. So let's talk about shocking\nthe budget constraint. Let's start first with\na change in prices. Suppose the price of pizza\ngoes from $12 up to $18. This is a really good\nslice of pizza, OK? Well, what happens to\nthe budget constraint? Let's look at figure 3-2. Figure 3-2 shows what happens. You have your original\nbudget constraint BC1. The equation of that line is\n12P plus 6C equals 72, OK? The price of pizza and the\nnumber of slices of pizza plus the price of cookies\ntimes the number of cookies equals 72. Now the price of\npizza has gone up. What that's done is that has\npivoted inward your budget constraint to BC2. It has flattened the\nbudget constraint because the slope,\nremember, is the ratio of the price of cookies to\nthe price of pizza, right? That's a ratio."}, {"content": "Well, that ratio\nhas just fallen. It used to be a 1/2. Now it's a 1/3. Negative 1/2-- well,\nit used to be a half. Now it's a 1/3. So the slope has fallen from\nnegative 1/2 to negative 1/3. So what's happened is you can\nstill have as many cookies as you had before. The y-intercept has\nnot changed, but you can have fewer slices of pizza. That's why it's a pivot\nbecause one price has not changed, only the other price. So it's a pivot inward. The other thing\nhere, you'll notice we have all these funny\ndots and stuff, OK? That represents\nwhat has happened to what we call your opportunity\nset, your opportunity set, which is an\nimportant concept, OK? Your opportunity set is the\nset of choices available to you given your income\nand market prices, the set of choices available\nto you given your income and market prices. So your opportunity\nset initially was the black dots\nplus the red dots. Now your opportunity\nset has shrunk. Your opportunity set is\nnow just the black dots. Given your income,\nyou can now get less stuff, same amount of\ncookies, but less pizza. And you are worse off. Your opportunity set has shrunk. Your opportunity set-- even\nthough your parents are still sending you the same check, you\nare worse off because you can now buy less pizza with it, OK? So that's what happens\nto the opportunity set when a price changes. And, likewise, you\nshould show to yourself the same thing will happen when\nthe price of cookies change. In that case, you'll\nget an increase in the steepness of the\nbudget constraint, OK? But your opportunity\nset will still-- your opportunity set\nwill still shrink, OK? Now what about-- yeah? AUDIENCE: Don't we not\ncare about all the dots below the line, though,\nbecause we're assuming we're spending all the money? JONATHAN GRUBER: Well,\nthat's a good point, and we're going to\ncome back to that. We haven't-- we assume they're\nspending all their money, but it's just a way\nof representing. You could think of the line\nbeing lower as the same thing. We care about-- we just\ncare about the area because it represents the\nset, but you're right. You could just focus\non the line and say the line is everywhere lower. So they're worse off. That's another\nway to look at it. But we like to think\nabout as a set. It comes in handy later\nfor various reasons, OK? But that's a good question."}, {"content": "Now let's ask about\na second thing. What if your income goes up? What if prices are\nback to 12 and 6, but your parents decide\nto send you more money? Suppose your parents--\nor send you less money. It turns out you haven't\nbeen paying enough attention in 14.01. You're parents are mad. They're monitoring you. That's why we have\nthe camera here. This goes directly to\nall your parents, OK?"}, {"content": "I'm sort of joking. And so let's say parents cut\nyour allowance to $60, OK? Well, what does that do? That's in figure 3-3. OK, in figure 3-3, the\nold budget constraint was that you get\npizzas and cookies at a price of $6 and $12,\nand you could get them until you spend $72. Now you can only get\nthem until you spend $60. Now what we see is not a pivot\nin the budget constraint, but an inward shift in\nthe budget constraint, because the relative\nprice of pizza and cookies has not changed. Therefore, the slope\nhasn't changed. OK, the slope is\ndictated solely-- you don't do anything\nto control the slope. The market controls\nthe slope, OK? But you and your family\ncontrol the level, and the level has shrunk. So you're pivoting inwards, OK? And, once again,\nnow, instead of being able to buy say 12\ncookies and six pizzas, now you can only buy say\n10 cookies and five pizzas. That's the most you can get, OK? So, once again, your opportunity\nset has been restricted, but in a different kind of way\nthrough this pivot inward, OK? So that's how we\nsort of manipulate these budget constraints. And we're going to come\nback to that next lecture. That'll be important."}, {"content": "Yeah? AUDIENCE: So, in looking\nat the differences, can like an increase\nin the price of pizza or like a decrease\nin your budget-- is it more showing that\nlike the change in slopes doesn't really\naffect you if you're like say buying more\ncookies than pizza? But like, in terms of if\nyour budget as a whole decreases, then it\naffects you overall. JONATHAN GRUBER: That's\na great question, and we're going to actually\nanswer that question next lecture very explicitly. So hold on to that\nquestion, and we'll talk about we're going\nto compare explicitly why income changes\ndiffer from price changes and what are the\nunderlying mechanisms. Yeah? AUDIENCE: How do you\ndetermine your marginal rate of transformation? How do determine your--\nlike say it wasn't just pizza and cookies. Like say it was more products. How would you\ndetermine that value? JONATHAN GRUBER:\nGreat, great question."}, {"content": "So, as I said, we\nalways are going to start with simplifying\nassumptions to make life easy. There's no reason\nthat this couldn't be written in three dimensions. And you'd have\nrelative marginal rates of transformation, rates at\nwhich you're willing to trade off various things. So you could just extend\nthe math in all dimensions. It wouldn't add any\nrichness, and it'd just make your head spin. But the basic-- so\nall the basic ideas can come across with\ntwo goods, but it'd be the same mechanics\nwith more goods, OK? You essentially, when we get to\nthe constrained optimization, you'll essentially have\nmore first-order conditions in your constrained\noptimization. That's the way to\nthink about it."}, {"content": "OK, so let's-- actually,\nthat's a great segue. Let's turn to the\nsecond part, which is how we use budget\nconstraints and the utility function we learned about\nlast time to actually describe how consumers make choices. So we're going to take utility. Remember, I said\nlast time consumers are going to maximize their\nutility subject to a budget constraint. Well, now we've taught\nyou about utility. We've taught you about\nbudget constraints. Let's put them together, OK? How to consume-- how do\nconsumers put them together? Well, graphically, the\nrepresentation of preferences was our indifference curves. That represented\npeople's indifference with further out\nindifference curves made people happy, right? That was last time. So, essentially, what we're\ngoing to ask graphically is what is the\nhighest indifference curve you can achieve\ngiven your budget, right? We know you want to be that\nhighest indifference curve possible by more is better. So we're simply\ngoing to ask what is the highest\nindifference curve you can reach given your budget, OK? So let's consider the same\nutility from last time. Utility is square\nroot of P times C, OK? And let's consider the same\nbudget we wrote down up here-- $72 income, $12 price of\npizza, $6 price of cookies. And now let's ask where\ncan you go with that."}, {"content": "So let's turn to figure\n3-4 and do it graphically."}, {"content": "We'll do it mathematically\nin a minute, OK? So, in figure 3-4, you\nhave our budget constraint, which runs from 6\npizzas to 12 cookies. That's the original\nbudget constraint. And you have a series\nof indifference curves. And these indifference\ncurves, I1, I2, I3, I4, they all come directly\nfrom this utility function. So, simply, I've solved\nthis utility function. I'll talk about the\nmath in a little bit, and you'll do more math\nin section on Friday, OK? But, essentially,\nyou can solve-- we'll show you--\nyou'll drive on Friday how you take this\nutility function and literally can draw the\nindifference curves from it, OK? But, for now, take my word\nthat these indifference curves represent this utility function. And what we see is that\npoint D is the furthest out indifference curve you\ncan achieve while still meeting your budget, while still\nmeeting your budget constraint. And, therefore, we say that\nthe optimum, graphically, is the tangency between\nyour indifference curve and your budget constraint is\nthe optimal constrained bundle. You see how we brought-- last time, we talked about\nfurther out indifference curves make you happier. Today, we talked about\nthe fact that you're limited by your budget. So we have the furthest\nindifference curve you can get to is going\nto be, definitionally, at the tangent of the\nindifference curve and the budget constraint. And, once again,\nthat gives you-- we realize we don't want\nto measure utils, but, just for mathematical, for\nmathematical purpose, that gives utility at the tangency\nof square root of 18, OK? At that point, you are choosing\nsix cookies and three pizzas. That is the choice\nyou are making. That is the best off you\ncan get given your budget. And, to see this, let's\ntalk about some other points and why they're not better, OK?"}, {"content": "Let's talk about point A. Why isn't point A better? Why isn't it better\nto have two-- maybe you just-- maybe you\nlike cookies a lot and don't like--\nor like pizza a lot and don't like\ncookies that much. How can we say that point\nD is better than point A? Yeah? AUDIENCE: Because point D is\non a higher indifference curve. JONATHAN GRUBER: It's on a\nhigher indifference curve. So point D dominates\npoint A because it's a higher indifference curve. Well, fine. Same person, by that logic,\nwhy not choose point E? AUDIENCE: It's above the budget."}, {"content": "JONATHAN GRUBER: Yeah,\nyou can't afford it. So the bottom line is\nyou can see graphically why the tangency is\nthe best you're going-- is the best you're going to do. OK, likewise, point C\nyou wouldn't choose. Point C has the same slope. It has the same\nslope as point D. In other words, the slope\nis minus 1/2 at point C. You've drew a line tangent\nto point C. The slope will be minus 1/2, just\nlike it is at point D, but you wouldn't be\nspending all your money. So you wouldn't choose\nthat point either. Yeah? AUDIENCE: What if you have\njust three indifference curves so there is none\nthat hit the tangent? Do you just go for one that's\nlike the most tangent I guess? JONATHAN GRUBER: We're going\nto come to-- we're going to-- well, first of all,\nwe're not going to have discrete indifference. We could have lines, and\nthe lines could end up-- you could end up lying along. You could end up lying along a\nbudget constraint for example. Or you could have-- you could even have\nutility functions, which just touch a\nbudget constraint at one extreme or another. And we'll talk\nabout those cases. Yeah? AUDIENCE: So [INAUDIBLE]\nutility function go through lines and the\nbudget constraint, right? JONATHAN GRUBER: Yeah. AUDIENCE: Isn't this just\nLagrange [INAUDIBLE]?? JONATHAN GRUBER: Well,\nlet's come to the math then. OK, let's come to the\nmathematical derivation. So that's the graphic. So let's come to the math, OK?"}, {"content": "Now, always a bit\nof a tightrope act when I'm doing math up here on\nthe board, so bear with me, OK? But the key thing is the math\nof constraint optimization is all about the\nmarginal decision. Remember, it's hard to say\nhow many cookies you want. It's easier to say should\nI have the next cookie, OK? It's about constraint\noptimization. And what we want to ask is we\nessentially want to compare how do you feel about trading\noff pizzas versus cookies versus what will the market let\nyou do in sort of trading off pizzas versus cookies. That is the optimum\nis going to occur when we set your marginal\nrate of substitution, which, remember, we defined\nas minus MUc over MUp, equal-- I'm going to get rid of this-- equal to your marginal\nrate of transformation, which we defined as\nminus pc over pp. And this is the fundamental\nequation of consumer choice. If you understand\nthis equation, you can solve virtually\nevery consumer choice problem I'll give you, OK? That basically, at the optimum,\nthe ratio of marginal utilities equals the ratio prices. That is the rate at which\nyou want to trade off pizza for cookies is the rate\nat which the market will allow you to trade off\npizza for cookies, OK? Basically, it's saying\nthe ratio of the benefits. Think of this as the benefits\nand this as the costs. Think of the MRS\nas the benefits. It's what you want. MRT is the costs. It's where you're constrained. You want to set the\nratio of the benefits equal to the ratio\nof the costs, OK? Now I find it actually easier\nto think of it this way. If you just rearrange terms,\nyou can write it as MUc over pc equals MUp over p sub p. I like this way of writing\nit because I call this the bang for the buck equation. What this is saying, your\nmarginal happiness per dollar should be equal. This is sort of the happiness\nper dollar spent on cookies. This is the happiness per\ndollar spent on pizza. And you want those to be equal. You want the bang\nfor the-- you want to put your next\ndollar where it's going to make you happiest, OK? And so, basically, think of\nthat as your bang for your buck. So, for example, suppose\nyou were in a position where the MRS was\ngreater than the MRT. You're in a position where the\nmarginal utility of cookies-- and I'm getting\nrid the negatives. There's negative on both sides. So I'm just going to get\nrid of the negatives, OK? The marginal utility of cookies\nover the marginal utility of pizza was greater\nthan the price of cookies over the price of pizza, OK? That is the slope of\nthe indifference curve was greater than the slope\nof the budget constraint. This is the slope of\nthe indifference curve. OK, this is slope of\nthe indifference curve. This is the slope of\nthe budget constraint. In absolute value, the slope\nof the indifference curve is greater in absolute value\nthan the slope of the budget constraint, OK? That would be true at points\nlike point A, point A where you intersect-- where you basically intersect\nfrom above the budget constraint by the\nindifference curve. So a point like point\nA has a steeper slope of the indifference curve than\ndoes the budget constraint. What that says is\nintuitively-- and, once again, I want you to understand\nthe intuition-- the rate at which you\nare willing to give up, the rate at which you\nare willing to give up cookies for pizzas-- I'm sorry. Let me say it-- let me say it a better way. The marginal benefit to\nyou of another cookie relative to another\npizza is higher than what the market will charge\nyou to turn pizza into cookies. Let me say it again. The marginal benefit to you of\nanother cookie, which is this-- this is how much more\nyou want the next cookie relative to how much more\nyou want the next pizza-- is greater than\nwhat the market is going to charge you to trade\nin your pizza for cookies. Therefore, you should trade\nin your pizza for cookies, OK? So let's say this\nmathematically. At a point like A,\npoint A, OK, you have your marginal\nutility for pizza is the derivative of the\nutility function with respect to the number of\nslices of pizza. It's the marginal utility. It's derivative of\nthe utility function. So it's dU dp, which is equal\nto 0.5 times C over square root of P times C, OK? And, at point A,\nat point A, we had two cookies and five pizzas. At point A, P was five. C was two. OK, that's true of point A. So we can evaluate the\nmarginal utility dU dp, which equals 0.5 times C\nover square root of P times C. So that's 1 over the\nsquare root of 10. That's the marginal utility\nof the next slice of pizza. The next slice of\npizza makes you 1 over square root of 10 happy. Once again, that\nnumber is meaningless. So we only care\nabout it in ratios. So we need the ratio. So let's do the marginal\nutility of cookies. That's dU dC, which\nis 0.5 times P over square root of P\ntimes C, which is 2.5 over the square root of 10, OK? So the marginal utility of pizza\nis 1 over square root of 10. Marginal utility of cookies is\n2.5 over the square root of 10. Therefore, your marginal rate\nof substitution is minus 2.5. Remember, marginal rate of\nsubstitution is MUc over MUp. So your marginal rate of\nsubstitution is minus 2.5. What does that mean? Can anyone tell me\nwhat that means? Your marginal rate of\nsubstitution is 2.5. What does that mean?"}, {"content": "That is a meaningful concept. Utils are not, but that is. Yeah, say it loudly\nso we can hear. AUDIENCE: You're\nwilling to trade-- you're willing to trade\ntwo pizzas for one cookie. JONATHAN GRUBER: You're\nwilling to trade. Exactly, you're willing to\ngive up 2.5 slices of pizza for one cookie. That's what that number means. And that is a meaningful number. That's not an ordinal. That's cardinal."}, {"content": "We can use that. You are willing to give\nup 2.5 slices of pizza to get one cookie. What is the market\nasking you to give up? How much pizza do you have\nto give up to get one cookie? Half a slice. You are happy to give up\n2 and 1/2 slices of pizza to get a cookie,\nbut the market is saying we'll let you\nhave a cookie for half a slice of pizza. So what should you do? AUDIENCE: Trade. JONATHAN GRUBER: Eat less pizza. Eat more cookies. That will unambiguously\nmake you happier. And that's why you should move\nfrom point A towards point D. OK, that's the intuition, OK? You basically want to\ntrade pizza for cookies until these things are equal. Indeed, I'd like you to go home\nand do the same math starting at point B. If you do the\nsame math starting at point B, you'll find the MRS\nis much below 1/2. That is, at that point,\nyou are happy to give up tons of cookies to get pizza\nbecause, jeez, you've got 10 cookies and one slice of pizza. You'd give up tons of\ncookies to get pizza. But the market says you\nonly have to give up two cookies to get pizza. So you'll happily do it, and\nyou move back towards point D. And that's sort of in a bundle\nsort of the intuition and math and graphics of how we do\nconstrained optimization. OK, that is hard\nand very important."}, {"content": "Questions about that? Don't hesitate to ask."}, {"content": "OK, that is hard\nand very important. If you understand\nthis, you're sort of done with consumer theory, OK? This is sort of the core of what\nconsumer theory is all about. It's all about\nthis balancing act. The whole course\nis fundamentally all about one equation,\nwhich is marginal benefits equals marginal costs, OK? Everything we do is going\nto be about weighing the marginal benefit\nof an activity against its marginal costs. If we take the next\nstep, what's the benefit? And what's the cost? Well, here the marginal\nbenefit is the MRS. The marginal cost is the MRT. We want to set them equal."}, {"content": "And this sort of example\nI hope explained why, OK? So that is how we think\nabout constrained choice. Now I want apply it. I want to apply it by looking at\nthe example of food stamps, OK? Now food stamps are not actually\ncalled food stamps anymore. When I was a kid, they\nwere called food stamps. It's basically a program\nthe government has that provides money\nfor individuals to buy food if\nthey're low income. Essentially, we have in the US\nwhat's called the poverty line. And I'll talk a lot more about\nthis at the end of the class, but the poverty\nline is essentially a measure of what's a\nminimum level of resources you need to live in America. The poverty line for an\nindividual is about $14,000. OK, for a family of\nfour, it's about $28,000. How you feel about\nthat number obviously is going depend on\nwhere you're from. If you're from Boston,\nyou'll say that's insane. If you're from some rural\npart of the country, you think, yeah, that's\npoor, but manageable. OK, we'll talk later\nabout the poverty line, what's good and bad about it. But, in any case, if you're\nbelow the poverty line in America, roughly speaking,\nyou get help with buying food. And that comes through a\nprogram we now call SNAP. It used to be\ncalled food stamps. I've got to update my notes. Supplemental Nutrition--\nI don't know. I know the N is for nutrition. OK, so, basically, what\nthe SNAP program does is it gives you a debit card. If you qualify on income\ngrounds, you get a debit card, and that debit card can be used\nto buy food and food only, OK? So you essentially get a\ndebit card from the government that you can use to buy\nfood if you're poor enough. And they give you sort of\na fixed amount every month, and that amount can be\nused to purchase food. So here's the question. Why go through this rigmarole? Why not just give people cash? This fancy thing, if we want\nto give poor people money, why don't you just\ngive them money? And we're going to-- I don't want the answer yet, OK? What I want to do is\nshow you graphically how we think about\nthe trade-off, and then we'll\ncome to the answer. So hold your thoughts. So let's actually graph how\nwe think about food stamps. Let's go to figure 3-5A. And let's start with\na cash transfer. So here's the setup. Imagine people start\nwith an income of $5,000. That's super poor, OK? $5,000 is their whole family\nincome for the year, OK? And let's say all they can\nspend it on is food or shelter. Remember, as this gentleman\npointed out, in life, there's more than two goods,\nbut it makes it a lot easier to have two goods. So imagine this case. Your two goods are\nfood and shelter. And, actually, quite\nfrankly, if you're that poor, that probably is\nthe only two goods you have to-- you can worry\nabout at that level of income. OK, it's food and shelter. So you $5,000 to devote\nto food and shelter. So you have some\noriginal budget line, which is labeled\nthere original budget line, that runs from 5,000\nin food to 5,000 in shelter. And then you can have some of\nin between, some along the way, OK? Now let's say we give\nsomeone $500 in cash. Obviously, this graph\nis not to scale, OK? It looks like you're doubling\nhis income, but it's only $500. This just sort of makes it\neasier, a not to scale graph. Let's say we give someone-- we\nsay to them, look, you're poor. We're going to give\nyou $500 in cash. Well, now all we've done\nis shift out your budget constraint from 5,000 to 5,500. OK, we've shifted out\nyour budget constraint from 5,000 to 5,500. What does that do\nto your choices?"}, {"content": "Well, consider two\ndifferent types of people. Person y, OK, they used to\nbe on indifference curve I0. They used to spend almost\nall their income on food and not a lot on shelter."}, {"content": "They were probably homeless, OK? So they spent all\ntheir money on food and were basically homeless. Now what do they do? Well, they spend a little\nmore on food and a lot more on shelter. Maybe now they get--\nyou know, $400 still doesn't buy you much shelter. They spend a little more, OK? Maybe, a night a week,\nthey can get shelter, OK? So, basically,\nthat's what they do. That's their constrained\noptimization. We're not saying\nit's right or wrong. This is not normative economics. It's positive. The positive thing is, given\ntheir utility function, they move from point y1 to y2. Now imagine someone\nlike individual x."}, {"content": "They're different. Their tastes are such that\nthey don't need to eat. They just want to have shelter. So they're up at\npoint x1 initially. And you give them\nthat $500, and they spend just a little bit more of\nit on food and even more of it on shelter. They just love\ntheir shelter, OK? And they're just super-- they're super Weight Watchers. They don't eat, OK? So, basically, they\nmove from x1 to x2. Once again, not\nnormative right or wrong, it's just these are\nfeasible choices people could make given the opportunity\nset with which they're faced. And that's what happens when\nyou give them the $500 in cash. Questions about what I did\nhere on this graph alone? Yeah? AUDIENCE: Like, even\nif like you gave them money specifically for\nfood, couldn't they then just reallocate\ntheir other money? JONATHAN GRUBER: OK,\nthat's a good point."}, {"content": "We'll come back to that. That's time out if\nyou're not a sports fan. OK, so we will\ncome back to that. And, in fact-- OK, but do people\nunderstand what the cash transfer is, how it works? OK, now let's go to SNAP. And let's say, with SNAP,\ninstead of giving them $500, we'll give them the debit card. Instead of handing\nthem a $500 check, we give them a debit\ncard with $500 on it that can only be used on food. How does this affect\ntheir budget constraint? Now we see where budget\nconstraints start to get interesting and fun\nand the kind of challenges you're going to face in this\ncourse in drawing budget constraints. The original budget\nconstraint continues to be the original budget line\nrunning from 5,000 to 5,000. The new budget constraint\nis this kinked line that runs from 5,000 on\nthe y-axis to the point x2 at 5,000 on the y-axis. So it starts at 5,000 on\nthe y-axis, 0 on the x-axis. There's a flat line that goes\nto 5,000 on the y-axis, 500 on the x-axis. And then it slopes down\nparallel to the original budget constraint to 5,500. Can someone explain to me\nwhy that's the new budget constraint? Yeah? AUDIENCE: You can't\nspend a negative amount. So you can't spend\nlike negative amounts of your non-food-stamp\nmoney on food. JONATHAN GRUBER:\nExactly, you have-- we are forcing you to\nspend at least $500. Compared to cash, where you can\ndo whatever the hell you want, we are forcing you to spend\n$500 of your money on food. Coming to the\nquestion back there, it doesn't have to be a\nspecifically labeled 500. It can be any 500. But we're forcing you to\nspend at least $500 on food. Well, what does that\ndo to your choices? Well, for person y,\nit makes no difference whether they get cash or\nwhether they get food stamps. Now the person, light blue\nshirt, turquoise shirt, asked that question. Why does it make no difference? Yeah? Why does it--\nwhatever, greenish, I don't know, yeah, you. Why does it make no\ndifference for person y if I give him food\nstamps or cash? AUDIENCE: He's already spending\na lot of his money on food. So any money he gets he can\njust reallocate differently so he can spend\nsome of the money he would have used\non food on shelter. JONATHAN GRUBER: Exactly, he can\njust reallocate his money, OK? That's exactly right. So, for person y,\nthere's no difference. Look, they're already\nspending, what, $4,900 on food. You give him a thing\nlabeled $500 for food. It's not going to\naffect their life. They'll just take 500. They'll just spend-- they'll\njust treat it as $500 more in cash. They're indifferent. So nothing affects them. But what about person x? Well, person x, remember,\nthe dashed portion of this budget constraint is\nfrom the old cash example. And the dotted\nindifference curve is what they would\nhave chosen with cash. Remember, person\nx with cash would have chosen to still spend\nless than $500 on food. Even when you gave\nthem $500, they still only spent $300 on food. So we are forcing them to not\nbe on their preferred budget constraint. Rather, we're forcing\nthem down to point x2, which is they'll spend the\nminimum they can on food, but the minimum is $500, OK? We are forcing them\ndown to point x2. Now why do I say forcing them? Why do I know for sure they\nare being forced, that they're less happy at x2 than they would\nhave been when they gave them the cash? How do I know that for sure?"}, {"content": "Yeah? AUDIENCE: They're at a\nlower indifference curve. JONATHAN GRUBER: Exactly. Think of it this way. The fundamental-- one\nof the important things is people always get\nto the point that makes them happiest, OK? We call it the robustness\nof economic equilibria. People get to the point\nthat makes them happiest. They want-- they\nalways before had the choice of spending $500 on\nfood, and they chose not to. Therefore, if you force\nthem to spend $500 on food, they must be less happy, OK? Think of it that way. They always could have\nspent $500 on food. They didn't. Therefore, in\nforcing them, you're making them less happy, OK? So they are worse off, OK? They are forced to spend. They'd rather spend\nsome of that money and find a nicer place to live,\nbut we're not letting them. We're making them buy food, OK? Do people-- I don't want-- I just want to know if people\nunderstand the graphics here and the conclusions I drew. OK, now why?"}, {"content": "Why are we doing this? Why would you-- they're\nbetter off with cash. Why would we force\nthem to have food? Yeah? AUDIENCE: Say\nbecause what makes-- what puts people on the\nhighest indifference is just what makes them\nhappiest, but not necessarily what makes them like\nlive the longest or like have the best\nhealth So, perhaps, like if you never spend money\non food, and then you die, that would be really bad. JONATHAN GRUBER: OK, but,\nbasically, what you're saying is you know better than the guy. Let me-- I'm not accusing you."}, {"content": "I'm just saying, look,\nif people knew best, maybe they'd like to just like\nhave a nice house and die, OK? If people knew\nbest, then there'd be no reason to do this."}, {"content": "The reason to do this is because\nwe think they don't know best. So, for example, let's change\nthe label on the y-axis, just a small change. Let's cross out shelter\nand write cocaine."}, {"content": "[LAUGHTER] OK? Well, in that case, maybe\nwe don't feel so bad about forcing the guy to buy\nfood instead of cocaine, OK? In other words, this a\nprogram which might make sense if we are paternalistic. Now we're getting into normative\neconomics, paternalistic. If we think that people\nwon't necessarily make the right decisions\nfor themselves, then it may be worth\nactually making them worse off because\nthey're not worse off. Their perceived\nbenefits are worse, but they don't know\nwhat they're doing, OK? Now you can see why-- I hope you can\nsort of immediately see why this concept makes\neconomists a little nervous because why do we know what they\nwant better than they do, OK? So it makes people a\nlittle bit nervous, economists a little\nbit nervous, and a lot of people a little bit nervous\nto say, gee, maybe they're just happier doing cocaine. And how do we know that\nthat's the wrong way for them to spend their resources?"}, {"content": "Yeah? AUDIENCE: Well,\nlike can't you look at it from the perspective of\nlike this is taxpayer money, right? So then aren't you\nalso just factoring in how the taxpayer wants to\nspend their money and then their indifference curve\nand all their information? JONATHAN GRUBER: That's\na very good point. Now but there's sort\nof two points there. First of all, if the taxpayers'\ngoal is to help poor people, then why shouldn't you make them\nas happy as possible, right? If tax-- why am I giving\nmoney to this poor guy? Because I'm sad his poor. But, what you're saying, I'm\nnot actually that sad he's poor. I'm sad he's not eating. If you're really\njust sad he's poor, then you should give him money. If what you're\nsad about is, gee, I don't like how he's living-- I don't like his-- I'm sad he can't have better\nfood to eat, sad at the place he lives. Then you're starting to\nimpose your preferences, but let's be important."}, {"content": "That's imposing\nyour preferences."}, {"content": "Yeah? AUDIENCE: I feel like the\nindifference curve only goes for happiness or\nlike contentedness, but, really, the point\nof SNAP isn't really with contentedness or\nhappiness, but rather like what would be to a\nmore sustainable life. JONATHAN GRUBER: Well, that's a\nrelated point of the taxpayer. If the taxpayer\ncares about, look, we want a healthy\npopulace that's going to live a long time and\nbe productive and pay taxes, then that would be\na reason to do this. But, once again, I\nwant to emphasize, OK, this is paternalism. If you really just care\nwhat makes people happiest, you should give them cash, OK? So that raises\ntwo questions, OK?"}, {"content": "First of all, first\nquestion-- yeah? AUDIENCE: So how about\nlike negative [INAUDIBLE].. Because, for example, if\nwe pump a lot of money-- if we allow people to\nspend a lot on shelter, that's not really\ngoing to help people. It would just make the real\nestate developers rich. And say the amount\nof shelter is kind of fixed, but like the amount of\nfood that eaten [INAUDIBLE].. So, if we let people\nspend more money on food-- JONATHAN GRUBER: Yeah,\nyeah, so, basically, that's a great question. And, in general,\nwe're going to-- I'm going to answer a\nlot of those questions with the same cheat\nthis semester, which is we're going to assume\nthe markets are perfectly functioning. So there's no-- you're imposing\nsort of a market failure. If there's no market-- once\nthere's market failures, all bets are off. But, with no market\nfailure and no paternalism, you'd want to give them cash. So this raises an\nimportant question. Do food stamps actually\nincrease food purchases? First of all, there's two\nreasons why they might not."}, {"content": "Reason one is everybody\ncould be like y."}, {"content": "x is sort of a\nsilly case, right? You're going to die if\nyou eat that little. And food stamps\naren't that much. They're maybe like\n$3,000 a year. Everybody is going\nspend $3,000 on food. So the first issue is the first\nreason why food stamps may not matter is that, in\nfact, everybody is spending at least that amount. Everybody is like y,\nand nobody is like x. What's another reason\nwhy it might not matter? What's a way people could\nget around food stamps? Yeah? AUDIENCE: Buy food with\nfood stamps and sell it. JONATHAN GRUBER: Yeah,\nthey could set up a black market where they,\nessentially, say, look, I only want $2,000 of food. The government is\nmaking it worth $3,000. I'll buy my extra\n$1,000 of food, and I'll sell it to\npeople who do want it. And I'll end up still\neating $2,000 worth of food. So we actually want to know do\nfood stamps actually increase food consumption in practice. Are they making a difference? Well, actually, we've run\nan experiment on this, OK? We're going to talk\nin this class a lot about empirical\nresults in economics. This class is mostly going\nto be a theoretical class. That is we'll talk\nabout models and ideas. But we're also--\nsince, basically, I'm an empirical\neconomist, we're going to talk about empirical\neconomics, which is results and testing the\ntheories we develop. Empirical economics,\nhere's a great example of empirical economics is we\nset up a theoretical model. You always want to\nstart with the theory, but the theory sometimes\nhas predictions, which are uncertain. Here we have an uncertain\nprediction from theory about whether food stamps will\naffect food purchases or not. So let's test it. And the way we test\nit is we actually have run food stamps cash out\nexperiments where we literally take guys on food stamps\nand give them cash instead and watch what happens to their\nconsumption before and after. It's a real randomized trial. We literally flip a coin. Heads, you keep\nyour food stamps. Tails, we replace\nthose food stamps with an equal amount of cash. Then we watch what happens. What happens is that people\nspend about 15% less on food when you give them cash\ninstead of food stamps. That is food stamps is forcing\npeople to spend about 15% more on food than\nthey would like to unconstrained by the cash. Yeah?"}, {"content": "AUDIENCE: Yeah, this gets\nyou into the behavior of [INAUDIBLE]. I remember reading an\nexperiment like, if you have the price of gas go down,\nthe actual like amount of money spent on gas is constant. And this might\ntranslate to food stamps because like food stamps\nare like explicitly on food. JONATHAN GRUBER: Yeah, you\nknow, that's a great question. And that's you're asking about\nricher theory, richer theory. And I'm telling you that\nI'm going to give you the empirical evidence. So, whatever the theory\nis, the empirical evidence tells you what happens. And there's different\nexplanations for why. So the empirical evidence is\nthat, basically, the price of our paternalism is 15%, OK? We are making people,\neffectively, 15% worse off. We're making them spend 15%\nmore food than they want to. So is it worth it? Well, actually, the\nevidence is starting to pour in that it might not\nbe worth it because there's starting to be a lot of\nexperiments where we're giving people just cash,\nespecially in developing countries. In developing\ncountries, the answer seems to be just\ngiving people cash makes them better\noff, that actually, especially in\ndeveloping countries, people use the cash\nin productive ways. So, for example, they have a\nseries of evaluation programs where they've given people cash,\nmostly in developing countries, in Africa in particular,\nsome in the US. And they find that people\nspend relatively little of that on drugs and\nalcohol, but they actually tend to spend it productively. And, in fact, they found,\nin developing countries, this often provides valuable\nresources for individuals to start businesses. So they ran experiment Uganda\nwhere a nonprofit company randomly offered\na group of women $150, which is huge\nrelative to their income. That's 50% to 100% of annual\nincome in Uganda, $150. And what they found was, after\ntwo months-- after 18 months, these women had used that\nmoney to start businesses. And that actually\nraised their earnings. That actually effectively\ndoubled their earnings. From that one\ninjection of cash, it led them to actually double\ntheir annual earnings, OK? So that leads one to\nthink that maybe we should stop being paternalistic\nand just give cash. Unfortunately, if you're a\nreader of policy websites like I am, the best one\nof which is vox.com-- it's a great website-- they had an article just\nthe other day pointing out how they actually followed\nthese women up nine years later. And, nine years later, the\neffect had totally gone away. So the story isn't quite\nnecessarily positive, but it's not negative. They're not worse\noff, but it looks like, at least what\nin the short run made them better off, well,\nthat effect fades over time. But the bottom line\nis, at this point, I think the evidence\nis sort of probably in favor of being\nless paternalistic and just giving\npeople cash, but that runs into a lot of difficulties\nin terms of our concerns about how people will spend it. So let me stop there. We will come back\non Monday, and we'll talk about how we actually go\nfrom this stuff to the demand curves we started\nthe class with."}], "Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)": [{"content": "so let's get started uh so I'll be talking about building llms today um so I think a lot of you have heard of llms before uh but just as a quick recap uh llms standing for large language models are basically all the chat Bots uh that you've been hearing about recently so uh Chad GPT from open ey Claud from entropic Gemini and and lman other type of models like this and today we'll be talking about how do they actually work so it's going to be an overview because it's only one lecture and it's hard to compress everything but hopefully I'll touch a little bit about all the components that are needed to train uh some of these llms uh also if you have questions please interrupt me and ask uh if you have a question most likely other people in the room or on Zoom have other have the same question so please ask um great so what matters when training llms um so there a few key components that matter uh one is the architecture so as you probably all know LMS are newal networks and when you think about new networks you have to think about what architecture you're using and another component which is really important uh is the training loss and the training algorithm um so how you actually train these models then it's data so uh what do you train these models on um the evaluation which is how do you know whether you're actually making progress towards the goal of of uh llms and then the system component so that is like how do you actually make these models run on uh Modern Hardware which is really important because these models are really large um so now more than ever system is actually really an important topic um for llms so those five components um You probably all know that llms and if you don't know LMS are all based on Transformers or at least some version of Transformers uh I'm actually not going to talk about the AR lecture today uh one because I gave a SE lecture on um Transformers a few weeks ago and two because you can find so much information online on uh Transformers but I think you can it's there's much less information about the other four topics so I really want to talk about those um another thing to say is that most of Academia actually focuses on architecture and training algorithm and losses um as academics and I've done that for a lot big part of my career is simply we like thinking that this is uh like we make new architectures new models and it it seems like it's very important but in reality honestly what matters in practice is mostly the three other topics so data evaluation and systems uh which is what of most of Industry actually focuses on um so that's also one of the reason why I don't want to talk too much about the architecture uh because really the rest is super important um great so overview of the lecture I'll be talking about pre-training so pre-training uh you probably heard that word this is the general word this is kind of the classical language modeling uh Paradigm uh where you basically train your language model to essentially model all of internet and then there's a post training which is a more recent Paradigm which is taking these large language models and making them essentially AI assistants um so this is more of a recent Trend since Chad GPT uh so if you ever heard of gpt3 or gpt2 that's really pre-training land uh if you heard of chat GPT which you probably have this is really posttraining land uh so I'll be talking about both but I'll start with pre-training and uh specifically I'll talk about what is the task of pre-training llms and what is the laws that people actually use so language modeling this is a quick recap uh language models at a high level are simply models of probability distribution over sequences of tokens or of words so it's basically some uh model of P of X1 to XL where X1 is basically word one and Excel is the last one in the sequence or in the sentence um so very concretely if you have a sentence like the mouse ate the cheese what the language model gives you is simply a probability of this sentence being uttered by a human or being found on on online uh so if you have another sentence like the the mouse at cheese uh here there's grammatical mistakes so the model should know that this uh should have some syntactic knowledge so it should know that this has less likelihood of appearing online uh if you have another sentence like the cheese ate the mouse uh then the model should hopefully know about the fact that usually cheese don't eat Mouse um so there's some semantic knowledge and this is less likely than the first sentence so this is basically at a high level what language models are um one word that you probably have been hearing a lot in the news are generative models uh so this is just something that can generate models that can generate sentences or can generate some data uh the reason why we say language models are generative models is that once you have a model of a distribution you can simply sample from this model and now we can generate data uh so you can generate sentences uh using a language model so the type of models that uh people are all currently using are what we call Auto regressive language models and the key idea of autor regressive language models is that you take this distribution over words and you basically decompose it into the into the distribution of the first word multiply the by the distribution of or the likelihood of the distribution of the second word given the first word uh multiply by P of the third word given the first two words um so there's no approximation here this is just the chain rule of probability which you hopefully all know about uh really no approximation this is just one way of modeling a distribution uh so slightly more concisely you can write it as a product of U of PS of the next word given everything which happened in the past so of the context and uh so this this is what we call Auto regressive language models again this is really not the only way of modeling distribution this is just one way uh it has some benefits and some downsides one downside of autoaggressive language models is that when you actually sample from this autoaggressive language model you basically have a for Loop which generates the next word then conditions on that next word and then regenerate an other word so basically if you have a longer sentence that you want to generate you it takes more time to generate it uh so there are some downsides of this current Paradigm but that's what we currently have so I'm going to talk about this one uh great so Auto regressive language models at a high level um what the task of autoregressive language model is is simply predicting the next word as I just said so if you have a sentence like she likely prefers uh one potential next word might be dogs and the the way we do it is that we first tokenize so you take these words or subwords you tokenize them um and then you give an IDE for each token so here you have 1 2 three uh then you pass it through this black box as I already said we're not going to talk about the architecture you just pass it pass it through a model and you then get a distribution a probability distribution over the next word over the next token and then you sample uh from this distribution you get a new token and then you DET tokenize so you get a new ID you then DET toonize and that's how you basically sample from a language model uh one thing which is important to not is that the last two TS uh two steps are actually only need needed during inference uh when you do training you just need to predict uh the most likely token and you can just compare to the real token which happen in practice and then you basically change the weights of your model to increase the probability of generating that token um great so autoaggressive neural language models so to be slightly more specific still without talking about the architecture uh the first thing we do is that we have all of these oh sorry yes on the previous slide when you're predicting the probability of the next tokens does this mean that your final like output VOR has to be the same dimensionality as the number of tokens that you have yes how do you deal with like if you have more to like if you're adding more tokens to your cor something yeah so we're going to talk about tokenization actually later uh so you will get some sense of this you basically can deal with adding new tokens I am I'm kind of exaggerating there are methods for doing it but essentially people don't do it um so it's really important to think about how you tokenize your text and that's why we'll talk about that later but it's a very good point to notice that you basically the vocabulary size so the number of tokens that you have is essentially the output of your uh language model so it's actually pretty pretty large okay so autoaggressive new language models first thing you do is that you take every word or every token you embed them so you get a um some Vector representation for each of these tokens um you pass them through some ual Network as we said it's a Transformer then you get a representation for all the word in all the words in the context so it's basically representation of the entire sentence uh you pass it through a linear layer as you just said to basically map it to the number so that the output the number of outputs is the number of tokens uh you then pass it through some soft Max and you basically get uh probity distribution over the next words given every word in the context and the law that you use is basically it's essentially a task of classifying the next token so it's a very simple kind of machine learning task so you use the cross entry P loss where you basically you look at the actual Target that happened which is a target distribution which is a one hot encoding which here in this in this case says I saw uh the real word that happened is cat so that's a one hot um distribution over cat and here this is the actual uh do you see my mouse oh yeah this is the distribtion that you generated and basically you do cross entropy which really just increases the probability of generating cat and decreases all the the probility of generating all the other tokens one thing to notice is that as you all know again uh this is just equivalent to maximizing the text log like the text log likelihood because you can just rewrite the the max over the probability of um this autoregressive language moding task as just being this minimum over I just added the log here and minus which is just the minimum of the loss which is the cross enty loss so basically minimizing the loss is the same thing as maximizing the likelihood of your text any question questions okay tokenizer um so this is one thing that people usually don't talk that much about tokenizers are extremely important uh so it's really important that you kind of understand at least uh what they do at a high level so why do we need token in the first place uh first it's more General than words so one simple thing that you might think is oh we're just going to take every word that we will have you just say every word is a new is a token in its own um but then what happens is if there's a typo in your word then you might not have any token associated with this this word with a typo and then you don't know how to actually pass this word with a typo into a large language model so what do you do next and also even if you think about words words is a very like words are fine with like Latin based languages uh but if you think about a language like taii you won't have a simple way of tokenizing by spaces because there are no spaces between words um so really uh tokens are much more General Than Words first thing second thing that you might think is that you might tokenize every sentence character by character you might say a is one token b is another token uh that would actually work and probably very well the issue is that then your sequence becomes super long and as you probably remember from the lecture on on Transformers uh the complexity uh grows quadratically with the length of sequences so you really don't want to have a super long sequence um so tokenizers basically try to deal with those two problems and give common subsequences a certain token and usually how you should be think about is around uh an average every token is around three four letters um and there are many algorithm for tokenization I'll just talk about one of them to give you a high level which is what we call bite P en coding which is actually pretty common one of the two most common tokenizers and the way that you train a tokenizer is that first you start with a very large Corpus of text and here I'm really not talking about training a large language model yet this is purely for the tokenization step uh so this is my large Corpus of text with these five words um then you associate every character in this Corpus of text a different token uh so here I just split up every character with a different token uh and I just color coded all of those tokens and then what you do is that you go through your text and every time you see pairs of tokens that are very common the most common pair of token you just merge them so here you see three times the the the tokens T and O next to each other so you're just going to say this is a new token and then you continue you repeat that so now you have to talk which happens three times to with an E that happens sorry two times and an token which happens twice and then ex which also happen twice so this is that if you were to train a tokenizer on this Corpus of text which is very small that's how you would uh finish with a token with a pre like a trained tokenizer uh in reality you do it on on much larger corpuses of text um and this is the real tokenizer of uh actually I think this is gpt3 or chat GPT uh and here you see how it would actually separate these words so basically you see the same thing as what we gave in the previous example token becomes its own token so tokenizer is actually split up into two tokens token and iser um so yeah that's all about tokenizers any questions on that yeah how do you deal with spes and how do you deal with yeah so actually there's a a step before tokenizers which is what we call pre- tokenizers which is exactly what you just said uh so this is mostly in theory there's no reason to deal with spaces and punctuation separately you could just say every space gets its own token every um uh punctuation get its own token and you can just do all the merging the problem is that so there's an efficiency question actually training these tokenizes takes a long time uh so you better off because you have to consider every pair of token so what you end up doing is saying if there's a space this is very like pre- tokenizes are very English specific you say if there's a space we're not going to start looking at the the token that came before and the token that came afterwards so you're not merging in between spaces but this is just like a optimiz like a computation optimization you could theoretically just deal with it um the same way as you deal with any other character and yeah when you merge tokens do you delete the tokens that you merged away or do you keep the the smaller tokens that merge um you actually keep the smaller tokens I mean in reality it doesn't matter much because um usually on large Corpus of text you will have actually everything uh but you usually keep the small ones and the reason why you want to do that is because if in case there's as we said before you have some um some grammatical mistakes so some typos you still want to be able to represent these words by character um so yeah yes are the tokens unique so I mean say in this case T Ken is there only one occurrence or could do you need to leave multiple occurr so they could have take on different meanings or something oh oh I see what you say no no it's every token has its own uh unique ID um so a usual this is a great question for example if you think about a bank which could be bank for like money or bank like water um it will have the same token but the model will learn the Transformer will learn that based on the words that are around it it should associate that I'm saying I'm being very high wavy here but associate that with the with a with a representation that is either more like the bank money side or the Bank water side um but that's a Transformer that does that it's not a tokenizer yes yeah so you mentioned during tokenization keep the smaller tokens you started with right like if you start with a t you keep the T and then you build your tokenizer to the that you can now in token so let's say maybe you didn't train on token but like in your data you are trying to encode token so how does the tokenizer know to encode it with token or a great question you basically when you so when you tokenize so that's after training of the tokenizer when you actually apply the tokenizer you basically always choose the largest uh token that you can apply uh so if you can do token you will never do T you will always do token um but there's actually so people don't usually talk that much about tokenizers but uh there's a lot of of computational benefits uh or computational tricks that you can do for making these things faster uh so I really don't think we and honestly I think a lot of people think that we should just get away from tokenizers um and just kind of tokenize character by character or bites by bites uh but as I said right now there's this issue of like length uh but maybe one day like in five or 10 years we will have different architectures that don't scale quadratically with the length of the sequence and uh maybe we'll um yeah move away from tokenizes so can you share with us the drawback why do people want to move away from the tokenizer oh um yeah so think one good example is uh math if you think about math actually numbers right now are not tokenized so for example 327 might have its own token which means that models when they see numbers they don't see them the same way as we do and this is very annoying because what I mean the reason why we can kind of generalize with math is because we can deal with every every letter separately and we can then do composition where you know that basically if you add stuff it's just the same thing as adding every one separately plus like whatever the unit that you add so they can do that um so then you have to do like special tokenization and like one of the big changes that GPT 4 did uh is changing the way that they tokenize uh code so for example uh if you have code you know you have like often in Python these four spaces at the beginning those were dealt with uh kind of strangely before um and as a result like the model couldn't really understand uh how to deal with code uh so so toiz actually a lot um okay so I'll move on right now but we can come back later on token Isis great so we talked about the task the L the tokenizer let's talk a little bit about evaluation uh so the way that LMS are usually evaluated is what we call is using what we call perplexity um at a high level it's basically just your validation loss uh the slight difference with perplexity is that we use something that is slightly more interpretable which is that we use the average per token loss and then you expon entiate it and the reason why you exponentiate it is because you want I mean the loss has a log inside and you like one humans are actually pretty bad at thinking in log space but two logs depend on the base of the log uh while when you exponentiate you basically have everything in the uh kind of the vocabulary size uh unit um and the average proten is just so that your your complexity is independent of the length of your sequence um so perplexity is just two to the power uh average of the loss of the sequence um so perplexity is between one and the length of the vocabulary of your tokenizer uh one it's simply well if you predict perfectly the thing which uh every word then every word will have basically product of ones uh so the best perplexity you can have is one if you really have no idea you basically predict with one divided by uh size of vocabulary um and then you do simple math and you basically get perplexity of size of vocabulary uh so the intuition of perplexity is that basically the number of tokens that your model is kind of hesitating between uh so if you if your model is perfect it doesn't hesitate it know exactly the word if it really has no idea then it hesitates between uh all of the vocabulary uh so perplexity really improved that's perplexity on a standard data set between 2017 and 2023 it it went from kind of 70 tokens to less than 10 tokens over these five six years so that means that the models were previously as dating between 70 words every time it was generating a word and now it's as dating between like less than 10 words so that's much better perplexity is actually not used anymore in academic benchmarking mostly because it depends on the tokenizers that you use uh it depends on the actual data that people are evaluating on but it's still very important for development of llms so when you when you actually train your own llm people will still really look at the perplexity uh one common other way and now more common in Academia of evaluating these llms is just by taking all the classical NLP benchmarks and I'll give you a few examples later and just kind of aggregating everything um so collect as many automatically evaluatable benchmarks and just evaluate across all of them um so one such if uh or actually two such uh benchmarks of what we call uh Helm which is from Stanford and another one is the hugging face open LM leader board which are the probably two two most common ones right now um so just to give you an idea in Helm there are all of these type of tasks which are mostly things that can be easily evaluated uh like question answering so think about many different question answering uh tasks um and the benefit with question answering is that you usually know what is the real answer um so you can the way that you evaluate these models and I'll give you a concrete example in one second um is that you can just look at How likely the language model is to generate the real answer compared to some other answers and that's essentially at a high level how you evaluate these models um so to give you a specific example mlu is probably the most common um academic Benchmark for llms uh and this is just a collection of many question and answers in all of those domains for example College medicine College physics astronomy and these type of topics and the questions are things like so this in astronomy what is true for type 1 a supernova then you give uh four different potential answers and you just ask the model which one is more likely so there are many different ways of doing it either you can look at the likelihood of generating all these answers uh or you can ask the model which one is the most likely uh so there are different ways that you can promp the model but at a high level you know which one is correct and there are three other mistakes um yes kind creating is like unconstrained text as the output yeah how do you evaluate a model if it give something that's you know semantically completely identical but is not the exact token list that expect yeah so that's a great question I'll talk more about that later here in this case we don't do unconstrained so the way you would evaluate MML is basically either you you ask the first question and then you look at the likelihood of the model generating a the likelihood of the model generating b c and d and you look at which one is the most likely or you can as the model out of ABC d which one is the most likely and you look at whe the to the most likely next token is A B C or D so uh you can strain the model to say it can only answer these four things you say you constraint the model you mean you constraint The Prompt or do you mean of its whole probability distribution outputs you only comparing the outputs like you're only comparing the a so uh in the second case I gave you you would do exactly the I actually you would do both you would prompt the model saying ABC or D plus you would constrain to only uh look at these two these four tokens in the first case you don't even need to generate anything so in the first case you literally just look given that it's a language model it can give a distribution over sentences you just look at what is the likelihood of generating all of these words what is the likelihood of generating the second choice and you just look at whether the most likely sentence is actually the real answer so you don't actually sample from it you really just use P of x one to excel does that make sense uh that being said evaluation of open-ended questions is something we're going to talk about later and is actually really important and really challenging yes earlier you mentioned that um like um metrics like flexity are not are not like usually used because it depends on like how you do your terization some design choices I was wondering if you could speak more to that oh um yeah so think about perplexity I told you perplexity is between one and vocabulary size so now imagine that Chad GPT uses a tokenizer that has like 10,000 tokens but Gemini from Google uses a tokenizer that had 100,000 uh potential tokens then actually the Gemini one will will have like the upper bound of the the perplexity that you can get is actually worse for Gemini than for Chad GPT does that make sense so that's just an idea it's actually a little bit more complicated than that but that's just like one uh first or the bit of you can see that the tokenizer actually matters um great okay so evaluation challenges there are many I'll just talk about two really briefly uh one as I told you there are two ways of doing evaluation for these mlu actually there are many more than two but I give you two examples um and it happens that for a long time even though that was a very classical Benchmark that everyone used uh actually different uh different companies and different um different uh uh different organization were actually using different ways of evaluating mlu and as a result you could you get completely different results for example Lama 65b uh which was the first model of meta in the Lama series uh had on Helm 63.7 accuracy but on this other um Benchmark had like 48.8 um so really the way that you evaluate and this is not even talking about prompting this is really just kind of the the way that you evaluate the uh the models prompting is another issue so really there are a lot of inconsistencies it's not as easy as it looks uh first thing yeah sorry how can we make sure that all these models AR trained on The Benchmark okay second thing this is a great question uh chain test contamination uh this is something which I would say is really important in Academia in uh given that the talk is mostly about training large language models uh for companies it's maybe not that important CU they know what they trained on uh for us we have no idea so for us it's a real problem uh so there are many different ways of trying to test whether uh the test set sorry whether the test set was actually in the training Set uh one kind of cute trick um that people uh in in the lab on T lab have found is that what you can do is that given that most of the data set online are not randomized you can just look at and in that language models what they do is just predict the next word um you can just look at the entire test Set uh what if you generate all the examples in order versus all the examples in a different order and if it's more likely to generate a thing in order given that there's no real order there then it means that probably was in a training set does that make sense um so there are many that's like one of them there are many other ways of doing it train test contamination again not that important for development really important for academic benchmarking great so there are many other challenges but uh I'll move on for now great data um so data is another really big topic um at a high level people just say oh you basically train large language models on all of Internet what does that even mean um so or people sometimes say all of clean internet which is even less defined um so internet is very dirty and really not representative of what we want in practice if I download a random website right now you would be shocked at what is in there it's definitely not your Wikipedia um so I'll go really briefly on like what people do um I can answer some questions but I mean data is on its own is a huge topic uh basically first what you do is download all of Internet what that means is that you use uh web crowlers that will go on every web page on Internet or every web page that is um on Google uh and that is around 250 billion pages right now um and that's around one petabyte of of data so this is actually a common common C is one web crowler so people will usually write their own web crowlers what they do is that they use standard web crowlers and we common crawl is one of them uh that basically every month adds all the new websites that were added on uh internet that are found by by Google and they put it in a big uh basically a big data set um so that's on common call you have around 250 billion pages right now so 1 E6 gigabytes of data once you have this uh so this is a random web page like literally random uh from this common craw and what you see is that one it really doesn't look at type of things that you would usually see but actually so this is an HTML page uh it's hard to see but if you look through you will see some content for example here here uh tesing world is your ultimate source for the system X high performance server and then you have three dots so you don't even the sentence is not even finished that's how a random internet looks like uh so of course it's not that useful if you just train a like large language model to generate things like this so what are some of the steps that are needed first one you extract the text from the HTML so that's what I just try to do by looking at uh basically the correct text uh there are a lot of challenges by through this for example extracting math is actually very complicated but pretty important for training large language models um or for example boiler plates a lot of your forums will have the same type of headers the same type of Footers uh you don't want to repeat all of this in your data um then you will filter undesirable content uh so not safe for work harmful content pii uh so usually every company has basically a a black list of websites that they don't want to train the models on that Black List is very long and you basically say if it comes from there we don't train on this there are other ways of doing these things is that you can train a small model for classifying what is pii removing these things um it's hard every Point here that I'm going to show you is like a hard amount of work uh but I'm going to go go quickly through it so filter undesirable content second or fourth is the dup D duplication as I said um you might have things like headers and Footers in forums that are always the same you want to remove that another thing that you might have is a lot of URLs that are different but actually show the same website um and you might also have a lot of like U um paragraphs that come from like common books that are basically duplicated a thousand times or 10,000 times on internet so you have to duplicate also very challenging uh because you have to do that at scale once you do duplication you will do some heuristic filtering you will try to remove low quality documents uh the way you do that are things like rules-based um filtering for example if you see that there are some outlier tokens if the distribution of tokens in the website is very different than the usual distribution of tokens then it's probably some outlier if you see that the length of the words in this website is super long there's something strange going on on that website if you see that the the website has only three words maybe is it worth training on it maybe not if it has like 10 million words maybe there's something also wrong going on that page um so a lot of rules like this yes why we filter out undesirable content from our dat set instead of kind of putting it in is like a supervised loss right like can we not just say like you know here's this like hate speech website let's actively try to Let's actively penalize the for generating we'll do exactly that but not at this step that's where the posttraining will come from uh pre-training um the idea is just to say I want to model kind of how humans speak essentially um and I want to remove all these like headers photos and and menus and things like this but it's a very good uh like idea that you just had and that's exactly what we'll do later Next Step modelbased filtering so once you filtered a lot of data what you will do uh that's actually a very cute trick uh you will take all of Wikipedia and you will look at all the links that are linked through Wikipedia p because probably if something is referenced by Wikipedia it's probably some high quality website and you will train a classifier to predict whether something comes from whether a document comes from one of these references uh from Wikipedia or whether it's from the random web and you will try to basically say I want more of the things that come from Wikipedia references does that make sense so yeah so you will train a a machine learning uh model usually also very simp simple models because you need to do that really at scale I mean just think about the 250 billion Pages uh next one you will try to classify your data into different different um domains you will say okay this is entertainment this is books this is code this is like these type of domains and then you will try to either um up or down weight some of the domains uh for example you might say uh you might see that actually if you train more on code then actually your model becomes bettered on reasoning so that's something that people usually say in a very handwavy way if you train your model more code actually it helps reasoning so you want to upweight the coding uh distribution because that helps for General language modeling skills uh books is usually also another one that people usually um upweight entertainment they usually downweight uh so things like this of course you want to do it so people used to do it maybe uh kind of theistically now there's entire pipelines that we'll talk about of how to do these things uh slightly more um automatically and then at the end of training uh usually train um after training on all of this data that we saw usually train on very high quality data at the end of of training your large language model where you decrease your learning rate uh and that basically means that you're kind of overfitting your model on a very high quality data so usually what you do there is like Wikipedia you basically overfit on Wikipedia yeah and you overfit on like human uh data that was collected um the other things like continual pre-training for getting longer context I'm I'm going to skip over all of these things uh but I just to give you a sense of how hard it is when people just say oh I'm going to train on internet that's a lot of work um and really we haven't figured it out yet so collecting World data is a huge part of practical large language model uh some might say it's actually the key yes about data so basic question so usually when you start with like the terabyte of data after I go through all that steps the typical amount of data you have in and then like how how large a team does it typically think to go through all the steps you talk about so how is the question how large is the data after you filter yeah after you filter and then to go through all the step how large a team do you need to go through like the the other fation sttion uh how slow is it or how like how how many people would you need to be able to do this uh okay that's a great question I'm going to somewhat answer about the data uh how large is the data set uh at the end of this slide uh for number of people that work on it um that's a good question I'm actually not quite sure but I would say yeah I actually don't quite no but I would say it's probably even bigger than the number of people that work on kind of the two tuning of the pre-training of the model uh so the data is bigger than kind of the modeling aspect um yeah I I don't think I have a good sense I would say probably in Lama's team which have like 70 years people I would say maybe 15 work on data uh I yeah all these things you don't need that many people you need a lot of computer so because for data you need a lot of CPUs um so yeah and I'll answer the second question at the end of this slide so as I just kind of alluded to really we haven't solved data at all for pre-training so there's a lot of research that that has to be done first how do you process these things super efficiently uh second how do you balance kind of like all of these different domains uh can you do synthetic data generation that's actually a big one right now uh and because we don't have uh we'll talk about that later we don't have enough data on the internet um can you use multimodal data instead of just text data and how does that improve even your text performance um there's a lot of seccy because really this is the key of most of the pre-train pre-trained large language models so for competitive Dynamics uh usually these these um these companies don't talk about how they do the data collection and also there's a copyright liability issue they definitely don't want to tell you that they've trained on books even though they did um because if not you can uh sue them uh common academic benchmarks uh so that will kind of answer what you asked um it started so those are the smaller ones it's the names are not that important but it started from around 150 billion tokens which around uh 800 GB of data now it's around 15 trillion of to 15 trillion tokens which is also uh the size of the models that are right now the best models are probably trained on that amount of data so 15 trillion tokens uh which is probably I guess two order of manage bigger than that so 80 uh E3 gab so that would be around 100 to thousand times uh filtering of the common crawl if I'm not mistaken um so yeah one very one very uh famous one is the pile so this is academic Benchmark of the pile and we can just look at what distribution of data they have it's things like um archive PBM Central uh which is all the the biology stuff uh here it's Wikipedia you see stack exchange um some GitHub and some books and things like this um again this is on the smaller side so this is if we look at here this is on 280b so in reality it's like 100 times bigger so you cannot have that much of GitHub and and of Wikipedia um in terms of close Source models just to give you an idea uh Lama 2 um it was trained on 20 two trillion tokens lamb 3 15 trillion tokens which is currently the best model that we know on how much it was trained on which is the same thing as this the the the best academic or the biggest academic Benchmark which is 15 trillion tokens GPD 4 we don't really know but it's probably in the same water of magnitude or it's probably around that actually it's probably around 13 um from leaks if the leaks are true um great so scaling laws um any other questions on Data before you go to scaling laws sorry I know I'm giving you a lot of information but uh there's a lot into training at large language models great scaling laws so so the idea is that what people saw um around 2020 or at least from a long time but they've been able to kind of theoretically show it or impurely show it since 2020 is that the more data you train your models on and the larger the models the better the performance this is actually pretty different than what you've seen in this class in this class we teach you about overfitting overfitting doesn't happen with large language models uh larger models better performance um it's something that really took a long time for the community who took this type of class to realize um but for the exam overfitting exists so okay the idea of scaling laws is that if given that you know that more data and larger models will always give you better performance can we predict how much better your performance will be if you increase the amount of data and the size of your model and surprisingly it works uh so here you see three plots from a very famous paper called scaling loss from openi um here you see on the x-axis compute so how much did you train like how much compute did you did you spend for training and here you see test loss so this is essentially I mean it's not perplexity but it's your validation loss um so it's a log of the perplexity and if you put these two on uh log scale uh then you see that uh the the performance or like the this the sorry the the scaling law is linear uh that means that if you increase your compute by a certain amount you can you can say by how much your test loss will actually decrease same thing with data and same thing for parameters if you increase the data set size your loss will will decrease by an amount that is somewhat predictable if you increase the number of parameters it will decre the loss will decrease by amount which is somewhat predictable this is really amazing um very surprising I mean it looks in nocuous when you look at these type of plots but that's crazy because it means that you can predict uh how well we're going to perform in 2 3 years depending on how much compute we will add assuming that these things will hold there's nothing theoretical about it um yes two things one what is the loss that they're using here is this perplexity or so it's it's you know I said perplexity was like two to the power of the LW so this is the the the power of the perplexity and then the second thing is when you like increase the number of parameters or you increase the total data set size going dat times doesn't that just inherently increase your compute like do all this work to just specific no this is a great question so the compute here is actually a factor of two things the data and the parameter what I'm showing here is that you can um well actually we're going to talk about that in details but basically if you increase the number of parameters you should increase the number of data that you have um so you actually don't go multiple times through the same data set no one does EPO in a lar at least not yet uh because we have still kind of enough data um so yeah this is all the same Trend which is increase compute decrease loss yes have we seen the numbers for the last two years or is it still holding it is still holding I I don't have like good numbers to show you uh but it is still holding surprisingly yes is there no evidence like empirical evidence that you plateau expected PL no empirical evidence of plateauing anytime soon um why we don't know um will it happen probably I mean it doesn't need to because it's actually in log scale so it's not like as if it had to go it had to Plateau like mathematically it could continue decreasing like this I mean most people think that it will probably Plateau at some point we don't know when um okay so that's I'll talk more about scaling laws now so why are scaling laws really cool imagine that I give you um you're very fortunate I gave you 10,000 gpus for this month what model will you train how do you even go about answering that question and I mean this is a a hypothetical but that's exactly what these companies are faced with uh the old pipeline um which was basically you tune High parameters on the big models so let's say I have 30 days I will train 30 models for one day each I will pick the best one uh and that will be the final model that I will use in production um that means that the model that I actually used was only trained for one day the new pipeline is that you first find a scaling recipe so you find something that tells you for example oh like one common thing is that if you increase the size of your model you should decrease your learning rate so you find a scaling recipe such that you know if I increase the the the the size of my model here's what I should do with some high parameters then you tune your high parameter on smaller models of different sizes let's say I will say for 3 Days of my 30 days I will train many different models and I would do highper parameter tuning on these small models each of different sizes then I will fit a scaling law and try to extrapolate from these smaller models which one will be the best if I if I train it for much longer or sorry if I train it for a larger model and then I will train the final huge model for 27 days instead of just one day um so the new pipeline is not train things or do high prity tuning on the real scale of the model that you're going to use in practice but do things on smaller ones at different scales try to predict how well they will perform once you make them bigger I will give I will give you a very concrete example right now uh let's say Transformers versus lstms let's say you you have these 10,000 gpus you will not sure which one you should be using should I be using Transformer based model or LCM based model what I will do is I will train Transformers at different skills so here you see different parameters on the x-axis Y axis is my test loss I will then train different different lstms at different scales once I have these points I will see oh it kind of fits a scaling law I will fit my scaling law and then I will be able to predict oh if I had 10 times more compute here's how well I would perform for the LM it's actually slightly less linear for the lstm but like you could probably try to predict where you would end up and clearly from this plot you would see that Transformers are better um one thing to notice when you read these type of scaling laws is that are two things that are important uh one is really your scaling rate uh which is kind of the uh the slope of the the slope of the scaling law the other thing is your um your intercept like you could start worse but actually become better over time it just happens that lstms are worse for both uh but I could show you another one where things you can predict that actually after a certain scale you're better off using that type of model than others uh so that's why scaling laws are actually really useful any questions on that yeah so these are all kind of very how how sensitive are these to like small differences in the architecture like one one like Transformer architecture versus another Transformer architecture you basically have to like fit your own curve and make basically say like oh scaling law has tell me there should be some like logarithmic function let me extrapolate that for my own yeah so uh usually for example if you're an academic and you want to now at least that's like pretty recent and you want to propose a new like activation uh that's exactly what you will do you will fit a scaling law show another scaling law with the standard like I don't know G and you will say that it's better in reality once you start thinking about it in scaling loss terms you really realize that actually all the architecture differences that we can make like the small minor ones all they do is maybe change a little bit the The Intercept but really that doesn't matter uh cuz just train it for 10 hours longer or like wait for the next uh for the next Compu gpus and these things are really secondary which is exactly why I was telling you originally people spend too much time on the architecture and losses um in reality these things don't matter as much data though if you use good data you will have much better scaling loss than if use bad data so that really matters uh another really cool thing you can do with scaling laws is that you can ask yourself uh how to optimally allocate training resources should I train larger models because we saw that it's better when you train larger models but we saw that it's also better when you use more data so which one should I do should I just train on more data a smaller model or should I train a larger model on less data um so chinchilla is a very famous paper that first showed this uh the way they did it I want to give you a little bit of a sense of what these plots are uh here you see training loss again on the x-axis you see parameter parameter differences uh sorry parameter size uh number of parameters so the size of the model and here all these curves are what we call isof flops which is that all the models on this curve H have been trained with the same amount of compute um the way that you do that is that you train you change sorry you vary the number of tokens that we trained on and the size of the models but you vary in such a way that the total compute is constant okay so all these curves that you see with different colors have different amount of computers that were trained on then you take the best one for each of those curves once you have the best one for each of those curves um you can ask you can plot um how much flops it was and which curve were you on and how much parameters did you actually use for training that specific point you put that on the on the log log uh scale again and now you fit a scaling law again so now I have something which tells me if I want to train a model of 10^ 23 flops here's exactly the number of parameters that I should be using 100 100b and you can do the same thing with flops and tokens so now you can predict if if I tell you exactly I have one month of compute what size of model should I be training F your scaling law and I tell you um of course that all looks beautiful in reality like there's like there's a lot of like small things of like should you be counting like embedding parameters like there's there's a lot of complexities but if you do things well these things actually do hold um so the optimal number of parameters that that chinchilla Pap have found is to use 20 tokens for every parameter that you train uh so if you add one more parameter you should add you should train your thing on your model on 20 more tokens so one caveat here is that this is optimal training resources so that is telling me if you have 10^ 23 FL or if you have like 100 I don't know how much that is100 million or 10 no that's much less actually let's say I have $5 million to to train my best model that gets the lowest loss how how what would I train on in reality these companies need to think about inference also if you have a smaller model they will spend less over time um so actually if you consider the inference cost you have other papers that Tred to show that um it's around 150 uh parameters per sorry tokens per parameters because you prefer having a smaller model cuz over time you're going to you're going to actually um spend less money on inference of these models so 150 to one that's around what the best models are trained on right now at least the ones that are that are used um in practice for in production great any question on chin great oh sorry in practice how expensive is inference for these models rela to train actually very expensive uh I will not talk about inference because that would be another entire lecture but just think about Chad GPT where they have I don't know how much it is now like 600 million people that used it um like that's a lot um yeah so it's actually very expensive there's a lot of optimization you can do for in though um and that's an entire other lecture so I'm going to skip that uh this time but it's very interesting okay tuning um as I said there are many things that you can uh answer with scaling laws I just try to give you two examples uh but really there are many things what data do you use what mixture what data mixing waiting you use data mixtures that's what we talked about before uh what architecture you use whether you should make your models uh wider or deeper um should you be paying for more gpus or actually collecting more data um all these things are things you can try to answer with scaling laws one thing I want to say is the bit lesson if you ever heard of Richard sudden a very famous blog post in 2019 um what he realized uh which I think not enough people realize I didn't definitely did not realize at that time um is that once you see these type of scaling laws you know that the more compute you have the better models you will get so with skill you will get better model and you also know by Mo law or these type of variant of Mo law that you will always have better compute then the only thing that matters is just to have architectures that can leverage computation so what matters is basically systems data and less so the architecture like the small architecture differences like your your your activation and things like this uh so I think that's like one of the reasons why most of research focuses on um some things that for industry matters less and I was one of those researchers for a large part of my my career um so don't spend time over complicating do the simple things do it well seal them that's really what openi taught us with um with chat gpg and with all the gpts before okay I want to give you some backup the envelope computation so I might be off by a few factors here but I just want to give you a sense of how costly it is to train some of these models I'll give as an example Lama 3 400b which is currently the best open source model that you can get uh it was trained on 15.6 tokens it has 45 billion parameters so just now that you know what is like this uh optimal tokens per parameter that's around 40 so that's a little bit more than chinchilla but less than this like inference uh optimal um model so they went for training optimality uh flops for this model so one simple uh way to compute flops is six uh times the number of parameters times the number of data you train on uh so if you do the simple calculation here it's 3.8 e25 flops the reason why this is important is that if you follow the little bit the news there's an executive order from Biden that basically says that once you have uh 1 e26 parameters uh sorry flops uh then you have special scrutiny on your models so they went 2x less than that so they really went right below this to not have special scrutiny so 38 uh I might be off by a little bit but it's definitely under the 1 26 oh um so paramet p is parameters n is data number of tokens this is a uh this is just an approximation we yeah okay uh compute and we know that they trained on 16,000 h100s um and we know the throughput but they they said it too uh so if you do the computation it takes around 70 days um or 26 million GPU hours at least that's with my uh back of the envelope computation they actually said that they use 30 million instead of 26 million GPU hours um so maybe they had like some uh some challenges I don't really know but if you follow the simple computation it's around 70 days um cost uh I mean this it's hard to to approximate but I'm just going to say it's kind of the rent like what if I were to rent h100s that many h100s for that many days how much will I pay uh h100 a lower bound on the on the renting uh cost of h100 is around 2 hours uh $2 per hour so if you multiply this by 26 million uh hours uh you get 52 million uh dollars so they probably pay less than that but not actually much less because all these um all these services that actually rent gpus they don't make that much money so it's it's probably slightly less but not that much less um now salary I said 50 employees 500k per year say yeah it's probably the right ballpark 25 million uh so if you put all together around 75 million um dollars for training uh this Slammer model I'm probably off by like 10 million but but that's kind of right uh bpk carbon emitted um a lot of people might ask like also the cost is not the only thing that is important so I did the computation um it's around 4 uh 4,000 um tons of CO2 equivalent that is actually only 2,000 return tickets from JFK to uh London so right now uh carbon emitted is actually not uh I mean it's huge but it's not like um meaningful yeah yet I think in maybe GPT 6 gpt7 once you multiply this by 100 that might become a real issue right now it's still not uh I think um an issue in the grand scheme of things next model the way you should be thinking about these models is that every new generation the number of flops essentially uh multiplies 10x or at least that's what they try uh if they have enough energy and if they can buy enough gpus uh great any question on these back of the envelope math no okay so now we talked about pre-training I wanted to also chat about systems because now we know computer is really important so there's a question of how do you optimize the how do you optimize your computer I will leave that for the end because I'm not sure how much time we will have I think it's important but hopefully I I'll be able to to talk about it later it's slightly different than what we've been talking about right now so I'll move on to post training for now so the task of post training ER the reason why we need to do Post training is as I told you before um it's to make AI assistants so language modeling is not uh really the thing that you want when you have an AI assistant uh for example if you ask to gbd3 which is a purely language Model A pure language model not a um not an aligned one if you ask a question like explain the moon landing to a six-year-old the completion that you would get is something like explain the theory of gravity to a six-year-old because what it learned is that on on on internet if you have one question you usually have maybe another bullet point of other similar questions you don't usually have question and then answer later uh this is not what you want from an AI assistant so how do we uh do this alignment which is this post training and making these models assistance um so the goal of this alignment is to basically get LMS follow the instructions that are given um by users and and maybe some designers kind of desires um so think about moderation you don't want the model like open ey definitely doesn't want the model to say stuff that is very toxic um so here you see on the left hand side uh that when you ask a question it actually provides a a real answer so it's not like uh before the llm and on the right hand side you see that it would if you ask to write a tweet describing how a certain part of the population are evil it will say that it cannot do that um so that's kind of this alignment uh the background here is that uh basically the data that you want for training some of these models um is like we know what we want which is just asking humans this is a question this is the answer that you want uh but the thing is that it's very expensive to collect that data and it's hard to find it online uh in contrast pre-training data is not what you want but there's a lot of it um so what what we will do a the main idea is simply take a pre-train large language model pre-train all of internet and then you just fine tune so you just change a little bit of weights on the type of data that you actually want and hopefully given it you already pre-train it on all of Internet it basically learns or knows how to speak in English and and knows a standard um language syntax uh then you can really find tune in with very little data okay sft so supervis fine tuning is really exactly what I just said which is the idea of fine-tuning the large language model on uh basically the desired answers that are collected from humans um so why is it called supervis fine tuning because you basically want to do language modeling on the real ansers so language modeling is this like next word prediction and and that's the fine-tuning part and then you want to do it on desired answers given by humans so that's why we call it supervis so how do we collect this data well we I just said it you just ask humans uh to to tell you this is the this is a question this is the answer that you uh you would want from some of these models so this is an example um sorry I can't read very well on my computer but uh my kid uh needs to do a science um no let's read this one can you write a short introduction about the relevance of the term monopsony and then it says monopsony refers to a market structure blah blah blah and that's a human that wrote that um so actually this is open Assistant which was a a way to collect um uh data online by humans so this type of supervised fine tuning or alignment is really the key of Chad GPT this is what made uh the big jump from gpt3 which was mostly something that was known by AI researchers to Chad GPT which became known by basically everyone um so the problem with uh human data is that it's uh very slow to collect and very expensive um so one possible simple idea is to use llms to scale data collection uh so that's exactly what we did with alpaca uh one year ago what we did is that we asked uh humans or we use a data set of human uh question answers so there were 175 uh question answers here and we asked the best mod at the time so text3 to basically generate many more of these question and answers so all we did is like this is what humans would write now write similar answers and similar questions and we collected 52,000 LM generated question answers and then what we did is simply we took Lama 7B which was the best pre-train model at the time and we just fine- tuned this with supervised fine tuning as I told you and that's how we got um the Alpac s7b model uh and this is the type of data that we collected so things like what does algorithm mean an algorithm is a step by a stepbystep uh set of instruction used to solve a problem or achieve a goal blah blah blah blah so the data is not actually it's actually pretty good given it was LM generated by LMS from essentially two generations ago um so that really started at least for us kind of as an academic replication of chat GPT uh now it really there's a big field of like synthetic data generation of how to use llms to basically make development of llms faster um and by basically by decreasing the amount of of human hours that you need quantity of data so we talked about what type of data and how we collect it um one thing which is surprising with sft is that you don't need that much data uh so what this paper showed this is called Lima is that if you have if you scale the amount of data that use from uh supervised fine training from 2,000 to 32,000 it really doesn't help much so here scaling laws definitely don't help um so the the intuition here is that all you learn um is is you learn how to format your desired answers another way of saying it is that your pre-trained models they essentially model the distribution of every user on internet one that might write bullet points another one that might answer qu answer question with an answer so all you tell your model is like wait you should actually be optimizing more for this type of user than another one so you're not actually teaching it and you're not teaching anything through this um sft uh so supervis fine tuning all you do is you tell the model to kind of optimize for one type of user that it saw already in a pre-train data set so the knowledge is already in the pre-train llm uh and you basically just specialize to one type of user great any question on sft yes so I know it's a big issue with synthetic data where uh if you keep generating data from the same distribution eventually you're not learning a new distribution you're essentially playing with it it just bootstrapping that yeah surely you can't scale that forever right you can't keep going on and generating from the same distribution you hope to learn something new yeah uh so are there it's an active area of research but any thoughts that you have around how people are maybe thinking around this and uh better ways to bootstrap or to give up on this idea and and realize that the chart shows you don't need that many so just get humans to generate 2,000 really good uh yeah so that's a very good question uh so for the data stuff so I'm saying it's not that important for sft but there will be another thing we'll talk about right after where actually data does matter my intuition based on not that much empirical results is that you can still get um even though you use your LMS if you use purely LM generated text and you do that for like three four generations of llms I agree with you that probably you won't improve much but for me what is important is how do you use like human in the loop with llms not purely LMS not purely uh humans but maybe what you can do is just have the model generate some new text and just uh humans write a few Edits edits are much faster than writing the entire text and I think that if you have that type of collaboration then from like kind of an information theoretical point of view you still get additional information but you still much faster than if you use humans and I think that as a field we'll probably move towards these type of things uh which is um really just finding the examples that are important and and asking humans it's kind of active learning just asking humans exactly when uh you need to to get inputs yes do we train with like the same loss function the same like General training algorithm for the supervis tuning bit as we do for the for the pre-training right because like the examples you showed I think the the important thing of the good examples is they're like supera accurate there's these more complex still just like chain same so that's why here I yeah I didn't maybe didn't emphasize enough this is just language modeling fine tun the LM with language model on the desired answers so this is literally the same loss um it will be different in two seconds but the first step of sft is literally the same loss where you just say Okay I want to actually specialize on that type of data so there's even a question of like what is pre-training what is post-training because in reality it's just like a different data that you use the reason why we usually call it post training is that the way we collect that data is very different great great questions uh yes maybe it's the same question but why would these 2,000 examples have such an overweighted influence you tun so that's why we uh also that's another reason why we call it post training is that we use different type of hyper parameters so you know I told you basically at the end of pre training you essentially end up with a learning rate of zero and here you're going to increase your learning rate so like 1 eus 5 one E Yeah and and so um the weight that you give to them is actually different um okay uh Second Step or second part of this post training um is what we call reinforcement learning from Human feedback or rhf uh some of you might have heard of that um the idea is that sft has a problem namely that uh you do behavioral cloning which means that you just try to clone what the humans would say and that had that has many issues one of them is that you're bound by human abilities so if um like humans actually humans won't generate the things that they think is actually the best thing to generate so if you ask me to write a book I mean I can definitely enjoy a book I can probably say one book is better than another but I'm definitely not going to be as good as writing the book that I want to read uh so you're going to be bound by the human ability to generate things even though the humans might be better at distinguishing between things that's one issue issue number two uh I find that actually pretty interesting is that it might if you ever heard of the word hallucination so this is llms generating F like false information hallucination might these people have um hypothesized that that can come from the supervised fine tuning even if you do supervised fine tuning on data that is correct and the reason why that is is that if uh given I told you that basically sftt is with very little data and it's with data that doesn't the model doesn't learn anything new so what if the human gives an answer that the model didn't know was true from the model perspective you the human basically is telling the the model uh generate this thing that seems plausible but actually have no idea if it's true or not um so just to give you a very concrete example if we go back to this uh monopsony example can you write blah blah blah about monopsony uh imagine that a human uh wrote a reference on this type of book um and that book might exist that might be a correct reference but what if the llm never saw this reference during pre-training then it doesn't know that it's a correct reference so really what you tell the model is to generate or make up some plausibly sounding reference um rather than actually tell the real reference that it saw during pre-training uh so hallucination might be um uh a re like might be caused by this sft that's problem number two does that all make sense great problem number three price generating the ideal answers is very pricey and that comes back to your question um of like humans writing answer is actually pretty expensive um so that's where rhf comes in the idea is that instead of cloning the behaviors of humans we're going to maximize human preference um and the way we're going to do that so the pipeline is that for a certain for every instruction you're going to ask a model to generate two answers um and usually use a pretty good model so you usually don't use an LM here you use a sft uh fine tune you use a fine tuned llm already to give like pretty good answers and then you ask labelers which of these two answers was better so select the preferred one and then with different type of algorithms we're going to talk about the algorithms um you just fine-tune the model to generate more of the green thing than the red thing so more of the good stuff uh so now the question is how and we're going to talk about that right now so there are two ways that we're going to talk about and two that are mainly used in the community um the first one is simply the idea of of using reinforcement learning so hopefully you all know what reinforcement learning is now um so when you think about using reinforcement learning one important question is like what is the reward that we're optimizing uh so in this case there are really two options that I could think about the first one you could just say I'm going to compare the output generated by some baseline the output generated by my model U and I'm just going to ask the human to say which one is better and I'm going to use this as a reward so if I'm better than the Baseline this is a plus one if not it's a minus one one uh so now it's binary reward the problem with binary reward is that it's very sparse and you don't get much information out of it uh like maybe your answer was slightly better maybe it was like way better and you don't really know from this um how much better it was so option two is that you can train what we call a reward model which is simply a classifier uh so you use machine learning to to classify how much better uh two outputs are from the preference from the perspective of the human um so this is a little bit meta but what you basically do is that you train uh you take um a reward model R which is a uh just a large also a large um a large classifier and you basically ask this reward model you give it the input and the actual output that you have one of the two outputs uh and you just um exponentiate that so that's the soft Max law that you all know about and now you divide by um the the exponential reward uh on the first example sorry on the first output and this is on the second output and you basically train so the reason why you do that is that you train your your model you train this reward model to be able to classify um how much better one output is to another one so another uh slightly less convoluted way of saying it is that your reward model will output some reward that will be used as the logits of your soft Max so now if you have high logic in your softmax it means that you highly likely this um output is better uh so that's what we call Bradley ter model yes is this reward model going over the entire output or is it going um so this takes the entire uh yeah this takes the entire output at once so it takes all the input and all the output and it gives one number yes would human be sorry with the reward model where would a human be like oh I see okay sorry maybe I wasn't clear um you train this reward model to fit this green and and red preference from humans so basically you train a classifier to say whether the humans prefer red or green uh but instead of using the binary reward which is what the human would tell you you basically use the logits of the soft Max and the thing with the logits is that that logits are continuous so now you know that if your reward model said it has high logits then in some ways the human highly prefer this answer to some other answer great um so as I just said continuous information so it's better so that's what people uh use in practice or at least used to use in practice I'll tell you about uh the other algorithm later uh so what you do at the end is that you basically try to just use reinforcement learning that you know about now we know we have reward what you sample through is the generation from your large language model um and then you just use some regularization term so the reason why you do this regularization term is for avoiding what we call over optimization so this reward model might not be really represent like might not perfectly model human preferences so you don't want to maximize this thing to essentially Infinity um and you do it using uh po which is a common uh reinforcement learning algorithm um one thing to note here because it will be important for later is that when we use maximum likelihood um sorry now the large language models are actually a policy for your reinforcement learning it's not maximizing maximum likelihood anymore which means that you're not modeling any distribution anymore and the reason why this is important is that models that went through this type of Po actually don't give you likelihoods of text that are meaningful cuz what you optimize them to do is B basically just optimized for generating the most likely thing not optimize for modeling like all the answers that humans might say another way of saying that is that there's nothing that incentivizes here the model to not give a like a um a single possible generation nothing here says it's good if you have some distribution with some entropy um okay if you haven't followed it's not that important but just good to knowe great so PO is exact what chat GPT did originally so here's the on the blog post or what they have is step one do supervise fine training which now you all know about step two train a reward model on human preferences step three do po multiple steps which is where you see this this blue arrow so you continue you train the model once with po you collect new data you continue uh and that's why and that's exactly what Chad GPT did uh that was a big breakthrough between gpt3 and Chad GPT one thing to note is that uh P has many challenges reinforcement learning is something that's super nice theoretically in practice anyone who ever worked with reinforcement learning knows it's such a mess uh there's a lot of things like roll outs out of Loops clipping so many complications um so it's messy this is the idealized PO used for LM settings so that's already much more complicated than this expectation we saw before and in practice it's actually much more complicated so we have one implementation of it that we had to do and I'm not going to go through it but basically you have like so much stuff that you have to think about when you implement that type of of uh po algorithm so you have clipping everywhere you have a lot of complexities and things are not well documented all this to say um that we're going to there was a new method that was proposed uh also from Sanford one year ago called DPO which is essentially a simplification of Po um and the way uh what they did or the idea that they have is that instead of using reinforcement learning you can just maximize the probability of generating the stuff that you like and minimizing the probability of the stuff that you don't like uh so if you think about the human preference the red and green maximize uh green minimize red um so the loss is actually this one uh where what you see this is simply um some log of the model so this is the likelihood of a model generating the things that the human preferred given the the inputs um and what you try to do is basically maximize uh the likelihood of generating the things that you like minimize the likelihood of the things that you don't like um all the rest of the terms here it's not too important it's actually really not that complicated to understand but at a high level it's really just maximizing the things you like minimizing the the rest um and one thing to note uh which I was going to say just here is that actually all the rest is chosen such that um the global Minima of of Po and a global Minima of like this DPO under some assumptions are essentially equivalent so this is the right thing to do mathematically I'm not going to go through the derivations but that's the right thing to do uh it's pretty different with Po in the sense that now and with P what you had to do is collect the human preferences then train a uh reward model with maximum likelihood then use reinforcement learning now all you do is basically maximum likelihood much simpler yes I mean yeah so it seems like this is a much simpler and B like what you just intuitively do if this why did they start with this reward model like what what led them doing that I think it's a great question uh I don't really know what I can tell you is that at open ey the people who did the um uh who did basically this PP uh sorry who did Chad GPT initially are the ones who actually wrote Po and I think they were just like there are a lot of reinforcement learning people and I think that for them it was very intuitive um so there's also some additional like potential benefits for example I don't want to yeah for example if you use the reward model uh the cool thing here with reinforcement learning is that you can use unlabeled data with the reward model so here you can only use the label data for doing DPO um for PP for po you first train your reward model and then you can use unlabeled data uh where the reward model will basically label this unlabeled data so there there's additional kind of potential uh there could be potential improvements in practice it happens at down and on and I think just that a lot of people in this team were reinforcement learning experts including uh the main author of Po John hman um so much simpler in poo and is basically performs as well uh so now this is the standard uh thing that people use at least in the open source Community I believe it's actually the standard also in in Industry so that's called DPO gains um so those are all the papers on the left here this is on a summarization task you see all I want to show you is that basically the pre-train models uh were okay and they improve with scale if you do supervised fine tuning you improve them a little bit more if you do po or something with all HF with human feedback you get performance that are as often times depending on a benchmark even better than uh humans so this is the human uh reference summaries same thing this is on a uh on a paper that we have Alpaca Farm where we see uh the evaluation here is not too important but basically you see pre-train model you jump to sft and then you jump to PPO and popo have the exact same performance so basically all HF helps that's kind of the conclusion and DPO is simple uh data uh the way that you collect that type of data um first idea is just use humans as we already talked about uh guidelines are very complicated for what humans should be labeling and and it's really not that easy and actually if you ever do some of the labeling you will see that it's extremely complicated like if I zoom in to this uh here I have a question tell tell me about self-driving cars and you read both self-driving cars are vehicles that are capable of detecting their surroundings blah blah blah self-driving cars are cars that are equipped with sensors blah blah blah to navigate without the need for a driver I mean both seem okay like which one is better it's actually hard to say at a glance um and as a result uh the problem with humans is that you will start optimizing a lot of like high level features for example the second one is longer I can guarantee you that most humans will choose second one even though I mean maybe the first one is better I don't know I haven't read it carefully so challenges with humans first slow and expensive uh second as I just mentioned it's hard to focus on things that matter like correctness and people uh usually look at things that don't matter as much like the form like length uh and as a result so what I show here is that uh when you do lhf the more you do of lhf the longer the output of the of the models become so if you've ever been annoyed at chat GPT answering you super long sentences this is because of all rhf um annotator distribution shift uh like the distribution of annotators that you use matters a lot and you have to think like what is what is even the humans that we want to represent in these models uh now the question is like crowdsourcing ethics uh like usually these basically a lot of the the labeling that is done um like the people who do them are not paid well and they have to go through a lot of toxic data uh because you basically want the model to avoid saying the toxic data um so crowdsourcing ethics too so many challenges with human data um so what we did also last year is again the same thing as alpaca just the idea of like oh well they're challenges with humans maybe we can just replace them with llms uh so what we did is simply replace um oh I see that I'm just realizing that the slides are not sented anyways uh you replace a human preference with LM preferences uh so here on this uh figure you see on the xaxis the price that we paid uh for collecting human data it's around $300 for 1,000 examples and this is on mechanical turkers which are usually like cheaper than than maybe some of the other um companies that you could go through and on the Y AIS it's basically the agreement with uh other humans with the mode of other humans and what you see is that actually as I told you before labeling is really complicated humans agree with themselves only around 66% of the time on a binary Tas and it's not that the humans are not good here because uh we were five main authors on this paper we tried to label this data ourselves and we only had like say 67 or 68% accuracy even though we talk like we talk for like 3 hours of how we should be doing labeling really it's complicated it's not an easy task um and here I just showed many different models and um basically you see that models are much cheaper and they can actually get higher agreement with the mode of humans than human humans themselves and the reason why is because humans have a lot of varant models have no varant so they might be a little bit more biased but have less virence uh so it works surprisingly well and now it's kind of the standard in open uh Source Community I think even in Industry a lot of people use both humans and llms for improving uh the colle collection of allf data um and this is like this is the paper from last year but honestly now it's more like that llms would be around this agreement and this cost so around I would say 50x cheaper than humans and better agreement with human than humans themselves okay so that gets us to evaluation of post training um that goes back to your initial question at the beginning of the lecture how do you evaluate something like chpt uh the answers that chpt could give are basically unbounded and it's not that there one right answer there are many answers that are just as good um so there are many challenges one you can't use validation loss because one method might use po the other one might use DPO validation loss is not comparable second you can't use Cal uh sorry perplexity that's the thing I told you before these models uh are not calibrated they don't give distributions they they just optimize for one thing so you can't use perplexity for actually evaluating uh these type of models once they're aligned sorry one Z lined third uh there's a large diversity of questions that human might ask to these models generation open QA like some question answering some summarization and all of these things so there's so many things you have to cover um then the tasks are really open-ended so it's very hard to automate so that's what you were alluding to before so the idea uh is that instead of trying to come up with really easily automated uh benchmarks uh it's just we're going to ask questions that that users actually ask to these models in practice and we're just going to ask annotators to say between these two models which one is better like what's the what's the better output so basically do exact same thing as um basically the data from rhf but you use it now for evaluation yes I'm not sure I understand what you mean by like can't use perplexity and not calibrated right like LM is still doing like next token prediction so I can't so think about um the optim solution after doing PO is basically one model that gives you uh essentially a Delta um like basically says that there's only one sentence that is that could be generated for that question so now if you use it on something that is slightly semantically differently different it would actually give a likelihood of zero for that answer so in reality it's not that extreme because as you say it's still a distribution but I just shows you that there's a there's a fundamental issue with perplexity once these models are not llms anymore they were not trained at least with P they were not trained to to do maximum likelihood anymore they were trained to be policies okay um so probably the most common or like the most um yeah the most common Benchmark or the most trusted one is what we call Chad uh sorry chatbot Arena uh which is basically go on internet have random users on the internet blindly talk with two chat Bots just ask many questions see the two answers and rate which one is better and and you do that over hundred of thousands of users and then you get uh the actual preferences and you get rankings of models uh so you can go right now on chatbot Arena and actually interact with these models um one potential issue just to highlight is that while people who want to do these type of things are usually more like Tech driven um or like techsavvy uh so a lot of the questions that you will ask are more like Tech stuff discussing software errors inquiries about AI tools and all these things um so another issue is cost and speed if you really want to use something like this for development process um it will be too costly because you would need to basically pay a lot of humans to do that so one simple idea is again as we said many times just use LM instead of humans uh you probably know the drill at this point uh steps for every instruction generate outputs by some baseline and the model that you want to evaluate um so here you imagine that I I'm comparing an answer from Chad GPT and from I'm just asking a model uh another model uh which one is better and I just basically average that out uh yeah I asked gp4 which one is better I average that out over my entire distribution over my entire Benchmark or data set and that gives me a RN rate so RN probability for one model compared to another one and now you can rank models uh and this is the Alpa eval uh leaderboard so the benefits of this is that actually we show we get 98% correlation with Chad B Arena so very high correlation with humans um so this is yeah comparison with correlation with other benchmarks and it takes less than three minutes and less than $10 to run so it's pretty cheap um there are downsides though uh one of them is purus correlation um so as we already saw before LMS prefer this is one SP correlation not many I'll just talk about one LMS prefer longer outputs actually humans also prefer longer outputs but the problem or the issue once you use llms is that once there bias you will continue optimizing that humans at some point I can guarantee you if I ask a simple question and you give me five pages of answers I'll be like no I don't like that answer but LMS if they have this bius and they were trained for that they will continue preferring longer outputs so uh here we see um the the preference just showing that like humans and models prefer longer outputs um and here is another view of the initial apaka eval data uh Benchmark where when we asked um when we we rank gp4 when we look at the Run rate of gp4 versus actually uh gp4 itself if we com if we use the standard GPT 4 it gets 50% kind of by definition because we're comparing GPT 4 versus gp4 but if we ask a gbd4 to be slightly more verose so we just say in the prompt be Vos in your answers then it gets a r rate of 64.4% so really there's a huge variance and if we ask it to be concise it gets 20% so there's a huge variance depending on um whether you ask it to be concise of that's very annoying um so one possible solution which is what we did is uh just use some regression analysis I'm not going to go into details but basically use Cal inference tools to control for length and right now uh actually length matters much less so if you ask it to be veros we still get some gains but much less great so that's all about post training and now for the next eight minutes I might talk about systems or just answer questions yes can you um go back to your post training in terms of post training how did we tune those parameters using the small body of fine-tuning data and have such big effect on the model you mentioned earlier that there's a different set of hyperparameters are we changing just some of the weights the later weights or all the weights what's actually happening yeah uh yeah I I kind of skimmed through all of this you change all the weights actually um industry would change all the weights in open source land you might have heard of Laura which is going to change basically only some of the weights or it actually to be more specific it's going to add some differences to the output of every of every layer but but in Industry you're going to just fine tune all the weights um and also to say something else about the data actually the SL St all HF you usually going to collect uh a lot more data than with sft so if fft is like 5,000 10,000 maybe 50,000 with rhf I think you're going to be more around like the 1 million uh order of magnitude it's still much less than pre-training though yeah because pre-training is 15 trillion tokens I mean this is like that's not even a drop and yet you influence the weight a lot so because you do it I mean you have to think that how you do it is you use um I mean as I said the learning rate that you're going to use is going to be different but also you only do that so just imagine if I train even if I train on one sentence but over and over again all at some point my model will only that sentence even if uh it was just one sentence instead of the 15 trillion tokens so if you use a large enough learning rate and for enough time you will basically overfit that sentence so the the the key thing to to remember is that um the data is not I it's not as if you mix some posttraining data and some pre-training data you do pre-training and then you just start fine-tuning only on the post trining so another way maybe another perspective is that the post the pre-training is just the initialization of your model and once you view it that way that this is just initialization of Weights then there's nothing special like you don't need to remember that you train a lot of data before the only thing that matters is that you had an initialization and now I actually train a model so maybe think about it that way like there's a there's a mark of property in some way just like you had your weights this is my initialization now I'm training that one does that kind of answer your question kind of but you said something just now about it's almost the equivalence of just rerunning the find tuning data many times is it actually is that what actually happens in order to give so much more preference um you might I actually don't know right now how they do it in Industry when we did alpaca we had to do three box so you did run it three times to it um but I mean even the number of times that you run it through it's actually not important the only thing like the only thing is the is kind of the effective learning rate that what matters um so yeah great so I think I have five minutes [Music] right okay I might try to give a high level Overview at least from one of the systems trick systems as we said uh for everyone Bott neck is a sorry compute is the huge bottleneck uh one question you might ask is why not buy more gpus uh gpus are expensive but also are scarce even if you have $10 million right now you cannot buy the best gpus um there's oh yeah there's also some physical limitations when you have when you have multiple gpus you have to communicate between them that takes time um so just buying more gpus is not that easy um so it's really important to think about how do you allocate resources and how do you optimize your pipeline so system 101 on gpus I'm sorry I'm going slightly faster I hope for that some of you at least can follow uh gpus are basically optimized for throughput CPUs are optimized uh for latency so gpus the way you have to think about it is that there's one Comm there's one command that is run on many many Calles at the same time on different type of data um so this is how you see a GPU you see there are many different CES we call them streaming multiprocessors which is very different than the usual CPU architecture so just think High throughput paralyzation for gpus uh gpus are optimized for fast matrix multiplication so every time you will do uh you will do something on GPU if you can do it with a a matrix multiplication it's going to be 10 times faster than with anything else uh that is a little bit annoying because it means that we're kind of uh bottlenecked to doing anything with Matrix multiplications um another thing to note with gpus is that compute has been improving faster than memory and communication so right now gpus usually are hard to keep uh like the data that you send that send to gpus is actually hard to keep up with the processess so most of your gpus are actually going to be idle if you just run normal code if you don't optimize your code so communication and this will continue over time another thing to know about gpus is that there's a memory hierarchy this is the same thing actually with CPUs but basically the closer you are to your cuse the less memory there is but the faster things run if you're further more memory slower um okay I'm going to skip that okay actually I'm going to say it I told you about this uh the fact of communication uh the metric that people usually look at is model flop utilization so what is the theoretical maximum that GPU could run at no more flops that you could use per second divide sorry the number of OB observed through put divided by this theoretical um maximum and in general if you reach 50% you're very happy like Facebook I looked at Lama was at 45 or something like this so that that means that data doesn't come fast enough even for these big companies so one simple trick and that might be the only one I'm going to tell you about is low Precision one simple idea is that well if I'm going to put my floats in lower Precision then there's going to be fewer bits that I have to send to my gpus if there's fewer bits it's faster communication lower memory consumption things are going to go faster uh and for deep learning it just happens that de decimal is not that important uh so so when you do matrix multiplication when you do like for example SGD there's already so much noise that if you update something by 0.01 or 0.015 who cares uh so basically instead of using uh 32 bits per float which is um what people used to use or 64 for example which is what you would use in other domains you use 16 bits uh for matrix multiplication so for every float you use 16 bits um and for training you have this type of like uh what we call aut atic mix Precision which is that uh some of the things are in 32 bits others are in 60 bit in 16 bits um generally the way you should be thinking about it is that your weights are stored of your model are stored in 32 bits um but just before the computation you put everything in 16 16 bits like this you do computation super fast and at the end you update your weights in 32 Bits And the reason why you do all the updates in 32 bits it's just think that if your learning rate for example is very small you still want to be able to like make a difference in your weights uh so all the computation is done in 16 bits but the weights are actually stored in 32 bits so that's like the standard way that people are doing it um okay I'll actually talk just about this and then I'll skip all the rest operator Fusion because I think this is actually pretty cool as I just said communication is very slow and actually every time you use a pie torch line it basically moves variable to Global memory of your GPU so when you have something like this x do cosine uh equal X1 and then you do X1 do cosine what is happening behind the scenes is that you take the X which is data you ship it to your um to your actual processes of your gpus you apply the coign you ship it back to the main memory of your GPU and then you see the next sign you ship it back to the computer to the GPU processor you apply another cosign and you ship it back again um so another way to see that is that you go from your Dam which is your Global memory in your GPU and you ship it to compute you ship it back for every line This is a naive way of doing it this seems very wasteful um so the idea simple idea of operative Fusion is just communicate do all the computation ship it back once and this is exactly what fuse kernels are um so if you ever want to make your comp your computations in pytorch much faster just apply torch. compile on your model this is going to make your model around two times faster and what it does is simply that it rewrites your code uh your P like your py torch code basically in C++ in Cuda uh to to do the communication only once then do all the operations then uh ship it back okay I'm not going to have time to talk about tiling tiling is important paration paration is important um and mixture of experts mixture of experts is important Outlook there are many things we haven't T talked about we haven't talked about architectures we definitely haven't talked about inference um there are many other things that are important with LMS what is the UI that you use I mean arguably chat jpt the big novelty was just have a simple UI to use it multimodality what are all the misuses you could have uh the fact that there might not be enough data on the internet to train all these models legality of data collection so many other things if you are interested in all these topics uh I would suggest three classes cs224n is probably the one that touches the least on uh LMS uh but it gives some background and historical context um of all the LMS and gives kind of some adjacent material CS 324 I think it's called Uh I think it's just called large language models uh more in-depth reading and lectures on everything I talked about CS 336 which is large language model from scratch you actually build your own llm uh it's an amazing class also given by my two supervisors very heavy workload so be careful and um great"}], "15. Input Markets I\u2014Labor Market": [{"content": "[SQUEAKING]\n[RUSTLING] [CLICKING] JONATHAN GRUBER:\nAll right, let's get started today with our\nlecture on factor markets. So when we talked\nabout producer theory, we talked about input\nprices, that firms had prices for their\nwages and their capital. And we just sort of\nposed those as given. I just sort of gave\nyou values for the wage and the renter rate of capital. But we never really talked about\nwhere those prices come from. Given that they may be\nthe most important prices in our whole economy,\nit's probably worth spending a little time\non talking about where do w and r actually come from. And that's we'll do for\nthe next three lectures, is talk about factor markets,\ntalk about the markets that give us the price\nof labor and capital. We're going to start\nby talking about factor demand, the general demand\nfor labor and capital. And then we'll move on to\ntalk about factor supply, where does supply come from. We'll then develop\nthe equilibrium, and that will tell us where\nwages and the interest rate come from. So that's sort of the\nmap of where we're going, is we're basically\ngoing to develop the markets that give us the\nwage rate and the interest rate. So let's start with factor\ndemand, factor demand. And let's start, and\nwe're going to start with the cleanest case. We're going to assume that\nfactor markets are perfectly competitive. So unless I say\notherwise, we're assuming the market for\nworkers, or the market for machines, or capital,\nis perfectly competitive. OK, we'll come back and bend\nthat a little bit later. So what that means is that\nthere's basically many sellers and buyers, OK? So any worker is\nbasically competing with lots of workers for jobs. Any firm is competing\nwith lots of firms to hire the workers, OK? And we're also going-- we're going to assume a\nperfectly competitive input market, that is lots\nof firms and workers competing to match\nwith each other. We're also going to assume a\nperfectly competitive output market, that is, we're going\nto examine this for the case not of a monopoly firm but of\na perfectly competitive firm. So just think of this, you have\na perfectly competitive firm competing with lots of other\nfirms to hire workers, OK? So let's start by talking\nabout short run labor demand in this context. Let's talk about short\nrun labor demand. Now, in the short\nrun, capital is fixed. So our decision is just, do\nwe add another worker or not, or another hour of labor or not. Like I said, the units\ndon't really matter here, but let's take in\nterms of workers. Do we add another worker or not? Well, as with everything\nelse in this course, we want to consider the marginal\nbenefits and the marginal costs of that decision. The marginal benefit\nof an extra worker is that one extra unit of\nlabor raises productivity by the marginal\nproduct of labor, OK? One more unit of labor\nraises our output by the marginal\nproduct of labor, OK? But that's not the only\npart of the benefit, because we don't actually\ncare as a firm about units of output. We care about revenues. So the benefit of a worker is\nnot just the how many units it produces, but the\nvalue of those units. And what is the value of\nthe next unit produced? It's the marginal revenue. So the value of the\nnext unit of labor is what we call the marginal\nrevenue product, MRP sub L. The marginal\nrevenue product is the marginal product of\nlabor times marginal revenue. That's the benefit of\nanother unit of labor. It's not just what they\nmake, but what it's worth. It's not just what they make,\nbut what it's worth, OK? So that's the marginal benefit. The value of another\nunit of labor is it makes marginal revenue\nproduct amount more stuff, and you sell that at\nthe marginal revenue. That's the marginal benefit. What's the marginal cost\nof another unit of labor? So this is the marginal benefit\nof another unit of labor. What's the marginal cost? Well, the marginal cost\nof labor is just the wage. So we simply set this\nequal to the wage. We set the marginal\nrevenue product of labor equal to the wage, and that\ngives us our optimization condition for the\noptimal amount of labor the firms want to demand-- is to set the marginal\nrevenue product of labor equal to the wage. Marginal benefits of hiring\nanother unit of labor equals the marginal cost of\nhiring of the unit of labor. Now to go further,\nremember, I said this is a perfectly\ncompetitive output market. So what is the marginal revenue\nin a perfectly competitive output market? What's the marginal revenue\nof a firm producing-- yeah. Price. So I can write this\nmore to say that I want to set the marginal product\nof labor times the price equal to the wage, OK? So basically, what\nwe're saying here-- think about it-- is hire workers\nuntil the cost of the next unit of labor is the same as\nwhat that unit will actually produce for you, OK? The next unit of\nlabor costs you w. It produces for you MPL times p. So you want to hire workers\nuntil that condition is met, OK? So think about that, and figure\n15-1 sort of shows this, OK? We have a supply of labor. In 15-1, that's\nhorizontal, because we're assuming competitive\nmarket for workers, OK? We're assuming a\ncompetitive market for workers, that is a\nperfectly competitive market. So if I try to pay workers one\npenny more than other firms, every worker in the world\nwill want to work for me. If I pay workers one penny\nless than other firms, no workers will\nwant to work for me. That's what a perfectly\ncompetitive labor market means, that literally, I am\na price taker in the input market. I don't get to set the wage, OK? I don't get to set the wage. The wage is given to\nme by the labor market. So just like a perfectly\ncompetitive firm doesn't get to set the\nprice of their product-- it's given to them by\nthe competitive market. A perfectly competitive\nfirm in the input market doesn't get to set\nthe wage they pay. It's given them through\nthe kind of process that delivered us our prices\non the output side, OK? So we get a horizontal\nlabor supply curve. And then we have this downward\nsloping labor demand curve. Why is it downward sloping? Someone raise their\nhand and tell me. Why is the labor demand\ncurve downward sloping? Yeah. AUDIENCE: Marginal product\nof labor is diminishing. JONATHAN GRUBER: Exactly. The diminishing marginal\nproduct of labor means you have a\ndownward sloping marginal benefit of labor. Each additional-- remember,\nholding capital fixed is only one shovel. So each additional\nworker add less and less to digging that hole, OK? So marginal product\nis diminishing. Since p is a constant,\nthat doesn't really affect the slope. I mean, it affects the slope. It doesn't really\naffect the sign. Doesn't affect the sign. It's diminishing because the\nmarginal product of labor is diminishing. So the equilibrium is\nwhere they intersect. So the bottom line-- this\nis complicated and new-- the bottom line intuition\nis to think about, as I decide whether to hire\none more hour of work-- you've got a firm. You've got to decide,\ndo I want the worker to work one more hour? You do the tradeoff\nof, what am I going to pay them for an\nhour versus what are they going to get me for an hour. What they're going to get\nme is their marginal product times the price, OK? Now, that-- So in other words, the wage is\nnot just the marginal product. It's imagining if two workers\nwere equally productive. With one more hour of work,\nthey each make three more units. But let's say, in one case, a\nunit is a computer chip, OK? In another case, a\nunit is a potato chip. We clearly would not want to\npay the same wage to someone who produces three more computer\nchips to someone who produces three more potato chips. We'd want to pay a\nlot more to the person to do more computer chips. Why? Not because computers\nare inherently valuable. In fact, potato chips\nare much more delicious than computer chips. Because they sell\nfor a higher price. So therefore, you'd\nwant to pay more to the worker who produces more\nunits of a more valuable good. So let's think about\na sports example, OK?"}, {"content": "And I realize we're all\nabout baseball today, as we should be. Go, Red Sox. But let's focus on\nbasketball for a minute, OK? Now, imagine you're a owner of\na team in the NBA, the National Basketball\nAssociation, and you're trying to decide how much\nyou pay one of your players. So basically, in that\ncase, your goal is to-- your goal is wins. That's the goal. That's the profit you're trying\nto maximize, is your wins. Let's say you're probably\ntrying to maximize your revenues from\nads and stuff, but assume that's\nproportional to wins. OK, assume that\nbasically, the more you win, the more money you make. So let's say the thing you're\ntrying to maximize is wins, OK? So your labor demand,\nthe marginal product you care about, is the\ncontribution of the next player to your win total. That's what you care about. The marginal product of labor is\nhow much does that next player add to my win total, OK? So for example, LeBron James,\nthe best player in basketball, arguably the best\nplayer in history-- we could have that-- we could\nhave the LeBron versus Michael debate some other time, OK? LeBron James makes $31\nmillion, and that's because his marginal\nproduct is enormous. He adds a huge amount\nof wins to any team, OK? We'll see with the-- we'll run the\nexperiment to watch how the Cleveland Cavaliers\ntank this year once LeBron has left, OK? Now, other players\ndon't make as much. Let's compare LeBron\nJames to Nate Robinson. You guys might not\nknow Nate Robinson is. He's one of the shortest players\nin the history of the NBA at a paltry 5'9\", which sounds\npretty tall to you and I, but it's tiny for the NBA. He was a very exciting player. It's kind of fun to\nwatch this little guy run among these giants. But he was just OK. He wasn't a great player. He was a fine player. He made about $2 million a\nyear by the end of his career. So basically, you have\nLeBron making 31 million and Nate Robinson\nmaking two million, and that's sort of related\nto their marginal product. So LeBron adds a lot\nmore to your wins. Now, what happened\nis Nate Robinson quit basketball in\nthe US, and went to play basketball in Israel. In Israel, they love basketball. They have a league. And he went to Israel,\nand he was dominant. He was the best player in\nIsrael, because they don't-- it's not as good as the US, OK? So his marginal\nproduct went way up. Nate Robinson went\nfrom being someone that had a small marginal\nproduct to maybe the highest marginal product in the\nleague, and his wage went down from two million to 500,000. So this is a situation where\nsomeone's marginal product went way up and their wage went down. Why?"}, {"content": "Yeah. AUDIENCE: Because people\naren't paying as much to watch basketball. JONATHAN GRUBER: Right, because\nthe marginal product went up, but the price went way down, OK? And what we care\nabout is the wage equals to marginal\nproduct times the price. So you have a situation where a\nplayer got better but got paid less because they got better. He moved from making computer\nchips to making potato chips, OK? He moved from a\nmarket where he was earning a valuable commodity\nto one where he was earning one that was much less. So basically, it's a\nsituation-- that example shows why you have\nto care about both the quantity of the additional\nworker and the value of what they're producing, OK? Any questions about that?"}, {"content": "Yeah. AUDIENCE: When we talk about\nperfectly competitive input market, are we saying that\nlike all of the workers-- like a single hour of work\nregardless of who you get it from is equal, right? JONATHAN GRUBER: No, no. A single hour of\nwork is paid equally. It's not equal. Marginal product varies. We're talking about the market."}, {"content": "Let's think about a\nperfectly competitive-- I probably went\ntoo fast with this. Let's say a perfectly\ncompetitive output market is where the firms sell\nthe goods into a market where people have\nperfect information and can shop across\nall firms easily. A perfectly competitive\ninput market is where firms hire workers\nin a situation workers have perfect information\nand compare across all firms equally. So basically, the point\nis, think about a perfectly competitive output market. People are in a market where\nlots of people are shopping, and all the options\nare in front of them. A perfectly competitive labor\nmarket where you as a worker have lots of firms\nyou can work for, and they're all clearly\nin front of you, and they all offer a\nwage, and you can see it. AUDIENCE: OK, but\nwe're not saying that the firms have perfect\ninformation across all the laborers, and [INAUDIBLE]. Are we saying if we have the-- JONATHAN GRUBER:\nWhat we're saying is-- we're not saying the\nfirms have perfect information about the laborers. The firms essentially-- let\nme think of the best way describe this. So once again, the firms are--\nfrom the firm's perspective, they do have\nperfect information. No, the wages\naren't-- yes, right, the workers aren't the same. They have different\nmarginal products. The firms know you're better\nthan you or vice versa. But from the firm's\nperspective-- from the workers'\nperspective, is just like, think of the\nworkers as the consumers in a perfect competitive\noutput market. For a perfectly\ncompetitive output market, the consumers can easily\nshop across all the firms they might buy from. In a perfectly\ncompetitive input market, workers can easily\nshop among all firms they might work for, OK? That's a good question."}, {"content": "Other questions? OK, now let's think\nabout the long run. This is the short run. Let's think for a minute\nabout long run labor demand. Think for a second about\nlong run labor demand. Well, what's different? The only thing that's\ndifferent is in the long run, capital can adjust as well. The only thing\ndifferent about the long run-- all the intuition,\neverything's the same. It's just that capital\ncan adjust as well. And what this means\nis that long run labor demand is more elastic than\nshort run labor demand, OK? So we could see this\nin figure 15-2, OK? So the figure shows two\ndifferent short run labor demand curves at two\ndifferent levels of capital. So the short run labor\ndemand when k bar equals 32 is that lower one. The short run labor demand\nwhen k bar equals 108 is the higher one. And what this says\nis, in the short run, you've got these two\nlabor demand curves. In the long run, you\ncould optimize capital. You can pick a point\non either curve, depending on which level\nof capital you choose. And by definition,\nthat allows you be more elastic at choosing your labor. You're more flexible\nbecause you can optimize not just over workers,\nbut over machines as well. It's the same\nintuition we developed before talking about short\nrun and long run costs, that the long run cost\ncurve was a lower envelope than the short run cost curve. Same thing here. This applies that\nthe long run labor demand is more elastic, because\nI basically am more flexible. I not only can choose\na longer curve, I can choose which curve I use. And by definition, that\nmeans that the long run is more elastic, OK? Just a small sort\nof side point there. Now, the last thing I\nwant to talk about here is capital demand. We talked about short run\nand long run labor demand. Let's talk about capital demand. It basically is the same thing. Capital demand is the\nexact same intuition. You want to get machines\nuntil the marginal product of capital, marginal\nproduct of the next machine, times the price you get for your\ngood equals the interest rate. It's the same condition. So we want to hire workers\nso the marginal product of the labor times the price of\nour good equals the wage rate. We want to invest\nin more machines until the margin\nproduct of capital of the next machine times\nthe price for our goods is equal to the interest rate. So it's exact same logic. Here's the marginal cost. The next unit of\ncapital-- remember, we talked about the intuition. You're always renting things. So thinking about\nrenting a machine, the next machine\ncosts are to rent. Do you want to rent it?"}, {"content": "Well, it depends."}, {"content": "What will it produce, and what\ncan you sell that stuff for? So you rent the next machine\nif the marginal product of capital, if the\ngoods it produces, times what you sell\nthose goods for, you want to do that until that\nequals the interest rate, OK? Questions about that?"}, {"content": "Yeah. AUDIENCE: [INAUDIBLE]\nmachine that you buy and own? JONATHAN GRUBER: Yes. We're going to talk about that\na lot starting next lecture."}, {"content": "Right now, I think I'll\njust put this down here. We'll come back to\nit, but I'm going to focus on labor\nfor this lecture, OK? So let's focus on labor, and\nlet's-- so I just put that down, and we'll back to\ncapital, but focus on labor for a minute, and make sure to\nunderstand where labor demand comes from. Now let's talk about where\ndoes labor supply come from. We talked about,\nat the firm level, labor supply is\nperfectly elastic. So go back to figure 15-1. That was a firm level curve, OK? That was a firm level curve. That's a perfectly elastic\nlabor supply to a firm, but that doesn't\nmean labor supply to the market's\nperfectly elastic. So now we want to derive\nmarket labor supply. So I'll call this\nderiving market labor supply, deriving market\nlabor supply, OK? Now, this is basically\nthe question of, how do we model how hard\npeople want to work? This is, once\nagain, getting where the economics is exciting, OK? You sort of knew that economics\nwas involved in how much Ford charged for a car,\nbut you might not have thought so much\nabout that economics was involved in deciding how\nhard you work, but it is. And we're going to use the\nsame tools of consumer choice. Indeed, I used to teach this\nas an application of consumer choice, and now I teach it here,\nbecause it's the same tools of consumer choice. But now, consumers, instead of\nchoosing good A versus good B, are going to choose how hard\nthey're going to work, OK? So basically, like any\nchoice, there's a tradeoff. There's a tradeoff. On the one hand, if you work\nharder, you get more stuff. So you bring home more income. You can buy more\npizzas and cookies, OK? Remember, we talked about\nincome as a fixed thing your parents gave you, but\nin reality, sorry, kids, you're going to have to\nmake your own money someday. In reality, you're going\nto make a Y. It's not going to be given to you. And so if you want to buy\nmore pizza and cookies, you're going to have to\nraise your Y. It's not going to be given, OK? So the reason you\nwant to work harder is to buy more\npizza and cookies. The reason you don't\nwant to work harder is because you're not\nan MIT student, OK? That is, normal people actually\ndon't like work, newsflash."}, {"content": "OK? Normal people\nactually like leisure. There's a thing called\nleisure, it turns out, and normal people like it, OK? So the tradeoff for\nregular people-- so it's a hard\nthing teach at MIT-- is that basically, the\ntradeoff is if you work harder, you get more stuff,\nbut you spend more time doing something you\ndon't want to do. Now, this is weird. When we talked about\ntradeoffs before, we talked about the tradeoff\nbetween goods, pizza and cookies. Now we're talking\nabout the tradeoff between a good and a bad. The good is more stuff to eat. The bad is working harder,\nand we don't really know how to model that. So the trick we're\ngoing to use here is we're going to flip\nthe bad into a good. Instead of modeling labor,\nwe're going to model leisure. So to get labor supply, we're\ngoing to model leisure supply, and then just flip it around\nto get labor supply, OK? So that is, we're going to say,\nyour ultimate labor supply, the amount of hours you\nwork, the amount you work, the amount of hours\nyou work, call them H, is equal to 24 minus leisure. Let's call it leisure, because\nleisure's called little l. Leisure's little l. The amount of hours you work is\n24 minus the hours of leisure you take. What that means is I don't\nhave to model the bad. I can model the good and just\nuse this simple reflection equation to get the bad, OK? So this is the\ntrick in economics. It's a good modeling trick. We don't model bad\nso we don't have to do the tradeoff between\nthe bad and the good. We don't have to do the\ntradeoff between two goods. So turn the bad into a good. Don't model work, model leisure. Don't model your hours\nyou work, model how many hours of leisure, OK? This is a general\nmodeling trick. So what we want to\nask is, now, not how do you derive the\nsupply of labor, how do you derive the\ndemand for leisure? How do we derive how\nmuch leisure people want? Well, once I say it that\nway, you know what to do, which is what I just said. There are two goods,\nconsumption and leisure. I wonder how much of\none good you choose-- of each good you choose. Well, that's a consumer\nchoice problem. You know how to do that, OK? So basically, take\nfigure 15-3, OK? In figure 15-3, now, instead\nof doing pizza versus cookies, now our decision\nis all consumption. So we're thinking about\nconsumption as a bundle, OK, versus leisure. So on the y-axis is\nthe goods you choose. On the x-axis is how much\nleisure you take, OK? It says N but actually it\nshould be little l, OK?"}, {"content": "Should be little l."}, {"content": "So let's call that little l, OK? So basically, as you go\nmore positive on the x-axis, that's more leisure. But because this\nequation, that implies as you go to the left on the\naxis, that's more work, OK? Yeah. H is hours of work. H is hours of work. So as you go to the\nleft, you work more. As you go to the right,\nyou take more leisure. But we're modeling the\ngood, which is leisure. And then we just go\nto our standard-- we go to our standard\nconsumer choice equation. We have a budget\nconstraint and preferences. The indifference curve comes\nfrom your utility function. It comes from your indifference\nbetween how much you consume and how much leisure you take. And the indifference curve comes\nfrom like any consumer choice decision. But instead of choosing\nbetween pizza and cookies, now it's how much stuff you\nwant versus how much leisure you want to take. So it's the same sort\nof indifference curve. The budget constraint comes\nfrom what the market tells you is the cost of leisure. What is the price of leisure? What is the price of leisure? Someone else? Someone else got it? Yeah, AUDIENCE: Your wage. JONATHAN GRUBER: Your wage. Why is that the\nprice of leisure? AUDIENCE: Because\nevery hour you don't work is another hour\nof wage you don't get. JONATHAN GRUBER:\nWhich we call what? AUDIENCE: Opportunity cost. JONATHAN GRUBER:\nOpportunity cost. Remember, prices\nand opportunity cost are the same thing in economics. Here's once again where it gets\ninteresting to apply what we've learned, which is\nthat basically, this is why, once again, they call\neconomics the dismal science. Instead of having\nfun sitting around, we're telling you,\nyou know, by the way, you could be working\nand making a wage. So you're actually spending\nmoney by taking leisure. By taking leisure, you\nare spending money. What are you spending? You're spending the money\nyou could be earning. So the opportunity--\nso leisure has a price, and the price of\nleisure is the wage. It's what you could be\nearning if you were working. So the budget constraint\nhas the slope of minus w. So if you look at the\nbudget constraint, you could take 24\nhours of leisure and have zero consumption, OK? That's the x-axis intercept. Or you take no leisure and have\n24w worth of consumption, OK? So basically, that is\nthe tradeoff you face."}, {"content": "One other modeling\ntrick-- couple of them-- so a couple of\nmodeling tricks here. Modeling trick one is modeling\nthe good, not the bad, OK? Modeling trick two is, I\nwrote on the x-axis goods, but we don't think\nin quantities, we think in dollars. So to make life\neasier, I just said, let's assume the price of\nthe average good is $1. That way you can-- that's called-- that's\njust a normalization, OK, which allows you to think\nin terms of dollars of goods rather than quantity of goods. That's another modeling\ntrick we'll do. We call it making a\nnumerator good, OK? You don't have to\nremember that term, but the point is\na trick we'll do is we want to model\ndollars, not quantities. We just make the\nquantities cost $1, and then we can model\nquantities basically as dollars. So that's the trick we're doing. So the y-axis is dollars,\nbut it's also quantities, because we made the price\nof everything be $1, OK? It's just another trick\nthat makes life easier. OK, so two modeling tricks\nhere, the numerator trick, which is making the price $1\nso quantities become dollars, and the bad is good trick,\nwhich is model the good, and then reverse\nthat to get the bad. Having done that,\nwe know what to do. We get an optimum,\nwhich is the tendency between the indifference curve\nand the budget constraint, and we're done. And so what do you do?"}, {"content": "You choose-- we're going to call\nthis L."}, {"content": "We'll call it little l. You choose little l\nstar hours of leisure, which means you choose 24 minus\nlittle l star hours of work, OK? So basically, you sat down. You made the\ndecision, how much do I want to eat versus how\nmuch do I want to watch TV. You make that tradeoff, and that\ndetermines how hard you work, OK? Now-- yeah. AUDIENCE: Aren't there things\nthat are kind of necessary? Like for example, if\nyou wanted to-- like if your preference was\ncompletely to work, then wouldn't we be like\nan inefficient worker if we didn't sleep? Doesn't-- JONATHAN GRUBER: Well,\nand in some sense, that would be in your\nutility function, or it would be in\nyour utility function and/or your budget constraint. That would be true, absolutely. But that would be a feature. That wouldn't change this\nmaximization problem. It'd just change\ngeneral structure of the equations that go into\nthe maximization problem, OK? So basically, now, what's really\ninteresting about this is now we finally understand why\nwe learned all that shit about income and\nsubstitution effects. Remember, let's think\nof substitution effects. And you're probably saying\nlike, \"Why do I care? Price goes up. Quantity goes down. Why do I care?\" Here's why you care, because now\nit gets really interesting, OK? Because when we're doing\nsubstitution effects for a good, they work together. As long as the good was\nnormal, they work together. When the price went up, you\nsubstituted away from the good and you are poor. So it gets substituted\ndown for two reasons. Now, a normal leisure effect\nis an inferior labor effect. What I mean by that is that\nwhen your wage goes up, you work more through the\nsubstitution effect, but now you're richer. And when you're richer,\nyou buy more of everything, including leisure. So if you take more\nleisure, you do less labor. So the income effect naturally\ngoes against the substitution effect. I'll go through this\na couple of times. Don't worry. The income effect naturally goes\nagainst the substitution effect here. For consumption goods,\nthe income effect naturally work together, OK? We almost never saw sort\nof a Giffen good type phenomenon, where the\neffect could sort of switch the overall effect. For labor, that's\nmuch more likely, and it's much more likely not\nbecause of any inferior good. It's because leisure\nis a normal good, and labor is the\nopposite of leisure. So once again, let\nme say it again. The wage goes up. The substitution effect--\nthink of leisure as a good. When the wage goes up, that's\nthe price of leisure going up. When the price of\na good goes up, the substitution effects\nsays you want less of it, OK? So when the wage goes up,\nthe substitution effect says that leisure\ngoes down, right? Because you want to\nsubstitute-- wait, leisure just got more expensive. You now feel worse sitting\naround watching TV, because you could be out\nthere making more money. Yeah. AUDIENCE: Wouldn't\nincome-- [COUGHS] JONATHAN GRUBER: I haven't\ngot to income effect. Let me finish, then\nyou can ask it. AUDIENCE: Wouldn't\nincome effect be-- JONATHAN GRUBER: I haven't\ngotten to the income effects. Let me ask finish, then\nyou can ask it, OK? So the substitution effect says\nthat leisure goes down, OK? The income effect says\nthat you are richer, right? Your wage went up. You're richer. When you're richer, you want\nmore of all normal goods. Leisure for non-MIT\nstudents is a normal good. So you want more of it. So here, with consumption\ngoods, when they were normal, the income and substitution\neffects work together. With labor and leisure,\nthey work opposite. So what this is, the\nsubstitution effect says take more leisure,\nwhich means work-- take less leisure means work\nharder, work more hours. But the income effect\nsays take more leisure, which means work less hours. So you don't know what\nthe net effect is. So that's why we do income\nand substitution effects, because in a case like this,\nthey get much more interesting. Yes, now your question. AUDIENCE: Is this income effect\nin terms of income over time? JONATHAN GRUBER: No, this is\nyour income, your actual cash income. You are now richer,\nand when you're richer, you spend more on everything. So think of it this way."}, {"content": "Once again, imagine\nyou're not an MIT student. You're a normal guy. OK, if we won the lottery,\nif you guys won the lottery, you would use that\nto do a startup. If a normal person\nwon the lottery, they'd use it to not work, OK? That's the income effect. OK, when normal\npeople win lotteries, they don't go work harder. They don't work, OK? So that's the point. You are now richer\nbecause your wage went up. So you work less,\nand that offsets it. So let's show this in a graph. Let's go back to our income\nand substitution effect graph that we did before,\nfigure 15-4, OK? Now we're back to-- once again, this is just\napplied consumer theory, OK? Let's go back to the income\nand substitution effects. We start with budget\nconstraint one at wage one, and we have our initial tangency\nat A, OK, with leisure of N1 or little l1. Now our wage goes up. Our wage goes up. Therefore, the budget\nconstraint pivots up. Think of what that means. You can still only have\n24 hours of leisure. That's a fixed point. But as you take less\nleisure, you make more money. So the budget trade\nnow pivots up. Well, that has two effects. The first is the\nsubstitution effect."}, {"content": "Remember how we get that. We draw an imaginary\nbudget constraint at the new price ratio. The price ratio is\njust W because I assume the price of goods is 1. The new price ratio, tangent\nto the old indifference curve, that is point B. So\nthe substitution effect says, take less leisure, OK? The price of leisure has gone\nup, so holding utility costs, you want to take less leisure. The income effect,\nhowever, says, you are now richer\nso take more leisure. So the income effect\ngoes the opposite way of the substitution\neffect naturally. You don't need a weird\nthing for that to happen, like with pizza and cookies. It comes naturally. So for normal goods, the income\neffect goes the opposite way. Now, in this case, we end up\nwith leisure still going down. We end up with, the wage\ngoes up, leisure goes down, and therefore labor\nsupply goes up. So we end up with our\nstandard intuition, which is, I tell you, if I'm\ngoing to pay you more, you're going to work\nharder or less hard? The standard intuition\nis I work more hard, OK? But as figure 15-5\nshows, it would not be super odd to get a\nGiffen good effect here, which is, the wage goes up. The substitution effect\nshifts you to the left, but the income effect shifts\nyou even more to the right, and you actually end\nup with more leisure. So once again, my intuition, if\nI say to you the price of pizza went up, what happens to\nyour demand for pizza? You think of a standard--\nyou say, \"Well, I'm going to demand less pizza.\" If I say to you\nthe wage went up, what happened to\nhow hard you work? It's not clear."}, {"content": "Think of a simple example. Think of yourself\nactually back before you were an MIT student,\nwhen you were a kid saving for something. You were saving to buy a\nbike, and the bike was $150. OK, bike was $200, and you're\nearning $10 an hour, OK? So you had to work 20\nhours to get the bike. Now I gave you a\nraise to 15 hours-- to $15 an hour or $20 an hour. Would you work\nharder or less hard? Well, if all you want is the\nbike, you'd work less hard. You don't have to work 20 hours. You only have to work 10 hours. So in fact, a higher wage\ncaused you to work less hard. That's not that\nbizarre a case, right? That makes sense. The point is, it's\nactually quite sensible that you couldn't end up with\nthe labor supply being a Giffen good, with a higher wage\ncausing you to work less. It's not a crazy outcome. Giffen goods and\nconsumer goods are crazy. It's not at all crazy\nto think that in cases like having a target,\na purchase target, a higher wage would cause\npeople to work less. Yeah."}, {"content": "AUDIENCE: So does the law\nof nonsatiation not apply? JONATHAN GRUBER:\nAbsolute applies. Absolutely applies. There's no violation. We haven't violated\nany of the laws. All we've done is just\nsaid income effects-- it didn't apply with\nGiffen goods too. It's all just saying income\neffects dominate substitution effects, which we\nthought was sort of going to be pretty bizarre\nin the consumption good context, but it's not at all bizarre\nin the labor supply context. So this is pretty wild. What this says is\nthat basically, you've got a situation where\neven in the normal world, you can get that\npaying workers more makes them work less, which\nis kind of bizarre, OK? Questions about that, about\nthat intuition, or the math, or the graphs?"}, {"content": "Well, the math we haven't\ndone, but the graphs? We'll do the math on Friday. The graphs or anything?"}, {"content": "OK. Let's then say, well, does\nthat happen in reality? What does the evidence say? Let's go to the evidence."}, {"content": "What does the evidence say? And there may be sort\nof no question more worked on in economics than\nthe elasticity of labor supply or the shape of the\nlabor supply curve. There is thousands of articles\nwritten on this question, OK?"}, {"content": "And what I want to do here\nto make the intuition easy, I want to go back to\nthe literature circa probably 40 years\nago, when it was sort of the initial burst\nof interest in this, in like the 1970s."}, {"content": "In 1970s, there was a\nburst of interest in this. And what the literature did\nwas it looked separately at men and married women,\nbecause most of women were married, and back then we\ndidn't care about single women, OK? OK, it was a dark time, OK? So the literature\nlooked at men and women, and married women,\nand asked what was their elasticity\nof labor supply. Well, let's think for a\nsecond about what we'd expect, and to do that, let's think\nabout the substitution effect and the income effect. Let's start with men, the\nmale substitution effect. Let's go substitution effect. Men versus married women,\nwho has a bigger substitution effect and why? That is, when the wage goes up,\nwho has a bigger substitution response to that and why? Men or married women? Think about the world-- think about the Mad\nMen world or the world, you know, circa 40 years ago. You guys seen\nenough TV and stuff to know how life was\na little bit, OK? So who's going to respond? Who's the bigger-- yeah. AUDIENCE: Are you assuming\nmen were primary providers? JONATHAN GRUBER: Well, they\ncertainly were in the 1970s. AUDIENCE: Oh, OK. In that case, the men. JONATHAN GRUBER: Men have a\nbigger substitution effect? AUDIENCE: Yeah, they'll\nwork more, probably. JONATHAN GRUBER: OK,\nthat's one option, yeah. AUDIENCE: It'll be married\nwomen, because they're only working if they have to. JONATHAN GRUBER: Right. So it's actually married\nwomen, because men were already working 40 hours. They can't-- there's no-- So think about a\nmarried man in 1975. OK, men didn't raise their kids. Men quite frankly didn't\ngive much of a shit about their kids, OK? Men just worked. That's what men did in 1975, OK? They worked, and they\nworked their 40 hours, and then went home. OK, maybe they worked less\nor more than 40 hours, but certainly, the\nnotion of saying, \"Well, the wage went up. Maybe I'll take more\nleisure,\" never really crossed a man's mind in 1975. Because what were\nthey going to do? They have no one\nto play golf with. They didn't want to spend\ntime with their kids. What were they going to do? Whereas women had a real\nsubstitution possibility, OK? This was an era women were\nentering the labor force. There were real\nopportunities for work, but it was also fine\nto hang out at home. You had-- a lot of your friends\nwere hanging out at home. You could take care of kids. There were a lot\nof things to do. So women had a much larger\nsubstitution effect than men, OK? Because men-- remember, what's\nthe substitution effect? It's about the next\nbest alternative. For men, there was no\nnext best alternative. It was just work. Basically, between 9:00\nto 5:00 on a weekday, there was nothing\nelse to do, OK? For women, there was\nother things to do, which is, you can hang out with\nfriends who weren't working, or you could take\ncare of the kids. Yeah."}, {"content": "AUDIENCE: But what about\nlike working overtime? JONATHAN GRUBER: OK, well,\nlet's-- but once again, if I'm a man, you might\nthink that I could then-- but then once again, if I work-- the substitution effect could\nwork that way for overtime. But let's talk about\njust the decision to work at all, in some\nsense, or the decision to work sort of\nyour first 40 hours. Overtime is hard, because then\nyou get paid more, et cetera. OK, now let's go\nto the other side."}, {"content": "Let's go to the income effect. So let's not say this is zero. Let's say it's small, because\nthis is big and this is small. Because you can\nwork a little bit overtime or something\nlike that, and some men did care about the kids. I'm obviously being facetious. So it could be, some men\nwere willing to spend time with their kids, et cetera. OK, now let's go to\nthe income effect. For whom is the\nincome effect going to be bigger, men or women? For whom is the income\neffect going to be bigger? Yeah."}, {"content": "AUDIENCE: Maybe men. JONATHAN GRUBER: Because? AUDIENCE: Because they\nhave a goal of like, they need x amount of\nmoney to just provide for their families. So if they get this\nhuge raise in wage, then they become wealthier,\nand they could start doing more leisure in the week. JONATHAN GRUBER: Exactly. There's actually two\nreasons it's men. One, you're more likely to\nhave your target income. Two is, you can't have an\nincome effect if you don't work. The income effect is\nproportional to how hard you are working. If you weren't\nworking, then there's no income effect, right? Income effect is essentially--\nthe income effect for labor is essentially the\nhours times dH dy. What Manny said\nwas the reason why dH dy might be bigger\nfor men than women, because they have these targets. More relevantly, if\nwomen weren't working, they didn't have\ndH, so this is zero. So the income effect is zero. So for men, this was big, and\nfor women, this was small, OK? Put this together,\nand what does it suggest about the relative\nshapes of labor supply for men and women? Someone raise their\nhand and tell me. What does it suggests\nwhat the labor supply curve would look like for\nmen and women in this era? OK, given the\nintuition we talked about here, what does it\nsuggest the female and male-- the married women labor supply\nand the male labor supply curve should look like? You guys can get this, come on."}, {"content": "Well, let's talk--\nwhat did we talk about?"}, {"content": "We talked about the\nsubstitution effect. If the wage goes up, it leads\nto more leisure, which means it leads to more labor supply. By the income effect,\nif the wage goes up, it leads to less labor supply. So for men, with-- for women, with a big\nsubstitution effect and a small income effect,\nthis suggests a standard steep upward-- standard upward-sloping\nsupply curve. Think of the income\neffect being zero. Then we get the standard\nsubstitution effect. We know the sign of that. So for women, this suggests an\nupward-sloping supply curve, just like a substitution\neffect suggests a downward-sloping demand curve. For men, it's not clear. You could very much get\na Giffen effect here, because basically, there's not\nmuch option for substitution, but they might work a lot\nless if they get rich, OK? So that is sort of this-- what I like with this\nexample-- it's hard, but I like that this\nexample sort of illustrates how substitution and income\neffects can come together to get a bottom line answer. What do we know? What we know is that actually,\nevidence is that female labor supply was very elastic,\nthat circa this era, female labor supply\nwas in the elasticity of between 0.5 and 1. That if you raised women's wage\nby 10%, there was a 5% to 10% increase in their\nlabor supply, which is pretty not elastic-elastic,\nbut reasonably elastic, OK? Whereas for men it\nwas pretty much zero."}, {"content": "It wasn't negative. It wasn't positive. It was basically zero. Basically, men just worked 40\nhours and then went home, OK? So basically, in an era where\nfor women, the labor supply was very elastic and of\nthe standard direction, higher wages lead\nyou to work harder, an upward-sloping supply curve. But for men, it was pretty\nmuch a vertical supply curve, maybe even a\nbit backward bending, maybe even a wrong\nsign supply curve. But pretty much, you could\nthink of it as zero, OK? Now, what do we think has\nhappened in the 40 years since these two numbers? So elasticity of woman\nof between 0.5 and 1, and men of zero,\nwhat do we think has happened to these two\nnumbers in the 40 years since these studies, and why? What do you think has happened\nto these elasticity estimates and why? Yeah."}, {"content": "AUDIENCE: Are we talking\nabout these together? JONATHAN GRUBER: Let's\ntalk about women. What do you think has happened\nto the female estimate? AUDIENCE: Probably\ngotten less elastic. JONATHAN GRUBER: Because? AUDIENCE: More of them are\nworking in a primary role. JONATHAN GRUBER: Right. Well, first of\nall, this is going to come down, because in fact,\nit's now more standard just to work, right? In fact, now, for a woman\ntoday, in many communities, it's like being a\nman in 70s, which is if you don't go\nto work, there's no one to hang out with, OK? So basically, this is\ngoing to get smaller. And they're more of a\nprimary winner in the family. This is going to get bigger. So in fact, female labor\nsupply has fallen more to like about an\nelasticity about 0.2. It's actually fallen over time."}, {"content": "Now, for men, the question is,\ndo you get the opposite effect? Actually, men sort of care\nmore about their kids now, and there's more sort of\nactivities going on during the day, but in fact it hasn't. In fact, male labor supply\nstill is pretty inelastic. What's happened is kids\nare now in childcare. So basically, we've gone from a\nworld where, as wages went up, women went-- men worked. Women either worked or didn't\nwork, depending on the wage, and if they worked, the\nkids went in childcare. Now men work and women work,\nand kids are in childcare. And that's basically the change,\nthe evolution of the labor-- roughly speaking, obviously. Still, female labor\nforce participation is only about 70%, OK? Many women still do stay\nhome and raise their kids, and are in and out of\nthe labor force, OK? But by and large,\nwe moved to a world with just overall less\nelastic labor supply. Yeah. AUDIENCE: Between the average\ntwo-income household is richer now, or-- JONATHAN GRUBER: No. The average-- well,\nOK, we're going to get into this when we talk\nabout income distribution. What this has done is allowed\nthe average two-family household to tread water. So it's, the average\ntwo-family household today has the same income\nas they did in the 1970s. Why? Because workers earn a ton less\nin real terms than they did, and that's facts\nabout inequality we'll come to, that basically,\nthe average family in America, despite having-- going from the wife not\nworking to the wife working is no better off they\nwere 40 years ago. And that has lots implications\nwe'll talk about, OK?"}, {"content": "So any other\nquestions about that? So let me end with one final\nexample, an application, OK? Which is to the problem we have\nin the world of child labor."}, {"content": "It's a huge problem\naround the world, is kids being forced to work. It was a huge problem in the\nUS till the 20th century. It's a huge problem\naround the world, because A, work can often\nbe dangerous and bad for their health, but B,\nthey can't be going to school and having the opportunity\nbetter themselves. If a kid is spending\nall day working, then that kid is\ndestined to a life of working in the\nsame crappy job, because there's no way to\nget the skills that allows them to grow and go further. Now, one-- we will talk in the\nnext few lectures-- in a few lectures about\ninternational trade. And one criticism of\ninternational trade is people say, \"Well, if\nyou allow these developing countries to sell more stuff\nto the developed world, that will-- they'll put\nthe kids to work more.\" So if we have free trade and\nVietnam can suddenly sell a bunch stuff to America, that's\nmore kids they;re going to put to work making that stuff. So one common argument you\nhear against free trade is it's bad for kids, but in\nfact, that argument is not necessarily right, because it\nignores an important point. Manny? AUDIENCE: [INAUDIBLE] JONATHAN GRUBER: No,\nthat's a different issue. The point-- that's right,\nbut the point it ignores is free trade makes\nfamilies richer. And the families\nare richer, they may want to buy more\neducation for their kids. So on the one hand, it's true. Free trade makes kids more\nvaluable in the labor force. On the other hand, it\nmakes family richer and they want more\neducation for their kids. So to look at that two\nDartmouth professors did a study, who\nlooked at Vietnam, and looked at what happened\nwhen Vietnam liberalized trade in rice. So let's go to figure 15-6. Now, we haven't gotten\ninternational trade yet, so I'm just going to sort\nof hand wave through this. You don't need to really\nunderstand this graph, except what the bottom line is. OK, what happened was\nbefore trade liberalization of Vietnam, before\n1989, you could only sell rice made in\nVietnam in Vietnam. So what that meant\nwas the supply of rice was s sub v. The demand\nfor rice was d sub v, and the amount of rice\nsold was q sub v. And kids worked in the rice paddies. When they liberalized\ntrade, suddenly Vietnam could sell to a\nmuch larger market. They could sell to the\nworld market, d sub w. That's a bigger market. So they were able to shift\nup their supply curve and sell more rice. They could sell more\nrice, because now they're selling to the whole\nworld, not just to Vietnam. You don't need to notice this\nin the graph so much intuition. If you give someone\na bigger market, they're going to\nmake more stuff, OK? Yeah. AUDIENCE: But doesn't that\nalso put them in competition in other countries, whereas\nif it was just like-- if each country is just\nselling to themself, then Vietnam would have-- JONATHAN GRUBER: No, they\nliberalized in the sense that they let it send out. I didn't say they let more in. AUDIENCE: Oh. JONATHAN GRUBER: OK,\nbut we'll come back to international trade, OK? So basically, the\npoint is, there was this demand shock that\nallowed them to sell more rice. So what effect does that have\non the market for child labor? Let's go to the highly\ncomplicated last figure and let me walk\nyou through this. Here is the market\nfor child labor, OK? On the x-axis is the\namount of child labor. On the y-axis the\nwage of kids, OK? We start at point one, initial\ndemand and initial supply, wage 1, L1. Now we liberalize\ntrade, and that leads to more demand\nfor child labor, because we want to\nproduce more rice. So that shifts us out\nto D2 and point two. So we have more child labor. That's bad. But what this ignores is\nfamilies are now richer, and with the income effect, they\nwill buy their kids education. They'll pull their kids out of\nworking and put them in school. That's represented as a shift\nto the left of the supply curve. So we move from point\ntwo to point three through the income effect. Families are now richer. And indeed, if the income\neffect is large enough, you could move to point four. You could actually have a\nreduction in child labor. Why? Because the benefits\nof more kids working in terms of producing\nmore rice is exceeded by the value\nof the firms of taking-- of the families of\ntaking the extra money they're making and putting it\ninto education for their kids. And in fact, the studies showed\nthat we did move to a point like point four, OK? We actually found\nthat child labor fell when they\nliberalized trade, that the intuitive\nargument, that gee, if they sell more, more kids are\ngoing to work, it's wrong. That in fact, when you sell\nmore, yes, more kids-- demand for more kids, but\nfamilies are so rich, they put their kids in education\nrather than their fields, OK? And that is a wonderful sort\nof counterintuitive story of how what-- I'll talk about economies\nlike free trade, how free trade can actually have\nan unexpected positive effect. We might think it's negative."}, {"content": "And there's a question."}, {"content": "Come up if you want to talk,\nbut we've got to end now. So thank you for\nsaying a minute extra, and I will see you\nguys on Wednesday."}], "16. Input Markets II\u2014Labor and Capital": [{"content": "[SQUEAKING] [RUSTLING] [CLICKING] JONATHAN GRUBER: All\nright, let's get started. Today, we're going to continue\nour discussion of factor markets. If you recall, last\nMonday, we started talking about the labor market. And we talked about how workers\nmake the decision between work and leisure. And we talked about\nthe implications for setting the wage\nrate in the labor market. What I want to do today is\nreturn to that labor market equilibrium and talk\nabout the important case of the minimum wage. So today, I want to talk about\nthe labor market equilibrium and how it's affected\nby the minimum wage because it's an interesting case\nwhich allows us to introduce some complications as to how we\nthink about the labor market. So let's go back and think\nabout the labor market. So let's go to figure 16-1. The labor market,\nlike any other market, has a price and a quantity. The quantity is the\namount of labor supply. That's on the x-axis. The price is the wage. That's on the y-axis. The supply curve\nthat's upward sloping-- typically we'll assume an\nupward-sloping supply curve. But as we discussed last time,\nthat doesn't have to be true. If income effects dominate\nsubstitution effects, which they very well may,\nyou could actually have a backward-bending or\ndownward-sloping supply curve. So we talked about\nthat last time. Having taught that\ninteresting case, typically, we'll\nassume supply is upward sloping or at least\nnot backwards bending, not downward sloping. But remember, that's\nan assumption. So this upward-sloping\nsupply curve is not necessarily as obvious\nas a downward-sloping demand curve is. Downward-sloping demand\nwill almost always exist unless there's\na weird Giffen good, whereas\nupward-sloping supply is a little more questionable. So we have the equilibrium,\nand we have this equilibrium at L1 workers at a wage W1. So now we know where\nthis comes from. So basically, going\nall the way back to producer theory where\nwe just gave you a W, now we're telling\nwhere the W comes from. We're telling you where the\nwage comes from that you then plug into the firm's\noptimization for them to produce goods. Now, let's imagine that\nwe have a minimum wage. So let's go to figure 16-2. So this is a\nregulation which says that you're not\nallowed to pay workers below some minimum level. And let's say we set that\nminimum wage at the level W2 above the market wage W1. Quick question. What would happen\nif we passed a law and set a minimum wage\nthat was below W1? So there'd be a\nregulation which insists you couldn't pay workers\nbelow W2, but W2 is below W1. What would that do\nto the labor market? Nothing. And here's the key point. Markets in economics\nwill always endeavor to avoid government\nregulations if they can. So if a government regulation\nis not binding, it won't matter. Markets will just avoid it. So the interesting case is\nonly where the minimum wage is binding, as in the figure 16-2. So what happens? Well, if you set a\nminimum wage at W2, workers at that high wage\nwould love to work a lot. That's a high wage. They're high in\nthe supply curve. They would like to\nwork L sub s hours. They would like to\nsupply L sub s amount of labor supply to the market. Firms, however, if forced\nto pay a high wage, W2, are going to\nsay, wait, I'm only going to pay that high wage if\nthe marginal revenue product of labor is sufficiently high. Remember, we talked about the\nmarginal revenue of product last time. It's the marginal product\nof labor times the price. So if you're going\nto raise the wage I'm going to have to pay workers,\nunless that affects the market price, I'm going to need to\nhave a higher marginal product of labor, right? The demand equation\nwas, I said, the wage equal to the marginal product\nof labor times the price. Well, if the price\nhasn't changed with the minimum\nwage going in, I'm going to need a high--\nif the wage is forced up by the minimum wage, I'm\ngoing to need a higher marginal product of labor. How do I get a higher\nmarginal product of labor? By hiring less workers because\nthe marginal product of labor's diminishing. So if you're going to force\nme to pay a higher wage, you're going to force\nme to only hire workers until the point where the\nmarginal product of labor justifies that higher\nwage, which means I'm going to hire fewer workers. So firms demand only L sub d. Well, workers can't get jobs\nfirms don't want to give. So the equilibrium is L sub\nd jobs at a wage W sub 2, OK? What does this do to welfare? We can see before, before the\nminimum wage was in place, the market featured a consumer\nsurplus that-- here, consumers are firms, right? But there was a consumer\nsurplus of A plus B plus C. That is, firms\nwere willing to pay what was on the demand curve. They only had to pay W1. So their surplus\nwas A plus B plus C. Workers were willing to\nwork at a wage that's given by the supply curve S sub 1. They were paid at W sub 1. So they got a\nsurplus of D plus E. So here, the firms get\nthe consumer surplus. The workers get the\nproducer surplus because the workers\nare now the producers. Now let's say you roll\nin a set minimum wage. Well, two things have happened. One thing is you've then\ntransferred some resources to workers. That's the area B. You've taken\nthe area B that firms used to get, and now workers get it. That's the idea. You want to make\nworkers better off. So you transferred to\nworkers the area B. On the other hand, you've\ncreated a deadweight loss of the area C plus\nE. You've created deadweight loss in\nthe area C plus E because now there\nare fewer jobs. There are workers\nwho would happily work at a higher\nwage who are not being allowed to work by\nthe limited demand that comes from the minimum wage. So the bottom line is you end\nup with fewer workers, a higher wage, and ambiguous\nwelfare implications. Clearly, social\nwelfare goes down. Whether worker\nwelfare goes up or not depends a bit on the size of\narea B versus the size of area E. It's not clear if worker\nsurplus goes up or not. It depends on size of B\nversus E. In this diagram, workers are a net better off,\nbut it doesn't have to be true. What's clear is that social\nwelfare has gone down. Because remember,\nas I talked about, the cheat, the shortcut I\ntalked about when we talked about oligopoly, is,\nroughly speaking, welfare is proportional to\nthe quantity in the market. Essentially, the\nfurther you deviate from the perfectly\ncompetitive quantity, the bigger the deadweight loss. So that's what happens if\nyou put in a minimum wage. Questions about that?"}, {"content": "OK? Well, that seems\npretty straightforward, and that's what I\nlearned growing up as a kid in economics class. But then some empirical\neconomists, some very famous empirical economists,\nstarted doing a series of articles that\nactually studied, gee, what happens when the\nminimum wage does change. They did things\nlike, for example, comparing what happened\nwhen New Jersey raised its minimum wage but the state\nof Pennsylvania next door did not, and looked at fast\nfood workers in New Jersey, where the minimum\nwage went up, compared to fast food workers\nin Pennsylvania where the minimum\nwage didn't go up. And what they found was\nthere was no difference in employment, that jobs\ndidn't fall in New Jersey even though the\nminimum wage went up. And a series of\nfollow-on studies continue to find that, actually,\nhigher minimum wages didn't seem to cause jobs to\nfall, which is directly in contradiction\nwith this graph. So what's going on? That led to a big\nquestion and revision of what's going on in these\nmarkets that leads to that. And there's really\nthree possibilities for what's going on. Possibility one is that the\nminimum wage wasn't binding. Maybe New Jersey set a minimum\nwage below the market wage. But actually, empirically,\nthat's not true. We can look at what workers were\npaid before the minimum wage. It was well below where\nthe minimum wage was set for restaurant\nworkers that were studied in that most famous study. So this is not true. The minimum wage was binding. There's a second\npossibility that's absolutely consistent with a\nperfectly competitive market. What's a possible\nanswer for why I could impose a minimum wage in\na perfectly competitive labor market and have\nemployment not go down? Yeah?"}, {"content": "AUDIENCE: Price goes up. JONATHAN GRUBER: The price\nthat the firm charges goes up. But in a perfect\ncompetitive labor market, that still wouldn't happen. You might see some\nprice adjustment, but you'd still\nsee some adjustment in the marginal\nproduct of labor. But what else\nabout this diagram? Yeah. AUDIENCE: The firm's demand for\nlabor is perfectly inelastic. JONATHAN GRUBER: The firm's--\nactually, you're close. It'd be the worker's supply of\nlabor is perfectly inelastic. It's the right idea. If workers are perfectly\ninelastic in their supply of labor, then the\nsame amount of workers will work no matter\nwhat the wage. So basically, you're just\ngoing to essentially end up-- you'd also, in fact--\nthat's a good point-- also get inelastic demand,\nthe same thing. If either supply or\ndemand is inelastic, you'll end up with no\neffect of a minimum wage. So that's another possibility. But in fact, we've\ndone a lot of studies. So you could have\ninelastic supply or demand. But in fact, we've done lots\nof studies of supply and demand in these markets,\nand that's not true. Remember, supply was\nlargely inelastic for men, but it was somewhat\nelastic for women. And these low-income\nmarkets have a good mix of men and\nwomen working in them. Demand has been shown\nto be somewhat elastic. So neither supply nor\ndemand's very elastic, but they're sufficiently elastic\nthat that rules out as zero. So the third possibility and the\none economists have focused on is that we're not in a\ncompetitive labor market. They're focused on a\nnoncompetitive labor market. Just like we discussed\nnoncompetitive markets for goods with a\nmonopoly and oligopoly, you can have noncompetitive\nmarkets for labor. It's the basic same idea. So now let's look at-- so when we thought\nabout-- let's go back, think about perfect\ncompetition, the basics of perfect competition. We thought about\nperfect competition. The basic idea was, remember,\nI talked about laying out a bunch of rugs in a market\nwhere you could literally shop costlessly across\nall the people selling their little fake Eiffel towers,\nlittle statue Eiffel towers. And you could perfectly shop. It was easy to go\nfrom carpet to carpet. There was full information."}, {"content": "The prices were posted. And so basically\nwhat you ended up was perfectly elastic demand\nfacing any given firm. Any given firm, if\nthey tried to charge one cent more for their Eiffel\ntower, no one would buy it. If they charged one cent less,\nthey'd immediately run out. Everyone'd buy it."}, {"content": "Well, when we are\nmodeling labor markets-- and I discussed this last\ntime, but not very well."}, {"content": "So I want to come back to it. When we're modeling\nlabor markets, we're thinking about the same\nfeature of perfect competition. But here, it's not\nconsumers shopping over where to buy their goods. It's workers shopping\nover where to work. It's workers saying, gee, in\na perfectly competitive labor market, the idea is I know\nwhat I could earn at any firm and I can easily\nshop across firms, see where I'm going to work. So if any firm tried to pay me\none cent less than the market wage, I'd never work there. And if they tried to pay me one\ncent more than the market wage, every worker in the world\nwould want to work there. So in a perfectly\ncompetitive labor market, any given firm faces a perfectly\nelastic supply of labor. So we can see that\nin figure 16-4, which we actually showed-- and\nI'll let you skip this since we covered it-- 16-4, which I actually\nshowed in the last lecture. Remember the last lecture. I was focused on this\ndownward-sloping demand curve, but I casually threw in\nthis flat labor supply curve and botched explaining it. Now I'm explaining it,\nhopefully more clearly, which is to any given firm,\nthe labor supply curve is perfectly elastic because\nworkers can perfectly shop across job opportunities. So if that firm tried to pay\nless, they'd get no workers. So they faced a perfectly\nelastic supply of labor. But just like, in\nreality, there's no such thing as a perfectly\ncompetitive product market, in reality, there's\nno such thing as a perfectly\ncompetitive labor market. In fact, we can't shop easily\nacross all possible jobs and know what every\njob could pay. And the fact that we can't means\nthat firms on the labor market side will have market power. Just like we talked about\nmonopolists and oligopolists having market power\nover consumers through barriers to\nentry, firms will have market power over workers\nbecause workers can't perfectly shop across their\njob alternatives. So as a result, firms\nmay be able to get away with paying you less than\nwhat you might earn elsewhere. In a perfectly\ncompetitive labor market, a firm could never\npay you less than what you're worth elsewhere\nbecause you'd just go work somewhere else. But now, if McDonald's wants to\npay you less than you might get at Wendy's, but it's hard to\ngo find out what Wendy's going to pay you-- you have to go\na distance down the road, and you have to ask\nthem, and you're shy and it's embarrassing-- then\nMcDonald's might be able to get away with paying you less than\nyou might earn at Wendy's. So this is very much\nparallel to monopoly. In fact, we call\nthis a monopsony. A monopsony is a\nlabor market where firms have market\npower over workers just like a monopoly is a\ngoods market where firms have market power over consumers. Now, this is not so crazy."}, {"content": "And in fact, it applies\nvery much to me. Think about my situation at MIT. I've been here 25 years. I just got my 25th\nyear rocking chair, although actually it's\nnot a rocking chair because it comes in the box\nwith the rockers off it. And it arrived in my office,\nso it's sort of a short chair. My wife's 5 foot, and\nshe always complains how chairs are too big for her. So she sat, and she's like,\nit's a perfect chair for me. So now I have a nonrocking\nrocking chair in my office that she sits in. But anyway, I've been\nat MIT for 25 years. It's going to be really\nhard for me to move. I like my house. I like my colleagues. I like my friends. Kind of, I like my\nview out the window. It's going to be kind\nof hard for me to move. Moreover, it'd be pretty\nhard for me to figure out what I'd get paid if I moved. I can't go to other\nuniversities and say, hey, what would you\npay me if you hired me? That's be awkward. I can't really ask my\ncolleagues what they make. That's awkward. So at the end of the day,\nMIT has market power over me because I don't\nreally want to move and I can't really\nfigure out what I'd get paid if I did move. And MIT will exploit\nthat market power over me by paying me less than\nI might earn elsewhere. And we know this as a\nfact because in academia, the only way to get a raise is\nto go get an offer from someone else and have them say how\nmuch more they'll pay you, and then you take that to your\nboss and they say, match this. But if you're not\nwilling to do this, as, frankly, MIT knows\nI'm not willing to do, then MIT can\nessentially underpay me. So basically, any\nresponsible profit-maximizing or even nonprofit employer\nwill exploit this market power and they'll pay me less\nthan my market wage. And that means that MIT\nwill earn surplus on me. In a perfectly\ncompetitive labor market, the firm earns no\nsurplus on the worker. They pay the worker their\nmarginal revenue product. So if you go to this figure,\nwhat am I paying the worker? What I'm paying them is\nexactly the marginal revenue product just like, in\na competitive market for the goods, a firm is selling\nat exactly their marginal cost. So just like a firm makes\nno surplus in a perfectly competitive goods market,\na firm hiring workers makes no surplus in a\ncompetitive labor market. But in a monopsony market, the\nfirm makes surplus over me. They pay me less than they'd\nhave to because I don't shop and find a better opportunity. Now, are there questions\nabout how that market works? I'm not going to do all\nthe math and graphs. It's all the same as monopoly,\njust flipping demand and supply curves. It's a pain in the ass."}, {"content": "I'm not going to do it. I just want you guys to\nunderstand the intuition. So please, since I\nwent through this, are there questions about\nthis or how it works?"}, {"content": "OK. Now let's take this\nnoncompetitive labor market and let's throw\nin a minimum wage. Well, as before,\nif the minimum wage is below what the firm\nwas already paying, there's no effect. So let's assume it's a\nbinding minimum wage. Now, let's say the\nbinding minimum wage is above what my true\nmarket wage would be, what my wage would be in the\nperfectly competitive market. So in a perfectly\ncompetitive market, my wage would equal my marginal\nrevenue product of labor, right? That's in a competitive market. In this noncompetitive\nmarket, my wage is below my marginal\nrevenue product of labor. Firms are exploiting me\nbecause I can't effectively shop for a better job. I don't want to or\nit's hard to do so. Now, in this\nnoncompetitive market, if we set a minimum\nwage that's higher than the marginal\nrevenue product of labor, then the analysis is just\nlike it's a competitive firm. Once that marginal\nwage is higher than the marginal\nrevenue product of labor, it's just like a\ncompetitive firm. So it's not that interesting. The interesting case is, what\nif the minimum wage comes in and it's above the wage I make\nbut below the marginal revenue product of labor? So let's say McDonald's,\nsomeone working there yields a marginal revenue\nproduct of labor of $10, but they're only being paid $7. Let's say you roll in\nminimum wage of $9-- so above what they're\nbeing paid now, but below their actual marginal\nrevenue product of labor. Will the firm fire that worker? Why not? Yeah. AUDIENCE: They're still paying\nthem-- they're still making a profit off of that worker. JONATHAN GRUBER: They're\nstill making surplus, which is as long as the\nmarginal product of labor's bigger than the wage,\nthey love that worker. So before-- so let's write\ndown the numbers as an example. So imagine my marginal revenue\nproduct of labor at McDonald's is $10, but my wage is $7. And then you come and you\nset a minimum wage of $9. Well, 10 is still\ngreater than 9. So the firm has no\ndesire to fire me. So all you've done is\njust given me money. And where'd that\nmoney come from? The surplus the firm earned. So all you've done is\nshifted the surplus from-- you've shifted producer\nsurplus to consumer-- I'm sorry, consumer surplus--\nconsumers are the firms-- to producer surplus,\nthe workers. So in a monopsony\nmarket, a minimum wage doesn't cause deadweight loss. It just shifts surplus around. And that's a really\nimportant outcome because that, once again,\nsays the government isn't always bad here. This is just like--\nif you want to think about this graphically, go\nback to exactly the analysis we did of regulating monopolies. Remember we talked about\nregulating monopolies. We talked about, if a regulator\ncomes in and sets a price below the monopoly price but\nabove the competitive price, it reduced the deadweight\nloss of monopoly. It's the same thing. And if you set a minimum\nwage above the market wage but below the marginal\nrevenue product of labor, then you simply transfer\nsurplus to workers without causing deadweight loss. Now, that raised the\nquestion, of course, is the minimum wage\nin between the wage of the marginal\nproduct of labor? Well, we don't know,\nbut let's go back to the studies that\nmotivated this. The very fact that\nthe minimum wage doesn't seem to\ncause unemployment suggests we are\nhitting the sweet spot, suggests we are hitting\nthe sweet spot, that we're basically managing, with the\nminimum wage policy, at least to date, to essentially\njust find a way, without the government\nspending any money, to shift resources from\nbusinesses to workers. So what does this mean? Well, it means that around the\nlevel of current minimum wages, we can raise the minimum\nwage by a small amount pretty costlessly. It doesn't necessarily mean\nthat a $15 minimum wage is OK. So in some sense,\nthe existing-- this is the important thing\nabout empirical economics. You only learn the answer in\nthe range that you study it. So for example,\nthere've been studies that have looked at what happens\nif you have a $10 minimum wage, and those show no unemployment. There haven't been studies\nthat show what happens if you have a $15 minimum wage. Now, Seattle just actually\nput in a $15 minimum wage about two years ago. So we actually can\nrun the experiment. And the early evidence\nis the Seattle $15 minimum wage did lower\nemployment, that the Seattle $15 minimum wage actually went\nabove the marginal revenue product of labor. And once it's above, you're\nback in the competitive case. You're back in the case where\nyou're lowering employment. Yeah? AUDIENCE: How can you increase\ncompetitiveness in the market? JONATHAN GRUBER: Well,\nthat's the other question, is how could you\nincrease-- so you tell me. How could you increase\nthe competitiveness of a labor market? AUDIENCE: You make it easier\nto tell how much money you would get at each place. JONATHAN GRUBER: So Norway\nhas a day every year they call Envy Day,\nwhich was yesterday, I believe, where they literally\ncan go online and look up anybody's income in Norway. They literally make public\nevery single person's tax return in Norway. And you can go online and\nlook at what everybody makes."}, {"content": "That would do it. So you could provide\nmore information. You could make it easier\nto move between jobs. For example, there's a lot\nof restrictions in our labor market, like noncompete\nclauses, which say that if you\nwork for one firm, you can't ever go\nwork for another firm in that industry for x years. That gives some monopsony\npower to firms, et cetera. So we could do things which try\nto loosen the flow of the labor market, and that would\nclose this gap between wage and marginal revenue\nproduct of labor. Now, let's go back to Seattle,\njust to conclude this. This doesn't mean the\nSeattle policy was a bad one. The bottom line is what\nwe learned from Seattle was that basically,\nemployment fell a small amount and a bunch of workers\nmade a bunch more money. So is that good or bad? Well, it depends. If you're one of the\npeople that lost their job, it's really bad. If you're one of the workers who\ngot a raise up to $15 an hour, it's good. How do you weigh them\nagainst each other?"}, {"content": "That's exactly what we'll talk\nabout in a couple lectures. So once we start talking\nabout normative economics, about is a policy good or bad,\nthere's typically trade-offs. And this is a classic example. What we're learning here is, is\nthe minimum wage in the range we are now, right now, the\nfederal minimum wage at $7.25-- the evidence suggests\nit could easily rise without causing that trade-off. The evidence suggest\nwe could increase the federal minimum wage\nby some nontrivial amount, at least up to $9 or\n$10, without causing much of a trade-off. But once you get too\nfar ahead of that, there starts to be a trade-off."}, {"content": "Question about that?"}, {"content": "Yeah. AUDIENCE: Are there any states\nwhere it's actually still that low? JONATHAN GRUBER: Oh, yeah. Many states don't have\ntheir own minimum wage. Massachusetts is at $11,\nbut we're pretty unusual. We're one of the higher ones. A number of states have $7.25\nas the minimum wage, OK? And the evidence seems to be,\nfrom states like Massachusetts and others which are on the\n$10, $11 range, it doesn't seem to lower employment. It seems like we could clearly-- we'd be safe raising that\nfederal minimum wage. We would simply be\ntransferring resources and not causing unemployment. Yeah? AUDIENCE: Is there\nanything about the cost of living in areas where the\nminimum wage is more expensive? Is it possible that if a\nMcDonald's worker makes more money in this\nstate, McDonald's is more expensive in that state? JONATHAN GRUBER: That's\na great question. So what I assumed was I\nassumed firms would just say, oh, you got me. I'm going to throw some\nof my profits at workers. Firms don't have to do that. Firms could say, well, if\nyou make me pay workers more, I'm going to raise my price. Now, if it's a\ncompetitive output market, that shouldn't happen, right? Because in a competitive\noutput market-- well, no. Marginal cost goes up. It's not clear. It's not clear whether\nthat would happen or not, and the evidence is\nthat it's unclear whether higher minimum\nwage causes higher prices or whether it just\ncomes out of profits. We don't know yet, OK? All right, so that's what I\nwant to say about labor markets. Now I want to move on and\ntalk about capital markets. Now, as confusing as our\ndiscussion of labor markets was, that's easy compared\nto capital markets. Capital market's a lot\nharder to understand. And that's because\ncapital itself-- labor's something you\nget your hands around. It's the time you spend at work. Capital is this sort\nof amorphous thing that I've kept\npushing off defining. So I'll define it now. We talk about capital as this\nvague collection of buildings and machines and the other\nstuff that goes into production. And we know where\nlabor comes from. It comes from our work. But where does\ncapital come from? Well, capital is\na harder concept, but there's one unifying thread\nthat all elements of capital have, which is they\nrepresent the diversion of current consumption\ntowards future consumption. Capital is about\ndiverting consuming today towards consuming in the future. In fact, the original concept\nof capital came from farmers. Farmers, every year, when\nthey would pick their grain, they had a choice."}, {"content": "They could eat all\nthe grain, or they could save some to plant\nfor next year's grain. Now, the more they saved, the\nmore they'd have next year, but the less they'd have today. So farmers faced a trade-off-- literally, consumption today\nor consumption next year. That's what we mean by capital. In other words, in\ntoday's market economy, the link is not that direct,\nbut it's the same basic idea-- that firms have a choice,\nfirms and their investors have a choice. They can take what they\nmake and eat it now, or they can invest it in\nhaving more in the future. So basically, when we\nthink about capital, we're not going to think about\ncapital as physical capital. We're really thinking about\ncapital as financial capital. What links all types of capital\nis their financial aspect. What links machines and\nbuildings is all the aspect that, by putting\nmoney into them today, you have less you can\nspend on fun stuff today, but more you'll be\nable to spend tomorrow. And it's this\nfinancial aspect that links all forms of capital. Now, how do firms get\nthe money to invest in machines and buildings\nand stuff like that? They get it through going\nto the capital market. Where do firms get this\nmoney that they invest? They get it through going\nto the capital market, which is basically the pool of\nmoney that firms can draw on to make their investments. So think of it\nliterally as I'm a firm. I want to build a building\nand buy a machine. I literally go over, and\nthere's a big pool of money. And I have to take the money out\nof there to go buy my machine or build my building. And where does the money\nin that pool come from? It comes from household\nsavings decisions. So the capital\nmarket is a market where the demand for capital\ncomes from firm's interest in investing and having\nmore in the future. The supply of capital comes\nfrom people's decisions to save. And essentially,\nthe money firms use to buy stuff is\nborrowed from people. And that's the bottom line\nof how capital markets work. So just as the\nsupply of labor that determines how many\nworkers a firm can hire comes from your decision\nof how hard to work, the supply of capital\nthat determines how many machines a firm can\nbuy comes from your decision of how hard to save. So let's look at figure 16-5,\nequilibrium in capital markets. Let's start with the demand. We already talked, last\nlecture, demand for capital. The demand for capital comes\nfrom the marginal revenue product of capital. It's the marginal product\nof the next machine. So the demand comes from\nthe marginal product of the next machine times\nthe price the firm can get for its output, which\nis the marginal revenue product of capital. So it's the same\nlogic as for labor."}, {"content": "There's nothing\ninteresting there. Same logic as for labor. The supply's what's\nmore interesting here. Where does supply come from? The supply comes from\nhousehold savings, how much money is\naround for firms to actually get to\nget these machines. And how do they get it?"}, {"content": "They borrow. And what do they borrow at? They borrow at the\ninterest rate I. So I represents the\nrate that firms pay households to get their money. So think of this as-- we'll\ntalk about how it really works. But in theory, the idea is\nthink of literally a marketplace in the center of town. Downtown Boston, Haymarket,\nthere's this marketplace. And a firm comes and says,\nI need to borrow money to buy a machine."}, {"content": "And a person's there\nwith their savings and they say, well, I'll\nloan you some money. What interest rate\nyou going to give me? And that's the\nmarket for capital. So where the supply of capital\nmeets the demand of capital yields the interest rate. So basically, what this means is\nas the interest rate's higher, what that means is I have\nto pay people back more to borrow their money. So an interest rate of 10%,\nif I borrow $10 from you, I pay you back\n$1.10 next period. If I borrow $10, I pay you\nback $1.10 next period. If the interest rate's\n20%, if I borrow $10-- if I borrow $1-- I'm sorry. If I borrow $1 from you, I\npay you $1.10 next period. If I have 20% and I borrow $1, I\npay you back $1.20 next period, et cetera, OK? So basically, that\nis essentially how the transaction works. And the key point\nhere is the reason the supply curve is\nupward sloping is the more you're willing\nto pay me for my money, the more I'm\nwilling to lend you. So if you come to me\nand say give me $1 and next year I'll give you\nback $1, I'm like, I don't know."}, {"content": "Why would I do that? If you say, give me $1 and next\nyear I'll give you back $1.10, you're like, OK, now\nI'm interested. $1.20, I'm very interested. $1.50, for sure. Literally, I just\ngive you my money and, next year, I\nget back 50% more?"}, {"content": "Why not? So basically, the higher\nthe interest rate, the more I'm willing\nto loan the firm and, therefore, you get an\nupward-sloping supply curve. Now, of course,\nin reality, people don't actually-- we don't sit\nin Haymarket, downtown Boston, and give money to firms. In reality, this\ntransaction happens through capital markets. And essentially, there are three\nmechanisms by which implicitly I loan money to firms. The first is I could\nliterally buy corporate debt. I could literally loan\nthe money to firms. I could literally go\nand the firm could say, I, General Motors,\nam issuing a bond. This is through\nbond, issuing a bond. And the way that bond works is\nI promise that for every dollar you spend buying my bond,\nyou'll get 1 plus I dollars back at the end-- or next year, say, depends\non how long the bond is. So literally, you're loaning the\nmoney to the firm by buying-- you're buying their\npromise to pay you back. Now, a second way you can\nloan money to the firm is through investing\nin their equity. You can buy their stock. The way this works is GM says\nto you, buy a piece of me and you'll get paid back not\nsome fixed interest rate, but you get paid back\naccording to how well GM does. So with corporate\ndebt, I get paid back something that's predetermined. When I buy stock or\nequity, I don't get back a predetermined amount. I get back some-- it depends\non how well the company does. But it's the same basic idea. I'm giving the company\nsome money today in return for my getting\nmore money, I hope, tomorrow. That's the diversion\nof consumption from today to tomorrow. And the third thing I could do\nis I could put it in the bank. Now, how is that\nloaned to companies? Because the bank then\nloans it to companies. Why do banks say they'll pay\nyou interest on your money? Why did banks going crazy-- I'll give you 1-- it used to be interesting. Now it's 1%, 2%. When I was a kid, I\nwas like 10%, 12%. We'll give you lots of money. And we'll talk later about\nwhy it was so much higher when I was a kid. Why are banks so\neager to do that? It's not out of the\ngoodness of their heart. It's because when you give\nthem dollars, they turn around and loan them. They add a bunch to\nthe interest rate and loan them out to firms. So those dollars\nyou're giving the banks and they're paying\nyou 2% interest, they loan to firms at 6%. And that's why bankers are rich. So basically, the reason a bank\nexists is because it's a way-- corporate debt\nand equity markets are hard and complicated. It's much easier to put\nyour money in a bank. You put your money in a bank. But when you put\nyour money in a bank, you're essentially\nloaning it to companies. That's essentially\nwhat you're doing. So through these mechanisms,\nwe have a capital market where essentially, by my\nputting money away and diverting from today's consumption,\nI'm loaning to a firm. They'll produce\nmore, and they'll pay me back more in the future."}, {"content": "Questions about that? OK, so let's talk about where\nthe supply curve comes from. We know where the\ndemand curve comes from. It just simply comes\nfrom the marginal revenue product of capital. Where does supply\ncurve comes from? The supply curve comes from what\nwe call intertemporal choice. As I said, economists like\nputting fancy names on things. That helps us get\npaid more money. It just means choosing over\ntime, intertemporal choice. Intertemporal choice\nis essentially about how do you decide\nhow much to save. What's going to\ndetermine that is going to be your\ndecision of how much you value money today versus\nvaluing money tomorrow. So for ease, let's imagine\nI'm considering two periods, this year versus next year. When I talk about periods, I'm\ntalking about days and years and whatever. It's the basic logic. It's about now\nversus the future. Whether I say days or\nyears, it doesn't really matter right now. The point is I'm just talking\nabout today versus the future. So let's talk about this\nyear versus next year. And let's imagine prices\naren't going to change. I'll come back to\nprices next lecture. But let's imagine the price of\ngoods aren't going to go up. There's no inflation\nin this economy, which is roughly true today. And let's suppose I'm\ngoing to take next year off to care for my children."}, {"content": "Lord knows why I'd want to\ndo that when the youngest one's 19, but imagine\nthey still need my care. So let's say I'll take next--\nthis example gets dated. Let's say I take next year\noff to care for my children. And let's say my income\nis $80,000 a year. Now, here is my-- but I'm going to take\nnext year off unpaid. So I'm going to work\nthis year for 80k. Next year I'm going\nto take off unpaid. So I have a couple of choices. I could work this year,\nearn my 80k, spend my 80k, and have nothing\nnext year to live on. I could work this\nyear and eat nothing and save all of\nthe 80k to live on, or some combination in between. And we could illustrate--\nbut the key difference is every dollar\nthat I don't consume this year that I save to consume\nnext year earns interest. And that's where\nthe trade-off comes."}, {"content": "So let's look at figure 16-6. This is a familiar-looking\noptimization diagram. Now my optimization is not\nover pizza versus cookies, but my optimization is over\nconsumption this period versus consumption next period. It's a bit mind-blowing. We're a little\nscience-fictiony here, right? We're now not talking about\nchoosing between two goods, like leisure and consumption\nor cookies and pizza. Now I'm talking about two time\nperiods, consumption today versus consumption tomorrow. But that's the key\nthing about the tools we learn with consumer choice. Those tools are\nincredibly powerful."}, {"content": "You just need to shove your\nproblem into that framework. And we're going to shove our\nproblem into this framework. The problem we're facing is how\ndo I decide how much to save. Well, savings is a bad\njust like labor's a bad. What do we do when we\nhave a bad to model? We don't model the bad. We model the complementary good. So our choice is, how\nmuch do I consume today? My choice is, how much\ndo I consume today and how much am I going to save? Well, saving is a bad, but the\nother way to think about it is, how much am I\ngoing to consume today versus how much am I\ngoing to consume tomorrow? Then that's two goods and I can\nmodel them against each other. And that's what I\ndo in figure 16-6. I model consumption today\nversus consumption next year. So here's my choices. As I said, if I consume\neverything today, I'm at the\nx-intercept at 80,000. I have 80,000 to consume\ntoday, nothing next year. If I consume everything\nnext year, what do I get? Well, let's say the\ninterest rate is 10%. What that means is then\nI'll have $88,000 next year. Why will I have more next year? Because by saving,\nI earn interest. By diverting my consumption to\nthe future, I earn interest. At 10%, that means I would\nhave $88,000 next year. So my budget constraint is the\nline with the slope minus 1 plus I. My budget constraint is\nthe line with the slope minus 1 plus I. In other words,\nthe price of consumption today in terms of consumption\ntomorrow is minus 1 plus I. OK, let me think about it."}, {"content": "Let me say that again."}, {"content": "It's really confusing. The price of consuming today\ninstead of consuming tomorrow, assuming no inflation--\nso prices are the same in the market-- is minus 1 plus I. Think about that. I find it useful to think back\nto the labor case for parallel. In the labor case, what did we\nsay was the price of leisure? What was the price of leisure? Someone raise their\nhand and tell me. In the labor-- yeah? AUDIENCE: The wages. JONATHAN GRUBER: The wages. Why? AUDIENCE: Just because that's\nthe opportunity cost of not-- JONATHAN GRUBER: Right. So by that same\nlogic, can tell me why is the price of\nconsuming today 1 plus I? AUDIENCE: Because if\nyou choose to save, then we're effectively richer. JONATHAN GRUBER: Exactly. The opportunity\ncost-- remember, we are an annoying discipline\nwith a dismal science. We're telling you, hey, enjoy\nthat cookie, but by the way, if you weren't eating that\ncookie, you could have 1 plus I cookies tomorrow. So just like we nag you for\nsitting around watching TV, we nag you for eating\ntoday by saying, hey, the more you consume today,\nthe less you can have tomorrow. And in fact, that trade-off\nis that for every cookie you consume today, you forgo\n1 plus I cookies tomorrow. So that's the budget constraint. The slope is the opportunity\ncost of consuming today in terms of\ntomorrow's consumption or next year's\nconsumption, which is 1 plus I. That's the slope\nof the budget constraint, is the opportunity cost. And then, then we say, OK, well,\nthat's the opportunity cost. That's the budget constraint. Well, how do I decide? Well, then we know how to\nmake these decisions, which is go to utility function. You can write down the\nutility function, which is a function of C1 and C2. Now, what is C? C is all my pizza and cookies,\nbut we're aggregating it up. Just like our utility\nfunction last time was a function of\nleisure and consumption-- we said consumption was\nthe bundle of goods you eat and leisure is this thing. Now we're saying, OK,\nour utility function now is a function of this trade-off. Now, you might\nsay, wait a second. How can both those\nbe utility functions? And the answer is you have\nsome meta-utility function that includes consumption today,\ntomorrow, leisure, pizza, cookies, et cetera. But we can think about\nthis in sequential steps. First, we decide how we're\ngoing to split our income. Then we can decide what to\nspend it on each period. Then you can do a separate\nconsumer maximization decision. But our first\nquestion is simply how am I going to split my income. Well, that's going\nto be a function of my taste for consumption in\nthis period versus next period and the price the bank will\npay me for delaying consumption till next period. Now, what happens? Questions about that? Now, what happens\nin the scenario when the interest rate goes up? What do you think happens if\nthe interest rate goes up? Yeah?"}, {"content": "AUDIENCE: There's [INAUDIBLE]. JONATHAN GRUBER: Right. So what do you\nthink you should-- what do you think will happen\nto your consumption pattern? Yeah? AUDIENCE: You should\nspend less today. JONATHAN GRUBER:\nSpend less today and save more because\nit's rewarded. And why is that not\nnecessarily true?"}, {"content": "Yeah? AUDIENCE: Because you might only\nneed a certain amount of money to live. So you don't have to\nsave as much today because you'll make-- JONATHAN GRUBER: Because\nof what two effects? Income and substitution effects. You gave exactly the intuition\nthat the substitution effect gives you. The substitution effect\nis exactly right. If the interest\nrate goes up, that's like the price of\nconsumption today going up. And if the price of\nsomething goes up, the substitution effect\nsays you do less of it. But if interest rate\ngoes up, you're richer. And if you're rich, you\ndo more of everything, including consuming today. The income effect\ngoes the other way. It's like labor. Once again, income and\nsubstitution effects is why we bothered\ntelling you so. Because income and substitution\neffects, in these cases, go against each other. Let's look at figure 16-7, OK?"}, {"content": "In figure 16-7, we\nstart at point A. Now imagine the interest\nrate doubles to 20%. Now imagine the\ninterest rate doubles. As you said, that pivots the\nbudget constraint upwards. You could still consume\nonly $80,000 this year, but now for every dollar you\nsave, you get $1.20 next year. That has two effects\non your decision. The substitution effect, we get\nby drawing an imaginary budget constraint-- that's\nthe dash line-- tangent to the original\nindifference curve but at the new slope. By definition, that means\nyou consume less today. You consume less\ntoday by definition. If the price of\nsomething goes up, the substitution effect\nalways says you do less of it. You consume less today,\nwhich means you'll save more. Remember, savings is just\nincome minus consumption in period one. So just as labor was\n24 minus leisure-- and so if we just solve for\nleisure, we could get labor. Savings is just income minus\nconsumption in period one. So if we solve for consumption\nin period one, we get savings. People see that? So basically, the point here\nis the substitution effect says, well, gee, the price\nof consumption in period one just went up. It's more costly in terms\nof future consumption. I'm going to do less, but then\nmy savings is going to go up. Substitution effect\nsays you save more. But the income effect\nsays, wait a second. You're now richer. Every dollar of your\nsavings you are doing now yields twice as\nmuch in interest. If you're richer, you'll consume\nmore of everything, including period one consumption. So the income effect takes\nyou back the other way. Now, whether the income\neffect dominates are not, we don't know. In this case, it\ndoesn't dominate. In this case, you\nstill, on net, end up consuming less in period\none and saving more. But we don't know what's\ngoing to dominate."}, {"content": "And in fact, the evidence\nhere is incredibly weak. I won't spend a long\ntime on the evidence because it's not nearly as\ninteresting and strong as labor supply. The evidence is incredibly\nweak even about the sign. And let's come to the intuition\nthat was given for why. Well, think about how people\nmake savings decisions. Lots of people\nhave savings goals. I want to have x by\nthe time I retire. Typical way if you ask\npeople about their savings-- if you ask them,\nthey typically say I want to make sure I\nhave x in the bank in case I'm in an accident. I want to make sure I have\ny by the time I retire. Well, in those models, if\nthe interest rate goes up, savings rates go down. Because after all, to hit a\ntarget with a higher interest rate, I can save less. So it's actually\nnot that surprising that you'd have\na higher interest rate leading to less savings. It's kind of\nintuitive, actually. If people have savings\ntargets, a higher interest rate would lead to less savings\nbecause they can get to their target more easily. So actually, we don't even\nknow which way this goes. It's, I think, one of the\ngreat unsolved mysteries in economics empirically,\nis, once again, we typically assume-- and with a gun to\nmy head, I would say it's probably true that\nhigher interest rates leads to more savings. But the evidence on which\nthat rests is pretty weak."}, {"content": "And the key point for you is\nto understand it's uncertain and it depends on whether\nincome and substitution effects dominate. Questions about that?"}, {"content": "OK. So now let's step back\nand put it all together and think about you making\nyour decision about life. You can think about\nyour decisions about your life in three steps. Step one is you decide\nhow hard to work. Step one is you decide, how\nmuch money do I want to make? Well, that's about\nmaximizing utility over consumption and leisure. Step two is, having decided\nhow much you're going to make-- and that yields your labor. Step two is, deciding how\nmuch you're going to make, you decide, well, how do I\nwant to spread that over time? How much do I want to consume\ntoday versus tomorrow? Well, that's about\nintertemporal choice. That's about deciding\non C1 versus C2, and that's going to\nyield your savings. Step three is, now that\nI know how much I'm going to consume\neach period, now I want to maximize utility\nacross all my goods I might want to\nconsume-- x2, across all the goods I want to consume. That was our original\ncookies and pizza example. So you could think of\nit as a hierarchical set of consumer\noptimization problems that you're going to solve. Now, you might say,\nwell, gee, Jon, that's sort of confusing\nbecause, in fact, the interest rate and how much am I\nsaving could determine how hard I work, right? Let's say the interest\nrate goes way up and I have a savings target. I have to work less hard\nto hit that savings target. And I'd say to\nyou, good for you."}, {"content": "Take more advanced economics. More advanced\neconomics, we recognize this is one integrated whole\nand we allow these systems to affect each other. But for here, just think of\nthem as separatable steps, independent steps. But in practice,\nI hope you can see the steps will be integrated\nand they'll affect each other. Think of it. If the price of a good\nyou really want to buy goes up a lot, not only will\nyou buy less of that good; you might save more to\nbuy it and work harder. So you can imagine how\nthese things are integrated."}, {"content": "But for now, we'll keep\nthem separable, OK? Questions about that?"}, {"content": "OK. Next time, we're to\ncome back and talk about all the interesting\nstuff in capital markets and how we make decisions\nabout how much to save and things like that."}], "Overview Artificial Intelligence Course | Stanford CS221: Learn AI (Autumn 2019)": [{"content": "All right. Let's get started. Please try to have a seat if you can find a seat and let's, uh, get the show on the road. So welcome everyone to CS221, this is Artificial Intelligence. Uh, and if you're new to Stanford, welcome to Stanford. Um, so first let's do some introductions. So I'm Percy, I'm gonna be one of your instructors. I'm teaching this class with Dorsa over there. So if Dorsa wants to say hi, stand up. Hi guys, I'm Dorsa. Um, I'll be co-teaching [NOISE] this class with Percy. I'm a professor in robotics and robotic interactions. Super excited about teaching this class and [inaudible]. Great. So we're going to be trading off throughout the quarter. And we also have a wonderful teaching team. So these are your CAs. So if all the CAs could stand up and I'll give you each person an opportunity to say three words about what you're interested in. So um, let's start with [inaudible] because you're the head CA. Hello. My name is [inaudible]. I'm a PhD student, and I'm interested in natural language processing. Yay."}, {"content": "[LAUGHTER] Hi. My name is [inaudible]. I'm a second year masters student. I'm interested in, um, machine learning and data mining. Hi. I'm [inaudible]. I'm a second year masters student and I'm interested in machine learning and natural language processing. Hi everyone, my name is [inaudible]. masters student and I'm interested in computer vision. [BACKGROUND] [NOISE] Let's go over there. [BACKGROUND] [NOISE] Great. Now, any new TAs in the back? No. Well, um, well, they're all on the slide. Okay. So uh, as you can see, we kind of have a very diverse team and so when you're thinking about kind of final projects later in the quarter, you can tap into this incredible resource. Um, so three quick announcements. Um, so there's going to be a section every week which will cover both kind of review topics and also advanced, uh, uh, topics. So this Thursday there's gonna be an overview. Um, if you're kinda rusty on Python or rusty on probability, come to this and we'll get you up to speed. Um, homework, the first homework is out, it's posted on the website. It's due next Tuesday at 11:00 PM. So remember the time, that matters."}, {"content": "Um, all submissions will be done on Gradescope. There's gonna be a Gradescope coast- code that will be posted on, uh, Piazza. So look out for that, um, later."}, {"content": "Okay."}, {"content": "So now let's, let's begin. So when I first started teaching this class, uh, seven years ago, I used to have to motivate why AI was important and why if you study it you'll have a lot of impact in the world. But I feel like I don't really need to do this. Now it's kind of inescapable that you pick up the news in the morning and you hear something about, you know, AI. And indeed we've seen a lot of success stories, right? AIs that can play Jeopardy or play Go, Dota 2, pro- even poker, all these kind of games at super human level performance. It can also, you know, read documents and answer questions, do speech recognition, uh, face recognition, um, even kind of medical imaging. And all these tasks are, uh, you read about how successful these, uh, technologies have been. Um, and then if you take a look at outside the kind of the technical circles, there's a lot of people, um, in policy, um, and trying to ask what is going on with AI. And you, you hear about, uh, these kind of very, uh, broad claims of how transformative AI will be, um, to the future of work and, um, to society and so on, and even some kind of bordering on, uh, pretty castro- you know, catastrophic consequences. So what's gonna happen in the future, no one knows, but it is fair to say that AI will be transformative. Um, but how do we get here? And to do that, I wanna take a step back to the summer of 1956. So the place was Dartmouth College, John McCarthy, who was then at MIT, and then, uh, after that he founded the Stanford AI Lab, um, organized a workshop at Dartmouth College with, um, some of the best and brightest minds of the time; Marvin Minsky, Claude Shannon, and so on. And they had this not so modest goal of trying to think that every aspect of learning or any feature of intelligence could be precisely captured so that a machine can be just, uh, simulated. So they were after the, the big question of how do you kind of solve, um, AI. So now they didn't make that much, uh, progress over the, the summer, but a lot of programs and interesting artifacts came about from that time. Um, there were programs that could play checkers or prove, uh, theorems, and sometimes even better than what, um, you know, the human proof will look like. Um, and there was a lot of optimism. People were really, really excited, and you can see these quotes by all these excited people who proclaimed that AI would be solved in a matter of years. But we know that didn't really happen and there's this kind of folklore example, um, people are trying to do machine translation. So you take an English sentence like 'The spirit is willing but the flesh is weak', you translate into Russian, which is what, um, the choice language by the US government was at that time, and you could, uh, translate back into English; and this is what you get, 'The vodka is good but the meat is rotten'. Um, so the government didn't think that was too funny, so they cut off the funding [LAUGHTER] and, um, it became the first AI winter. Um, so, so there was a period where, you know, AI research was not very active and was not well- very well funded. Um, so what went wrong here? Um, these were really smart people, right? Um, they just got a little maybe ahead of themselves. So two problems; one is that the compute was simply not there, right? It was millions or even billions of order of magnitude compared less than what we have, uh, right now. And also, the problems, the way they formulate them, intrinsically relied on camp- exponential search which, um, no matter how much compute you have, you're never going to, you know, um, win that race. Um, they also have limited, you know, information, and this is maybe a kind of a more subtle point that if I gave you infinite compute and I asked you to translate, I don't think you would be able to figure it out because it's not a computation problem. You just need to learn the language and you need to experience all the subtleties of language to be able to, you know, translate [NOISE]. But on the other hand, AI wasn't solved, but a lot of interesting, um, contributions to computer science came out of it. Lisp was- is- uh, had a lot of ideas that underlay ma- many of the high level programming languages we have, garbage collection, um, time-sharing, allowing, uh, multiple people to use the same- one computer at the same time, which is something that, uh, we kind of take for granted. And also this paradigm of separating what you want to compute, which is modeling, and how you do it, which is inference, which we'll get to a little bit later. Okay. So um, people forget quickly and, um, in the '70s and '80s, there was a renewed generation of people getting excited about AI again. Um, and this time it was all about knowledge, right? Knowledge is power and, um, there were a lot of expert systems which were created. And the idea is that if you could encode expert's knowledge about the world, then you could do kind of amazing things, and at the time the knowledge was encoded in generally a set of rules. Um, and there were a lot of programs that was written, and you'll notice that the, the scope is much narrower now. The goal isn't to solve it- all of AI, but to really focus on some choice and problems like diagnosing the diseases or converting customer's order parts into parts, and, uh- customer orders into parts and, uh, this was the first time that AI, I think, really had a real impact on industries. So uh, people were actually able to make useful, you know, products out of this. And knowledge did actually play a key ingredient in curbing this, you know, exponential growth that people were worried about. But of course, um, it didn't last long. Um, knowledge as deterministic rules was simply not rich enough to capture all the kind of nuances of the world. It required a lot of manual effort to maintain and, um, again, um, a pattern of over-promising and under-delivering that seems to plague, um, AI people, led to the collapse of the field and the kind of a second AI winter. Um, okay, so that's not the end of the story either."}, {"content": "But actually it's not kind of really the beginning either. Um, so I'm going to step back further in time to 1943. So what happened in 1943? So there was, um, a neuroscientist, McCulloch; and logician, Pitts, who were wondering and marveling at how the human brain is able to do all of these kind of complicated things. And they wanted to kind of formulate a theory about how this could all happen. So they developed a theory of, um, artificial neural networks, um, and this is kind of you can think about the root as of, you know, deep learning in some sense. Um, and what's interesting is that they looked at, um, neurons and logic, which are two things that you might not kind of necessarily associate with each other, and showed how they were kind of connected mathematically. And a lot of that early work in this era were of- around artificial neural networks, was about studying them kinda from a mathematical perspective. Um, because at that time, the compute wasn't there, you couldn't really run any kind of training new models or um. And then 1969, something interesting happened. So there's this book by Minsky and Papert called Perceptrons. And this book did a lot of mathematical analysis. And it also showed that linear models, one of the results of many, was showing that linear classifiers couldn't solve the XOR problem. Um, the problem is- another way to think about the problem is basically given two inputs, can you tell whether they are the same or not, or different. And, um, so it's kind of not a- shouldn't be a hard problem but linear classifiers can do it. And for some reason, which I don't quite understand, it killed off neural nets research even though they had said nothing about if you had a deeper network, what it could do. Um, but it's often cited that this book, ah, swung things from people who were interested in neural networks to the field of AI being very symbolic and logic driven. Um, but there was always this kinda minority group, um, who were really invested in and believed in, um, the power of neural networks, and I think this was always just kind of a matter of time. So in the '80s, there was a renewed interest. Um, people kind of discovered or rediscovered the backpropagation algorithm which allowed a kind of, for a generic algorithm that could train these multilayer neural networks because single layer remember was insufficient to do a lot of things. And then one of the kind of the early success stories, as Yann LeCun in 1989, applied a convolutional neural network and was able to recognize hand digit- written digits, and this actually got deployed, um, by the USPS and was reading kind of zip codes. Um, so this was, you know, great, ah, but it wasn't until this decade that the, um, this area of neural networks really kind of took off, um, under the moniker deep learning. Um, and, you know, AlexNet in 2012 was kind of a huge transformation, um, where they show gains on the, kind of ImageNet ba- benchmark and overnight transformed the computer vision community. Um, AlphaGo as, you know, many of you know, and many kind of other, um, and there were kind of the rest is history. Okay, so- so there's this kind of two intellectual traditions. Um, you know, the name AI has always been associated with the kind of John McCarthy logical tradition, that's kind of where it started. But, um, as you can see that there is also kind of this neuroscience inspired tradition of AI, and the two are kind of really had some deep philosophical differences and over the decades fought with each other kind of quite a bit. But I want to pause for a moment and really think about, [NOISE] maybe if there were actually kind of deeper connections here. Remember McCulloch and Pitts, they were studying artificial and neural networks, but the connection was to logic, right? So from even in the very beginning, there is kind of this synergy that, you know, some- some people can kind of often overlook. And if you take a look at AlphaGo, which [NOISE] if you think about the game of Go or many games, it's a mathematically, you can write down the rules of Go in logic in just a few lines. So it's a mathematically well-defined logical- logic puzzle in some sense. But somehow, the- the power of neural networks allows you to develop these models that actually play Go really- really well. So this is kinda one of the deep mysteries that has, kind of, uh, I think is kind of o- opens standard challenge, you know, in AI. Um, as with any story it's not a full picture, and I want to point out on this slide that, AI has drawn from a lot of different, you know, fields, many of the techniques that we're gonna look at, for example, maximum likelihood, came from your statistics or games came from economics, optimizations, gradient descent, hence from- was, you know, in the '50s completely unrelated to AI. But these techniques kind of developed in a different context. And so AI is kind of like, you know, it's kind of like a New York City. It's- it's like a melting pot where a lot of the- these techniques that kind of unified and apply to kind of interesting problems. And that's what makes it, I think really interesting because of the- the new [NOISE] avenues that are opened up by kind of unique combinations of, um, existing techniques. Okay, so- so that was a really bre- brief history of, you know, where- how we got here. Um, now I want to pause for a moment and think about, you know, what is- what is the goal? What- what AI people are trying to do? And again this- this is kind of there's two ways to think about this which and- sometimes the conflation of these causes a lot of confusion. Um, so I like to think about it as AI as agents, and AI as tools. So the first view asks the kind of standard question of, how can we create or recreate intelligence? And the second one asked, you know, how can we use technology to kind of benefit, you know, society? [NOISE] And these two are obviously very related and they have, ah, a lot of shared technical, um, overlap, but, you know, philosophically they're kind of different. So let me kind of explain this a little bit. So the idea with AI agents is, and this is, I think a lot of what, um, um, gets associated with AI, um, and especially as, you know, with science fiction. That kind of, ah, po- portrayal certainly kind of encourages this kinda view where [NOISE] you're human- we're human beings. And what you do is you look in the mirror and you say, wow, that's must- that's a really smart person. And you think okay, how- how- what- what- what can humans do that is, you know, so amazing. Well, they can, um, they can see and they can perceive the world, recognize objects. Um, they can grasp cups and drink water and not spill it. [NOISE] Um, they can communicate using language as I'm doing to you right now. Um, we know facts about the world, [NOISE] declarative knowledge such as what's the capital of France and procedural knowledge like how to ride a bike. We can reason with this knowledge and maybe ride a bike to the capital of France. And then, really importantly, we're not born with all of this, right? We're born with basically nothing, none of these capabilities, but we are born with the capacity and potential to acquire these over time through experience. And learning it seems to be kind of this critical ingredient, which drives a lot of the success in AI today but also with, um, you, know, human intelligence it's clear that learning plays such a central role in getting us to the level that we're operating at. So each of these areas has kind of spawned entire sub-fields, and people in it are kind of wondering about how you can make artificial systems that have the language, or the motor, or the visual perceptual capabilities that, you know, humans have. But are we there yet? Um, and I would- I would like to think that we are, ah, very far. So if you look at the way that machines are, have been successful, it's all with a narrow set of tasks and, you know, millions or billions of examples, and you just crunch a lot of computation, and you can really kind of optimize, um, every- any tasks that you're going to come-come up with. Whereas humans operate in a very different regime. [NOISE] They don't necessarily do any, you know, one thing well, but they are have such a kind of diverse set of, you know, experiences, can solve a diverse set of tasks and learn from each individual tasks from very few examples. And still it's a kind of a grand challenge, in from a, uh, cognitive perspective, how you can build systems with this level of capability in that humans have. So the other view is, you know, AI tools. Basically we say okay well, you know, it's kind of cool to think about how we can, uh, you know, recreate intelligence. But, you know, we don't really care about making more, um, things like humans. We already have a way of, you know, doing that, that's called babies. [LAUGHTER]. Um, so when instead what we'd really like to do is not making something that's like a human but making systems that help humans. Because, you know, after all, we're- we're humans, I guess it's a little bit selfish but, um, we're in charge right now. Um, and- and a lot of this- this view and a lot of the success stories in AI are really different from the things that you expect, you know, this, uh, this humanoid robot to come into your house and be able to do. For example this is a project from Stefano Ermon's group. Um, there's a lot of poverty in the world and, um, part of it is- is just kind of understanding what's- what's going on and they had this idea of using, uh, computer vision on satellite imagery to predict things like, you know p-, uh, GDP. Um, so this is obviously not a task that, you know, the- our ancestors in Africa were like, you know, getting really good at. Um, but nonetheless it uses convolutional neural networks which is a technique that was inspired by, um, you know the brain and so that's- that's kind of interesting. Um, you can also have another application for saving energy by trying to figure out when to cool on datacenters. Um, as AI, is, uh, being deployed in more kind of mission critical s-, uh, situations such as self-driving cars or authentication. There are- there are a f- few th- new issues that come up. So for example, there are- thi- this phenomenon called adversarial examples, um, where you can take, um, these cool-looking glasses, you can put them on your face, and you can fool the computer, um, as- of- save our- our face recognition system to think that you're actually, you know, someone else. Um, or you can post these, uh, s- stickers on stop signs and you'd get this, uh- s- save our system to think that it's a, um, a speed limit sign. So there's obviously- there's- clearly these are, you know, big problems if we think about that the widespread deploy- deployment of AI. Um, there's also a less catastrophically but also p- pretty, um, you know, upsetting which is, uh, biases that you- many of you probably have read in the news about. So for example, if you take Malay which is a language that, uh, doesn't distinguish, um, in this writing form between he and she and you stick it into Google Translate. Um, you see that she works as a nurse but he works as a programmer, which is encoding certain, uh, societal biases, um, in the actual models. And one kind of an important point I wanna bring up is that, you know, it's -- it's how is machine learning and AI kinda working today? Well, it's, um, you know, society exists. Society is generating a lot of data. We're training on this data, and kind of trying to fit the data and try and mimic what it's doing and then using predictions on it. What could possibly go wrong, right? Um, and so- so certainly people- a lot of people have been thinking about, um, how these biases are kind of creeping up and is an open and active area of research. Something a little bit more, uh, kind of s- sensitive is, you know, asking well,  these systems are being deployed to all these- all these people whether they kinda want it or- or want it or not. Um, and this, uh, this actually touches on, you know, people's, uh, you know, livelihoods. It actually impacts people's lives in a serious way. Um, so Northpointe was this company that developed a- a software called COMPAS that tries to predict how risky, um, criminal risk or how someone- how risky someone is essentially. Um, and ProPublica this organization realized whoa, whoa, whoa, whoa. You have this system that, uh, given an individual didn't reoffend is actually, um, more- twice as likely to classify blacks as incorrectly as, you know, non-blacks. So this is, uh, seems pretty problematic. And then Northpointe comes back and says actually, you know, I think we- I think we're being fair. Um, so given a risk score of 7, uh, we were fair because 60% of whites reoffended and 60% of blacks reoffended. Um, the- the point here is that there's- there's- there's actually no, um, solution to this in some sense sadly. Um, so people are finding or formulating different notions of fairness and equality between, um, how you predict or record it on different kind of, um, groups. But, um, or you can have different notions of fairness and which all seem reasonable from first principles but mathematically they can be, um, incompatible with each other. So this is- this is again an open area of research where we're trying to figure out as a society how, um, to deal with the schema that machine learning might be using these in kind of critical situations. Okay. So summary so far, um, there's an agent's view. Um, we're trying to really kind of dream and think about how do you get these capabilities like learning from very few examples that humans have into, you know, machines and a whole- maybe opening up a kind of a- a different set of technical capabilities. But at the same time, and we really need to be thinking about how these AI systems are affecting the real world. And things like security, and biases, and fairness all kind of show up. It's also interesting to note that, you know, a lot of the challenges in deployment of an AI system don't really have necessarily to do with, um, you know, humans at all. I mean, humans are incredibly biased but that doesn't mean we want to build systems kind of in our- in, um, that mimic humans and kind of inherit all the kind of the flaws that humans have. Okay."}, {"content": "Any questions about this? Maybe I'll pause for a moment."}, {"content": "So let's go on. Um, so what I wanna do next is give an overview of the different topics, um, in the course. Um, and the way to think about all this is that, um, in AI we're trying to solve really complex problems. The real world is really complicated. And- but at the end of the day we want to produce some software or maybe some hardware that actually runs and does stuff, right? And so there's a very considerable gap between these things. And so how do you even approach something like self-driving cars or, um, you know, d- diagnosing diseases? You probably shouldn't just like go sit down at a terminal and start typing because then, um, there- there's no kind of- no overarching structure. So what this class is going to do is to give you one example of a structure which will hopefully help you approach hard problems, and think about how to solve them in a kind of more principled way. Um, so this is a paradigm that I call the, um, modeling inference and learning paradigm. Um, so the idea here is that there's three pillars which I'll explain in a bit. And, uh, we can focus on each one of these things kind of in turn. So the first pillar is modeling. So what is modeling? The modeling is taking the real world, which is really complicated and building a model out of it. So what is a model? Model is a simplification that is mathematically precise so that you can, you know, do something with it, uh, on a computer. Um, one of the things that's necessary is that modeling, um, necessarily has to simplify things and, you know, throw away information. Um, so one of the kind of, uh, the, you know, the art is to figure out what information to pay attention to and what information to keep. Um, so this is going to be important for example when you work on your final projects and you have a real world problem, you need to figure out, um, you can't have everything and you have to figure out judiciously how to, um, manage your- your resources. So here's an example. If you want to for example build a- a system that can find, uh, the best way to get from point A to point B in a graph- in a- in a city you can formulate the model as a- a graph where nodes are points in the city, and edges rep- represent ab- ability to go between these points with some sort of cost, um, on the edges. Okay. So now once you have your model you can do, uh, inference. And what inference means is asking questions about your model. So here's a model you can ask for example how- what is the shortest path from, um, this point, uh, to this point. Right. And that's because now your model land is a mathematically well-defined, uh, problem now you can- it's within the realm of, uh, you know, deve- developing algorithms to, you know, solve that problem. And most of the inference is ki- being able to do these computations, um, really efficiently. And finally learning addresses the problem, where does this model come from? So in any kind of realistic setting, um, the model might have a lot of parameters. Maybe it has, you know, millions of parameters and how do you s- if it- if it- wants to be faithful to the, you know, real world that how do you get all this, uh, information there. Um, manually p- encoding this information turns out not to be a good idea. This is, um, in some sense what, um, AI from the '80s was trying to do. Um, so the learning paradigm is as follows. What we're gonna do is specify a model without parameters. Think about it as a skeleton. So in this case we have a graph but we don't know what the edge weights are. Um, and now we have some data. So maybe we have data of the form people tried to go from X to Y and they took 10 minutes, or an hour, or so on, um, and then from this data we can learn to fit the parameters of the model. We can assign, um, costs to the edges that kind of are representative of what the data is telling us, okay? So now in this way, we can write down a model without parameters, feed the data, apply a generic learning algorithm and get a model with parameters. And now we can go back and do, um, inference and ask questions, you know, about this. Okay. So this is kind of the- the- the paradigm. And I want to really emphasize that, you know, learning is not- as I've presented is really not about any one particular algorithm like nearest neighbors or neural networks. It's really a kind of a philosophy of how you go about approaching problems by defining a model and then not having to specify all the details but filling them in later. Okay. So here is the plan for the course. We're gonna go from low-level intelligence to high-level intelligence; and this is the intelligence of, um, of the, of the models that we're gonna be talking about. So first we're gonna talk about machine learning, and like I've kind of alluded to earlier, machine learning is going to be such a kind of an important building block of- that can be applied to any of the models that we kind of develop. So the central tenet in machine learning is you have data and you go to model, its main driver of a lot of su- successes in AI because it allows you to, in software engineering terms, move the complexity from code to data. Rather than having, you know, a million lines of code which is unmanageable, you have a lot of data which is collected in kind of a more natural way and a smaller amount of code that can operate on this data and this paradigm has really been, it's really been powerful. One thing to think about in terms of machine learning is that it, it is, requires a leap of faith, right. So you can go through the mechanics of down- downloading some machine learning code and you train them all but fundamentally it's about generalization, right. You have your data, you fit a model, uh, but you don't care about how it performs on that data; you care about how it performs on new experiences. And that leap of faith is something that's, um, I think gives machine learning its power but it's also a little bit, um, at first glance perhaps magical. Um, it turns out you can actually formalize a lot of this using, um, probability theory and, and statistics but that's kind of a topic for another time. Okay. So after we talk about machine learning, we're going to go back and talk about the, the simplest of models, right. So a reflex model is this. So here's a quiz."}, {"content": "Okay. What is this animal? Okay, zebra. How did you get it so fast? Well, it's kind of a reflex, where your human visual system is so good, um, at, at doing these things without thinking. Um, and so reflex models are these, um, are models which just require a fixed set of computations. So examples like are linear classifiers, deep neural networks, um, and most of these models are the ones that people in machine learning um, use. Models is almost synonymous with, um, reflex on- in machine learning. The important thing that there's no feed for it."}, {"content": "It just like you get your input bam, bam, bam, and here's your output. Okay, so that's, that's great because it's fast. But there's some problems that require a little bit more than that. Right. So for example here's another problem. Okay, quick, white to move. Where does she go? Okay, there's, there's probably like a few of you who are like chess geniuses, um, but for the rest of us, um, I have no idea. I don't know, wait, who's moving again? Um, so, so in these kind of situations, we need something perhaps a little bit more powerful than a reflex. We need agents that can kind of plan and think, um, ahead. So the idea behind state-based models is that we model the world as a set of states which capture any given situation like, uh, a position in a, in a game and actions that take us between states which correspond to things that, um, you can do in the, in this game. Um, so a lot of game applications fall in this as category of robotics, motion planning, navigation. Um, also some things that are might not be- you might think of, um, planning as such as gen- you know, generation, um. In natural language or generating an image, um, you are, uh, can be cast in this way as well. So there's three types of state-based models each of which we'll cover in, um, you know weeks of time. So search problems are the classic, uh, you control everything so you're just trying to fi- find the optimal path. There are cases where there's randomness. For example if you're trying to go from point A to point B, maybe there's traffic that you don't, you know, don't know about or, um, in a game there might be dice that are- die which are rolled, and, uh, there's a third category which are adversarial games which is cases where your playing an opponent who's actively trying to destroy you. So what are you gonna do about it? Um, so one of the games that we're gonna, uh, be talking about, uh, when we talk about games is Pac-Man; and one of the assignments is, um, actually building, um, a Pac-Man agent such as this. So, uh, while you're looking at this, think about how- what are the states and what are the actions and how would you go about you know devising a strategy for Pac-Man to eat all the dots and avoid all the ghosts? So that's something, uh, to maybe look forward to. There's also gonna be a competition. So we'll see how- who ends up at the top. Okay, so state-based models, um, are very powerful and a value to kind of have foresight. Um, but some problems are not really most naturally cast as state-based models. For example, you know, how many of you play Sudoku or have played it before? So as the goal of Sudoku is to fill in these, uh, um, blanks with numbers so that, um, every row and column and three-by-three sub-block has the digits 1 through 9. So there's a bunch of constraints. Um, and there's no kind of sense in which you have to do it in a certain order, right. Whereas the, the order in how you move in chess or something is, you know, pretty important. Um, so, so these type of problems, uh, are captured by these variable-based models where you kind of think about a solution to the problem as an assignment to the individual variables, under some constraints. So constraint satisfaction problems, we'll spent a week on that, um, these are hard constraints. For example two people can be- or a person can't be in the two places at once for example. Uh, there's also Bayesian networks which we'll talk about which are variable-based models with, uh, soft dependencies. For example if you're trying to track, um, you know, a car over time, these are the positions of the car. These variables represent the position of the cars and these, uh, E's represent, uh, the- the sensor readings of the position of the car at that particular position and inference looks like trying to figure out where the car was given all this kind of noisy sensor reading. So that's also gonna be another assignment where you're going to deal with."}, {"content": "Okay. So finally, um, now we get to high-level. What's- so what is high-level intelligence here? Um, and I put logic here, um, for a reason that you'll see clear."}, {"content": "Yeah, is there a question? The Sudoku, can you explain why it's not a state-based model? Yeah, so the question is why is not the- why is the Sudoku problem not a state-based model? Um, you can actually formulate this as a state-based model, um, by just thinking about the sequence of, uh, assignments. But it turns out that, um, you can formulate in a kind of more natural way as a variable-based model which allows you to, uh, take advantage of some kind of more efficient algorithm to solve it. Right, it's- think about these models as kind of different, um, analogy as like a programming language. So yes, you could write everything in you know C++ but sometimes writing in you know, Python or, or SQL for some things might be more- might be easier. Yeah. [inaudible] state based problem where you have both adversarial elements and an element of randomness? Yeah, so the question is how do you categorize state-based models where there is both randomness and an adversary? Um, we're also gonna talk about those as well. Um, and those would be- I, I would classify them as adversarial but there is also a random component that you have to deal with, games like backgammon. Yeah, question. [inaudible] Yeah, so the question is about whether, uh, some of these are more continuous and some of them are more discrete. Uh, I don't necessarily think of, uh, so a lot of the reflex models actually can work in continuous state spaces, for example images. Um, actually it's, it's almost a little bit of the opposite where, um, the logic-based models are in some sense more, you know, discrete but you can also have continuous elements, you know, in there, um, as well. Um, so in this class, we're mostly going to focus on kind of discrete objects because they're just going to be simpler to work with. Okay, so what is this logic? So the motivation here is that suppose you, um, wanted a little companion who, um, you could boss around and, um, help or help you do things, let's say; that's a better way to say it. Um, so you'd like to be able to say okay, you know, tell us some information, um, and then later you wanna be able to ask some questions and have the system be able to reply to you. Um, so, um, you know how- how would you go about doing this? One way you could think about is building a system that you can actually talk to using natural language, okay. So I'm actually going to show you a, a little demo, um, which, uh, is going to come up in the last assignment on logic; um, and well, let's see what you think about it. Uh, okay, so this is going to be a system that is, um, based on logic that I'm going to, um, tell the system a bunch of things and I'm going to ask some questions. So, um, I want you all to follow along and you see if you can, you know, play the role of the agents. Okay. So I'm going to teach you a few things like, um, Alice is a student, okay. So it says I learned something. Now let's, let's quiz, um, is Alice a student? Okay."}, {"content": "Good. So that worked. Um, is Bob a student? What should the answer be? I don't know who's Bob. Um, okay. So now let's do, um, students are, uh, people. Um, Alice is not a person. I don't buy that [LAUGHTER] okay. So, um, okay it's, you know, it's doing some reasoning, right? It's using logic, it's not, uh, just, um. Okay. So now, let's do, um, Alice is from Phoenix. Phoenix is a hot city. I know because I've lived there. Um, cities are places, and if it is snowing, uh, it is, um, then it is cold. Okay, got it. So, um, is it snowing? I don't know."}, {"content": "Um, so how about this?"}, {"content": "Okay. So if, um, a person is from a hot place and it is cold, then she is not happy, okay. True."}, {"content": "Right, um. I guess those of you who have spent all your live in California would maybe appreciate this. But, um, okay, so ho- is it snowing now? How many of you say yeah, it's snowing? How many say no? You don't know?"}, {"content": "Okay. [inaudible] Ah, ah, [LAUGHTER] um, how about if I say Alice is, ah, happy. Okay, so is it snowing now? No, it should be no. Okay. So you, you guys were able to do this."}, {"content": "Okay. So this is kind of an example of a interaction which, um, if you think about it has is ve- very different from where you would see kind of in a typical, um, you know, ML system where you have to show it millions of examples of one particular thing and it can do a kind of one task. This is much more of a very open-ended set of, um, I wish to say that the, the experiences are super rich but they're definitely diverse. I teach- I just give one statement. I say it once and then all of a sudden it has all the ramifications and kind of consequences that built in and it kind of understands in a kind of a deeper level. Of course this is based on, you know, logic systems. Um, so it is brittle but this is kind of just a proof of concept to give you a taste of what I mean when I say logic. So, ah, these systems need to be able to digest this heterogeneous information and reason deeply with that information. And we'll see kind of how, um, logic systems can do that. Okay. So that completes the tour of the topics of this class. Um, now I want to spend a little bit of time on course logistics. Uh, so I wanna- all the details here are online. So I'm not going to be complete in my coverage, um, but I just wanna give you a general sense of what's going on here. Okay. So what are we trying to do in this course? Um, so prerequisites, um, there's programming, um, discrete math and, ah, probability. So you need t be able to code and you need to be able to, um, do some math and, uh, some kind of basic proofs. Right? So these are the classes that are, um, required or at least recommended that you- or if you have some equivalent experience that's, you know, fine too. Um, and what we- what should you hope to get out of this course? Right. So one had- the course is meant to be giving you a set of tools using the modeling inference learning paradigm. It gives you a set of tools and a way of thinking about problems that hopefully will be really useful for you when you go out in the world and try to solve real world problems. Um, and also by- as a side product I also want all of you to be more proficient at your math and programming because those are kind of the core elements that, ah, enable you to do kind of interesting, you know, things in AI. So a lot of AI and you, you read about it, it's very flashy but really the foundations are still, um, just you know math and programming in some sense. Okay. So the coursework is homeworks, exam, and a project. That's what you have to do, um, Homeworks, there's eight homeworks. Each homework is a mix of writing- written and programming problems centered on a particular application covering one particular type of model essentially. Um, like I mentioned before there's a competition for extra credit. There's also some extra credit problems in the, in the homeworks, um, and when you submit code, we're gonna run- we have an auto-grader that runs. It's gonna run on all the test cases but you get a feedback of only a subset. So you can, um, it's like, you know, in machine learning, you have a train set, and you have a test set. So don't train on your test set. [LAUGHTER] Okay. So um, the exam is, ah, testing your ability to use the knowledge that you learn to solve new problems. Right. So there's, um, I think it's worth taking a look at exam because this, this kind of surprises people every- the exam is a little bit different than the types of problems that you see on, on the homework and there are kind of more problem, you know, solving. So the exam isn't going to be like a multiple choice like, okay, you know, um, you know, when was Perceptrons published or something like that. It's gonna be, here's a real life problem. How do you model it and how do you come up with a solution? Um, they're all going to be written. It's closed book except for you have a one page of notes and this is a great opportunity to actually, um, review all the material and actually learn the ah, the content in the class. Um, so the project I think is a, a really good opportunity to take all the things that we've been talking about in the class and, um, try to find something you really care about and try to apply it. Work in groups of three and I really recommend finding a group early, um, and as I emphasize it's your responsibility to find, you know, a good group. Right? Um, don't come to us later like one week before the project deadline and say, \"Oh, you know, my group members they, um, they ditched me,\" or something. We really try to, try to nail this down use Piazza to- or your other social networks to find a good group. So throughout the quarter there's going to be these milestones for the projects. So, um, to prevent you guys from procrastinating into the very end, um, so there's gonna be a proposal where you try and brainstorm some ideas, progress report, a poster session which is actually a whole week before the final report is due, um, and the project is very open. So this can be, um, really liberating but also might be a little bit daunting. Um, we will hopefully give you a lot of structure in terms of saying okay, how do you define your task? How do you implement different, um, baselines or oracles? Which I'll explain later. How do you evaluate? How do you, um, analyze what you've done? And each of you will- each project group will be assigned a CA mentor, ah, to help you, ah, through the process and you're always welcome to come to my office hours or Dorsa's, or any of the CAs to get additional, um, help either brainstorming or figuring out what the next step is. Ah, some policies, ah, all assignments will be submitted on Gradescope, um, there are seven total late days you can use, and most two per assignment. After that there's no credit. Um, ah, we're gonna use Piazza for all communication so don't email us directly. Leave a post on Piazza. If- I encourage you to make it public if it's, it's not sensitive, but if it's, you know, personal, then obviously make it private, um, and try to help each other. We'll actually award some extra credit for students who help answer, um, other student's questions. So all of the details are on the course website. Okay. So one last thing and it's really important and that's the Honor Code. Okay. So especially if you're, um, you know, you've probably heard this if you've been at Stanford. If you haven't, then I wanna really kind of make this clear. So I encourage you all to have- collaborate, discuss together. But when you- when it comes to actually the homeworks, you have to write up your homework and code it independently. So you shouldn't be looking at someone's writeup. You shouldn't be looking at their code. Um, and you definitely shouldn't be copying code off of GitHub. Um, um, that's hopefully should be, you know, obvious and maybe less obvious, you should not- please do not post your homework assignments on GitHub. I know you're probably proud of the fact that your Pac-Man agent is doing really well but please don't post on GitHub because then that's going to be our Honor Code violation. Um, when debugging, um, with- if you're working together, it's fine to as long as it's kind of looking at input-output behavior so you can say to your partner, \"Hey, I put in this, um, input to my test case and I'm getting a 3. What are you getting?\" So that's fine but you can't."}, {"content": "Remember don't look at each other's code. Um, and to enforce this, we're gonna be running MOSS, which is a software program that looks for code duplication, um, to, to make sure that, ah, the rules are being followed and, you know, changing one variable name is- or you'll be so- anyway enough said. [LAUGHTER] Just don't, don't, don't do that. Okay? Any questions about this? I wanna make sure this is important or about any of those logistics. Yeah. [inaudible] The final project, ah, you can put on GitHub. Yeah. Yeah. Yeah, private GitHub repos, uh, is fine. Yeah, question in the back? Is it necessary to have a group or can you do a solo project? Uh, the question is can you, can you do a solo project? You can do a solo project, you can do a project with two people, or you can do a project with three. I would encourage you to try to work in, uh, groups of three because you'll be able to do more as a group, and there is definitely, uh, you know, it, it, it's not like if you do a solo project we'll be expecting like one third of the, the work. So okay."}, {"content": "Anything else? All right."}, {"content": "Okay. So in the fi- final section, I want to actually delve into s- some technical details. Um, and one thing we're going to focus on right now is, um, the, kind of inference and learning components of, of this course. So I'm going to talk about how you can approach these through the lens of, you know, optimization. So this is going to be, uh, it might be a review for some of you but hopefully, it's gonna be a, a good, um, you know, way to get everyone on the same page. Okay. So what is optimization? There's two flavors of optimization that we care about. There's, uh, Discrete Optimization, where you're trying to find the best, uh, discrete object. For example, you're trying to find the best, uh, path or the path P that minimizes the cost of that path. Um, we're going to talk about one algorithmic tool, um, based on Dynamic Programming which is a very powerful way of solving these, um, complex optimization problems. Um, and the key, you know, property here is that the set of paths is huge and you can't just, uh, trial them and compute the cost and choose the best one. So you gonna have to choose something clever. The second brand of optimization is continuous optimization and formally this is just finding the best of vector of real numbers that satisfies or minimizes some objective function. So a typical place this shows up is in learning where you define, uh, objective function like the training error and you're trying to find a weight vector W. So this notation just means it's a list of numbers, D numbers that minimizes the training error. And we're going to show that gradient descent is, uh, uh, easy and a surprisingly effective way of solving these, um, continuous optimization problems. Okay. So to introduce these two ideas, I'm going to look at two, um, problems and trying to kind of work through them. So this might be also a good, um, you know, way to think about how you might go approach a, you know, homework problems. And I'll try to kind of talk you through this, um, in a bit more detail. Okay, so the first problem is, um, you know, computing edit distance. Um, and this might not look, you know, like an AI problem, but a lot of, ah, AI problems have this as kind of a, you know, building block if you wanted to do some sort of matching between, um, you know, two words or two, um, biological sequences. So the input is you're given two strings. Um, we're gonna start writing over here on the board just to work this out. So given two strings, um, S and T. Um, so for example, um, a cat and um, the cats. Okay. So these are two strings and you wanna find the minimum number of edits that is needed to take transform S into T. And by edits I mean you can insert, um, a character like you can insert S, you can delete characters, I can delete this A and you can substitute one character for another. So you can replace this A with a T. Okay."}, {"content": "Um, so here's some examples. What's the edit distance of cat and cat? It's 0, you don't have to do anything. Cat and dog is 3, cat and at is 1, you insert the A or insert a C. Um, cat and cat is 1, um, and a cat and the cats is 4. Okay. So the challenge here is that there are, ah, quite a different number of ways to insert and delete. Right, so if you have a string of- that's very long there's just way too many things to like just try out all of them. Okay, so then, how do we, how do we go about, um, coming up with a solution? So any ideas?"}, {"content": "Yeah. [inaudible] simplify the output in terms of saying that the substitution tells us we considered [inaudible] deletion peoples who considered a substitution or vice-versa by saying like an empty character. Yeah, yeah. So let's try to simplify [NOISE] the, the, the problem a bit. And building up on your what you, um, what was said. So, um, one thing to note is that okay, where so the general principle, let me just write the general principle, um, is to, you know, reduce the problem to a simpler problem because then you can hopefully solve- it is easier to solve, and then you can maybe keep on doing that until you get something that's trivial. Okay."}, {"content": "So there's maybe two observations we can make. One is that well, we're technically saying we can, um, you know, insert into S right but if we insert into S, it makes the problem kind of larger in some sense, right? I mean that's not, that's not good. That's not reducing the problem. But, but whenever we insert into S, um, we probably want to insert things which are in T. We wanna like cancel something out, right? So we wouldn't insert a K there for any reason. We probably wanna insert a S in which case no S matches that and then we've reduced that problem, right? So we can actually think about, you know, inserting into S to S as equivalent to kind of deleting from, um, from T. Okay, does that make sense? All right. So another observation we can make is that, you know, we can start inserting anywhere. We can start inserting here and then jump over here and to this. But this just introduces a lot of, um, you know, ways of doing it which all kind of result in the same answer. So why don't we just start more systematically at one end and then just proceed and try to chisel-off the problem, um, kind of let's say from the end. Okay, so start at the end? Okay, so, so now we have this problem and to draw a problem in a little box here. Um, so let's start at the end. Yeah, question. What's the reasoning used to reach that principle start at the end? [NOISE]. [NOISE] the question is why are we starting at the end as oppo- well, the idea is that if you start at the end then you have kind of a more systematic and consistent way of, you know, reducing the problem. So you don't have to think about all the permutations of where I can delete and substitute. Why is it more systematic to go from the right to the left than from the left to the right? We can also do it left to right. So the end or the start is both fine. This is just- I just picked the end. Yeah. Are we not starting at the end and then give us the optimal strategy? Yeah, the question is how do we know that starting, um, at one end can give you the optimal strategy? Um, so, you know, if you wanted to prove this more rigorously there's some work but, um, I'll just try to give you a, you know, an intuitive answer. Um, suppose you didn't start at the end, and you just made a sequence of steps like I insert here, I delete here, and then I went over here and um, did all those operations to S. I could have equivalently also just sorted those by, you know, where it was happening and then just proceeded from one end to the other, and I would arrive at the exact same answer. So without loss of generality, I can start at that."}, {"content": "Any other questions? Okay."}, {"content": "So yeah. Instead of doing this wouldn't the more viable [NOISE] approach be that trying to recognize some patterns instead of doing this. I think between the two strings \"s\" and \"t\" like some form of- some sort of [NOISE] pattern [inaudible] string. Yeah. So the question is, maybe you can recognize some patterns. Uh, it's like okay, oh, cat. That's- that's- maybe those should be lined up. Um, I guess these examples are chosen so that these patterns exist, but we want to solve the problem for cases where, um, the pattern might not be obvious. So it could be- we want to work it for- it to work for all strings. Maybe there is no pattern, and we still would want to- kind of an efficient algorithm to do it. Yeah. Can't we just like use dynamic programming? Like we go one by one, there was always like [inaudible] - Yeah. Either we're doing, um, substitution, or, um, otherwise it's like the same character. Or we have to insert- Yeah. - um, and then we keep going, and you just like [NOISE] remember each like to- to strings that we have at one point- Uh-huh. -so that if we calculated that we don't have to do it again. Yeah."}, {"content": "Yeah. That's it. Yeah. Yeah. Yeah. Great idea. Let's do dynamic programming. Um, so that's what I'm kind of trying to build up from- uh, build up to. Okay so, um, so if you look at this- so dynamic programming is a kind of a general technique that essentially allows you to express this more complicated problem in terms of a simpler problem. Uh, so let's start with this problem. If we start at the end, um, if the two match then, well we can just immediately, um, you know, delete these two and that's- it's gonna be the same, right? So we can get- we are gonna get some free rides there. Okay, but when they differ, um, now we have many options. So what we could- what could we do? Well, we could, um, um, you know substitute. Okay, we can change the \"t\" to an \"s\". So what does that leave us with? So I can do a cat, [NOISE] \"t\" is the- the cat, the- [NOISE] Okay, so I can substitute. [NOISE] Um, [NOISE] okay. Um, what else can I do? [NOISE] Someone say something I can do. [NOISE] So I can insert, um, insert where into- [OVERLAPPING] So I can insert an \"s\", right? Yes. [NOISE] But that's the same as, you know, [inaudible] deleting from \"t\". So by, uh- you can basically also just delete this \"s\". Um, so this is our cat, [NOISE] and I deleted this \"s\" from \"t\". Okay, so this is, um, let's call it, uh, you know, um, I guess let's call this insertion- it's technically insertion [NOISE]. And then finally what can I do? [NOISE] I can also remove \"t\". So [NOISE] a, ca, the, cats. Okay, so this is delete. [NOISE] And right now you're probably looking at this like, well, obviously, you know, you sho- you should do this one. But in general it's hard to tell. What if I just give you some arbitrary strings, you know, who knows what the right answer is. Um, so in general how do you pick? Yeah. In the second one, the \"t\" is supposed to be for cats. [NOISE] [inaudible] You mean this one? Yeah. So here I inserted an \"s\", right? But then because there's two s's here, I just canceled them out and [NOISE] what was left [inaudible] So you can think about this as really deleting from- What if I'm considering [NOISE] [inaudible] Like in the original problem you said we're transferring \"s\" to \"t\". Yeah."}, {"content": "Yeah."}, {"content": "Yeah. So, um, um, because of this I'm kind of trying to re-frame the [NOISE] problem a little bit. Okay, so which one should I choose?"}, {"content": "Yeah. What about the substitution the other way? Um, the substitution the other way meaning change- \"s\" to \"t\". Sorry there's too many s's and t's here which [LAUGHTER] is going to be a bit unfortunate. And then replace the last s in cats with \"t\". Oh, you could- How do we eliminate that [inaudible] [NOISE] Um, that's- you can think about that as kind of equivalent. So, if you identify two letters that you want to make the same, then [NOISE] you could- you can replace the one to be the other, or the other to be that. I mean if- officially we've been kind of framing it as we're only editing \"s\" which is the reason that it's asymmetric. [NOISE] Okay, so which one of these? Door \"a\" door \"b\" or door \"c\"? Yeah. Would you look [inaudible] between \"s\" and \"t\" for every step [NOISE] [inaudible] because there's \"cat\" in both of them? Yeah, so you could try to look inside but, um, but remember these are- might be really complicated. So you- we wanna kind of a simple mechanized procedure to tell. [NOISE] What about the next letter? The next letter. \"t\" [inaudible] Um, yeah let's- let's pretend these are- you- you can't see inside them. Okay."}, {"content": "[LAUGHTER]. Keep going with each of the different cases. Yeah, okay, so let's keep on going. [NOISE] So, I'm not going to draw everything, but you can also try to break this down into- maybe there's three actions here, and three actions here. All right. Um, and at the end of the day you hopefully have a problem that's simple enough, that, um, where \"s\" equals \"t' or something then you're done. Um, but then, you know, how- how do I- how do I know? Suppose I've solved this. Suppose if someone just told you, okay, I know this cost, I know this cost, I know this cost. What- what should you do? [inaudible] Yeah, you should take the minimum, right? Like remember we want to minimize the edit distance. So, um, there's three things you can do. Each of them has some costs of doing that action which is, you know, one. Every edit is the same cost. And then there's a cost of, you know, continuing to do whatever you're doing. And so we're just gonna take the minimum over those. Yeah. [inaudible] How do we know that that's, like- that's the maximum amount of distance that we have to take? Yeah, so I was trying to argue that, um, with- if you're going to right to left, it's, uh, without loss of generality. Because if you've- went left to right, or in some other order, you can also replay the edits, um, in order. [inaudible] [NOISE] one letter that you needed one assertion like [inaudible] like upstream. But if you went from like the left it looks like as if you're [inaudible]. [NOISE] Yeah. [inaudible] Okay. Yeah. I think it works. [NOISE] Um, okay, so- so let's, um, try to code this up and see if we can make this program work. Okay, so, um, I'm gonna do editDistance. Can everyone see this? Okay, so, um, so I'm gonna define a function that takes two strings, and then I'm going to um, define a recurrence. So, recurrences are- are, I guess, one word I haven't really used, but this is really the way you should th- kind of think about, uh, dynamic programs, and this idea of taking complex problems and breaking it down. It's gonna show up, in you know, search problems, MDPs, and, you know, games. So, I guess it's something that you should really be comfortable with. So, let's um, define recurrence, uh, as follows. Um, so remember at any point in time, I have, uh, let's say a sub problem, and since I'm going right to left, I'm only considering the first, um, \"m\" letters of \"s\" and the first letter \"n\" letters of \"t\". Okay, so recurse is going to return the minimum edit distance between two things, the first \"m\" letters of \"s\", and the first \"n\" letters of \"t\". Um, I'm gonna post this online so you guys don't have to, like, copy- try to copy this."}, {"content": "Um, okay, so, um, okay, suppose I'm gonna- I'm gonna define this function. Uh, if I have this function what should I return? Recurse of-. [inaudible] So \"m\" is an integer, right? So \"n\" is an integer, so I'm going to return the length of \"m\" and the length of \"n\". Okay, so that's kind of, uh, the initial state. [OVERLAPPING] Sorry. Yup. Okay."}, {"content": "Um, All right. So now you need to fill out this function. Okay, so let's- let's um, consider a bunch of cases. So here's some easy cases. Suppose that, um, \"m\" is zero, right? So I have- comparing an empty string with something that has \"n\" letters. So, what should the cost of that be? [NOISE] I heard some mumbling."}, {"content": "[OVERLAPPING]. It should be \"n\" [NOISE] and symmetrically if \"n\" is 0 then result should be \"m\", um, and then if now we come to the kind of initial case that we consider which is the end [NOISE] match a match. So, if \"s\" um, the last letter of \"m\", you know, this is 0-based indexing. Um, so that's why there's a minus 1. So, this matches. [NOISE] Then what should I do? [NOISE] So, now we reduce this to a sub problem, right? [inaudible] So, I have \"m\" minus 1 and \"n\" minus 1. Okay. And now comes the fun case which we looked at. So there's- um, in this case the last letter doesn't match. So, I'm gonna to have to do some sort of edit, can't just let it slide. Yeah."}, {"content": "Question. Would you- do you need a full \"s\" to \"t\" compare or \"s\" through \"m\" and then \"t\" through \"n\" to compare? Worse than doing a full s, a compare. [OVERLAPPING] rather than waiting until, um, first- Yeah. -stream at the last slide than that. There- there's probably a way you can make this more efficient. I'm just gonna try to get the basic thing in there. Okay. So substitution. Okay. So what's a cost of a substitution? I pay 1 to do the substitution, but and in- as a reward I get to, um, reduce the problem to n minus 1 and n minus 1, right? So I lop off a letter from s and I lop off a letter from t. So what else can I do? So I can, um, you know, delete. [NOISE] So that also costs 1. And when I delete, I delete from s and then n. So this remains the same. And then now you can think about the insertion, um, is n minus 1, right? Because remember insertion into s is deletion from t, that's why this is n minus 1. Okay."}, {"content": "And then the result is just gonna be a minimum of, uh, all these things. Okay. Return result. Okay. So just, uh, and then, how do I call this function? Um, a cat, the cats. [NOISE] So let me print out the answer. Um, let's see if it works. Okay. Print out 4. Therefore, I conclude it works now. [LAUGHTER] I mean if you were doing this, uh, you would probably want to test it some more, but in the interest of the time, I'll kind of move on. So let me just kinda refresh."}, {"content": "Okay. So I'm computing this at a distance between two strings and we're gonna define a recurrence that works on sub problems, where the sub problem is the first m letters of s and the first n letters of t. And the reason I'm using integers instead of, um, strings is to avoid like string copying, um, implementation detail, but it doesn't really matter. Um, so base cases. So you wanna reduce your problem to a case where it's- it's trivial to solve. Um, and then we have the last letter matches. And then we have a letter doesn't match and you have to pay some sort of cost. I don't know which action to take. So I'm gonna take them, you know, minimum of all of them. And then I call it by just calling, you know, recurse. Okay. So this is great, right? So now I have a working thing. [NOISE] Um, let's try another test case. So I'm gonna make this. Um, so if I do times 10, this, uh, basically, uh, replicates this string 10 times. So it's a- it's a long string-longer string. [NOISE] Okay. So now I'm gonna run it. [OVERLAPPING] Maybe I shouldn't wait for this. Is there a base case? Um, there is a base case, I- I think that it expanded- it's- what- what's wrong with this code? Very slow. Um, yes, it's very slow. Why is it slow? [BACKGROUND] Yeah, right? So- so I'm recursing. [NOISE] Every point recurses three times. So you kind of get this exponential, you know, blob. Um, so there's kind of a- how do you solve this problem? [BACKGROUND] Yeah. You can memo I think I heard the word memoize, which is another way to kind of think about. Memorize plus, um, I guess, recurrences is dynamic programming, I guess. Um, so I'm gonna show you kind of this, um, way to do it which is pretty, uh, uninvasive. Um, and generally I recommend people. Well, get the slow version working [NOISE] and then try to make it faster. Don't try to be, you know, too slick at once."}, {"content": "Okay. So I'm gonna make this cache, right? And I'm gonna say if m, n is in the cache, then I'm gonna return whatever's in the cache. So cache is just a dictionary mapping. Um, the key which is, um, identification of the problem I'm interested in solving, and the result which is the answer that I computed. So if I already computed it, I don't need a computer again, just return it. And then at the end, if I have to compute it, then, um, I have to put this in the cache. [NOISE] Okay? So three lines or four lines, I guess. Yeah. [BACKGROUND] [NOISE] Yeah. That's a great point. Uh, this should be outside of the recurse object. Yeah."}, {"content": "Glad you guys are paying attention. Um, otherwise, yeah, it would do basically nothing. Any other mistakes? [LAUGHTER] Yeah. Um, there is also function decorators that like implement memoizing for you. In this class, are you okay if we use that or would you rather us like make our own in this case? Um, you can use the deco- you can be fancy if you want. Okay."}, {"content": "Um, yeah. But- but I think this is, you know, pretty transparent. Easy for learning purposes. Okay. So let's run this. So now it runs instantaneously as opposed to- I actually don't know how long it would have taken otherwise. Okay. And sanity check for t is probably the right answer because there's four was the original answer and multiply by 10. Okay."}, {"content": "any other questions about this? [NOISE] So this is an example of, you know, kind of basic, uh, dynamic programming which are, uh, you'd solve a problem trying to formulate it as a recurrence of a complicated problem in terms of smaller problems. Um, and like I said before this is gonna kind of show up, um, um, over and over again in this class. Yeah. [BACKGROUND] Yeah. So the question is why does this reduce, uh, redundancy. [NOISE] Is that right?"}, {"content": "Um, so maybe I can do it kinda pictorially. Um, if you think about, let's say, you have a, um, a problem here, right? And this gets, um, you know, reduced to, um, um, I'm just making kind of a arbitrary, um, diagram here. So this problem gets reduced to these two. And this problem gets reduced to these two, um, and- and so on, um, right? So if you think about- if you didn't have memoization, you will just be paying for the number of paths. Every path is a kind of you have to compute from scratch. Whereas, if you do memoization, you pay the number of nodes here, which a lot of this has shared like here. Um, you know, once you compute this, no matter if you're coming from here or here, you're kind of using the same value. Okay. So let's- let's move on. So the second problem, um, we're gonna talk about is, uh, has to do with continuous optimization. [NOISE] And the motivating question here is how do you do, um, regression? Which is a kind of a bread and butter of, um, you know, machine learning here. [NOISE] So here we go. Regression. Okay."}, {"content": "So imagine you get some points. Okay, so I give you a point which is 2, 4. Then I give you another point, let's say 4, 2. And so these are data points, you want to, let's say, predict housing price from, you know, square footage or something like that. You want to predict health score from, um, your blood pressure and some other things. So this is pretty common in machine learning. And the question is how do you fit a line? I'm going to consider the case where your line has to go through the origin, just for simplicity. Um, so you might want to like find, you know, a fit. Two points is maybe kind of a little bit degenerate, but that's the simple example we are going to work with. In general you have lots of points and you want this to fit the line that best kind of, uh, is close to the points. Okay, so how do you do this? So there's a principle called least squares, which says, well, if you give me a line which is given in this case by a slope w, I'm going to tell you how bad this is. And badness is measured by looking at all the training points, and looking at these distances. Right. So here I have, you know, this particular, uh, a particular, let's say point x_i. If I hit it with a w, then I get, basically the, uh, you know, the y-intercept here, not the y-intercept but the- like the y value here. That's my prediction. The real value was y_i, which is, you know, up here. And so if I look at the difference, I want that difference to be zero. Right. So in, in least squares, I square this, and I say, I want this to be as small as possible, right. Now, this is only for one point. So I'm going to look at all the points. Let's suppose I have n points, and that's a function that I'm going to call f of w, which basically says, for a given weight vector, which is a slope, give me a number that characterizes how bad of a fit, um, this is. Where 0 means that I fit everything perfectly, and large numbers mean that I fit poorly. Okay?"}, {"content": "All right. So, so that's your regression. So how do I solve a regression problem? So how do I optimize this? Can you do this in your head? So if I actually had these two points, what should w be? Okay, it doesn't matter. We'll, we'll compute it. So how do we go about doing this? So one principle, which is maybe another general takeaway is, abstract away the details. Right. Um, this is also true with the dynamic programming, but sometimes, you know, you get- if you're too close to the board, and you're looking at, oh man, these, these points are here and I need to fit this line. How do I do that?"}, {"content": "You kind of get kind of a little bit stuck. Why don't we think about this f, as say some function? I don't, I don't really care what it is. And let's plot this function. Okay. So now this is a different plot. Now, this is, ah, the weight, and this is f of w. [NOISE] Always label your axes. And let's say this function looks like this. Okay. So which means that for this slope, I pay this, you know, amount, for this slope, I pay this amount and, and so on. And what I want to do, I want to minimize f of w, which means, I want to find, um, the w which, um, has the least value of f of w, right? Question?"}, {"content": "Okay. So you take the derivative. So what is the derivative giving you? It tells you where to move, right? So if you look over here, so you can- in general, you might not be able to get there directly, in this actually particular case you can because you can solve it in closed form, but I'm going to try to be more general. Um, so you start here. This, this derivative tells you, well, the function is decreasing if you move to the right. So then you should move to the right. Whereas over here, if you end up over here, the derivative says, the function is decreasing as we move to the left. So you should move to the left, right? So what I'm going to introduce is this, uh, algorithm called gradient descent. It's a very simple algorithm. It basically says, start with some place, and then compute the derivative, and just follow your nose. Right? If the derivative says it's negative, then just go this way. And now you're on a new point, you compute the derivative again, you descend, and now you compute it again. And then maybe you compute the derivative and it says keep on going this way and maybe you overshoot, and then you come back. And then, you know, hopefully you'll end up at the minimum. Okay."}, {"content": "So let's try to see what this looks like in code. So gradient descent is one of the simplest algorithms, but it really underlies essentially all the algorithms that you people use in machine learning. So let's do points."}, {"content": "We have two points here. Um, and I'm going to define, um, some functions. Okay, so f of w, so what is this function? So I'm going to sum over all the different, um, you know, and basically at this point it's converting math into Python. So I'm going to look at all the points. So for every x, y, what the model predicts is w times x minus y. And if I square that, that's going to be the error that I get on that point. Then, if I sum over all these errors then I get my objective function. Okay. Array of- so yeah. So you can put array here if you want, but it doesn't matter. It's, it's actually fine."}, {"content": "Okay. So now I need to compute the derivative. So how do you compute the derivative? So if your calculus is a little bit rusty, you might want to brush up on it. So what's the derivative? Re- remember we're taking the derivative with respect to w, right? There's a lot of symbols here. Always remember what you're taking derivative with respect to. Okay. The derivative of the sum is the sum of the derivative. So now I need to take the derivative of this. Right. And what's the derivative of this? Something squared, um, you bring the two down here, and now you multiply by the derivative of this. And what's the derivative of this? Should be x. Right? Because this is a- y, this is a constant, and w derivative- w times x with respect to w is x. Okay."}, {"content": "So that's it. Okay, so now let's do gradient descent. Let's initialize with w equal 0. Then I'm going to just, um, you know, iterate a hundred times. Normally, you would set some sort of stopping condition, but let's just keep it simple for now. Okay, so for every moment, I'm going to- I have a w, I can compute the value of the function, and also take the gradient of the derivative. Gradient just means derivative in higher di- dimensions, which we'll want later. Um, okay. And then, what do I do? I take, uh, w, and I subtract the, the gradient. Okay. So remember- okay, I'll be out of here. Okay. So, uh, I take the gradient. Remember I want to have the gradient. Uh, gradient tells me where the function is increasing, so I want to move in the opposite direction. And eta is just going to be this, uh, step size to, um, keeping things under control. We'll talk more about that next time."}, {"content": "Okay, so now, I want to do, print out what's going on here. So iteration, print out the function, and t value. Okay. All right, so let's compute the gradient. And, um, so you can see that the iteration, we first start out with w equal 0. Then it moves to 0.3, then it moves to 0.79999999 and then it looks like it's converging into 0.8. And meanwhile, the function value is going down from 20 to 7.2 which happens to be the optimal answer. So the correct answer here is 0.8. Okay, so that's it. Next time we're going to keep, uh, we're going to start on the machine learning lecture."}], "Sam Altman on His Feud with Elon Musk\u2014and the Battle for AI's Future": [{"content": "what is the fundamental conflict between Elon Musk and his various allies meta being one of them and you guys like what what is the disagreement fundamentally about look I'm I don't live inside elon's head so this is a little bit of of speculation uh Elon definitely did a lot to help open eye in the early days and in spite of all of this I'm very grateful and I think he's just a sort of legendary entrepreneur um he's also clearly a bully and he's also someone who clearly likes to get in fights you know right now it's me it's been Bezos Gates Zuckerberg lots of other people hi honestly listeners Barry here and I'm so excited for you to listen to this wide ranging and fascinating conversation with open AI Sam Altman before you watch it I wanted to come and talk to you about a big endof year Push by the end of 2024 in just a few days we want to get to a million Free Press subscribers a million people who Value Independence and curiosity and who above all want a news source that reflects reality if you're here if you're on this YouTube page we know it's not just because you believe in Fearless old school journalism for yourself it's because you think it's a necessity for democracy free pressers tell us again and again that we're not just a media company we're a public trust I'm confident that by becoming one of the first million free pressers you will be getting in on the ground floor so before you watch this video take out your cell phone or open a new tab and go to vf.com subscribe and become a free presser today if you're already signed up you can earn a free year subscription or even a pair of TGIF Socks pre-worn by Nelly if you refer your friends or family members okay one more time before we get to Sam Altman go to the free press's website at vp.com ssubscribe and help us get to our goal of a million free pressers by 2025 we are so close and we really appreciate your support on to the show from the free press this is honestly and I'm Barry Weiss just a few years ago as AI technology was beginning to spill out of startups in Silicon Valley and hit our smartphones the political and cultural conversation about this nent technology was not yet clear or at least it wasn't clear yet to civilians like me I remember asking former Google CEO Schmidt on honestly in January 2022 if AI was just like and this is actually what I said the sexy robot in exmachina I literally said to him what is AI how do you define it I do not understand I cringe listening back to that because today in the waning days of 2024 not only has it become clear what AI is and how to use it chat GPT averages more than 120 million daily active users and processes over a billion queries per day but it's also becoming clear what the political and cultural ramifications and the arguments in the debates around AI are and what they're going to be over the next few years those are the big questions who gets to lead us into this new age of AI technology what company is going to get there first and Achieve market dominance how those companies are going to be structured so that bad actors with bad incentives can't manipulate this technology for evil purposes what role the government should play in regulating all of this at the center of these important questions at least for right now are two men Sam ultman and Elon Musk and if you haven't been following they aren't exactly in alignment I don't trust open AI I don't trust Sam Alman and I and I don't think we want to have the most powerful AI in the world control by someone who is not trustworthy it'd be profoundly unamerican to use political power to the degree that Elon has it to hurt your competitors and Advantage your own businesses they started off as friends and business partners in fact Sam and Elon co-founded openai the company that makes chat GPT in 2015 but over the years Elon Musk grew increasingly frustrated with open AI until he finally resigned from the board in 2018 the feud between ultman and musk escalated this past year when Elon sued Sam and open aai on multiple occasions to try and prevent open AI from launching a for-profit arm of the business a structure that Elon claims is not only never supposed to happen in open AI he likes to remind people that a nonprofit transparent company should not become a closed for-profit one but he argues that changing its structure in this way might even be illegal now on the one hand this is a very complex disagreement to understand every single detail of it you probably need a law degree and special expertise in American tax law neither of which I happen to have but you don't need any special degree or specialization to understand that at its heart the feud is about something much bigger and more existential than open ai's business model although that's extremely important and something we discuss today what this is really about I think foundationally is a fight over who will ultimately control this technology and technology that some say if used incorrectly could very well make human beings obsolete so the stakes are low here to tell his side of the story is Sam ultman we talk about where AI is headed why he thinks super intelligence in other words the moment where AI surpasses human capabilities is closer than ever we talk about the Perils of AI bias and censorship why he donated a million dollars to Trump's inaugural fund as a person who had long opposed Trump what happens if America loses the AI race to a foreign power like China and of course what went wrong and is going wrong between him and the richest man on earth we'll be right [Music] back Sam Alman thank you so much for coming on honestly thank you the last time we spoke and I know you've given a zillion interviews since then but it was in April of 2023 and it feels like a world away Chachi PT had just launched and people were just at the very beginning of trying to figure out like in the abstract what this technology was and how it might transform their everyday lives and now sitting here in December of 2024 chat GPT is a household name so is open Ai and of course some of your competitors are too like perplexity and Gemini and Claude average Americans are using these tools every day everything from math tutoring to debugging code to dra emails and it's very very good at doing that tell me about how chat GPT and I guess AI technology more broadly has changed since we last spoke a year and a half ago and whether or not it's where you expected it to be today or further along so I think there's two different things we talk about one is how much the technology itself has changed and that has gotten way better I if you think about the AI we were excited about back in April of 2023 it was so primitive relative to what we have now and the things that the technolog is capable of are pretty mind-blowing to me but even more than that the the rate at which it will continue to get better over the next year and if we came back in another 18 months and talked about what it can do I think it'll feel like as big or maybe even bigger as a gap from April 2023 to December of 2024 um as projecting that same amount of time I guess that's more like 20 months going forward the other thing that's happened is it's really integrated into society like back then it was still a curiosity something may people had heard of people really use it a lot for like a lot of their work their personal lives their it's I've never seen a technology become widely adopted this fast not just as something people like dabble with but something that people like really use in all the ways you were talking about so that that part of the adoption curve happened much more quickly than I thought I expected the technology to happen quickly give me a sense of like how are you using the tool that you have helped create in your daily life like the way that most people know we're using it Tyler Cowen and lots of people who are like passionate early adopters it almost seems to have like replaced Google for them and it's just like a much much deeper Google is that how for I use it in all sorts of ways but the newest one um a few months ago we reled we released search integration and now chat GPT can search the internet for kind of real-time information and of everything we've ever shipped that was the one that felt like it doubled my usage all at once and since then I mean I must have still used Google for something but I can't remember what it is wow and I switched chat gbt to be my default search in Chrome uh and I have not looked back the degree to which that behavior changed in me for something that was really deeply ingrained and now the fact that like when I remember the way that I used to search um feels like kind of oh man that was like a pre- iPhone kind of equivalent that's the sort of like level of shift that I feel about it um that's that's been the most surprising change to me in the last few months is that I don't I do all my searching now inside of chat what do you call it do you call it searching or is there a verb in the way that Googling is a verb I still call it search I mean I just like people other people say like I chatted it or I chat whatever people say I chatted it a lot like people seem to just only call it chat but I I I would say I just use search Sam in September so just a few months ago you published this Manifesto on your website predicting the emergence of super intelligence in the next few years or as you put it and memorably in the next few thousand days explain to us what super intelligence is tell us how we'll know if it's actually here and how it stands to change people's lives over the next decades one one thing that I use as a sort of my attempt at my own mental framework for it is the rate of scientific progress um if the rate of scientific progress that's happening in the world as a whole tripled maybe even like 10x you know the discoveries that we used to expect to take 10 years and the technological progress that we used to expect to take 10 years if that happened every year and then we compounded on that the next one and the next one and the next one that me would feel like super intelligence had arrived and it would I think in many ways change the way that Society the economy work it what it won't change and I think a lot of the sort of AI commentators get this wrong is it won't change like the Deep fundamental human drives uh and so in that sense you know we've been through many technological revolutions before things that we tend to care about and uh what what drive all of us I think change very little or maybe not at all through most of those but the world in which we exist will change a lot okay well Sam one of the reasons we wanted to have this conversation with you today is not just because we want to hear about the ways that AI is going to transform the way that we live and work but because you're in a very public battle right now with your original open AI co-founder Elon Musk and I think it's safe to say that most listeners of this show will like vaguely know that there's a conflict between Elon Musk having to do with this one of his companies one of his many companies but there's certainly not following the nitty-gritty details of of the various lawsuits and of and of the conflict more generally so I want to try and summarize it for you in the most Fair way that I can and then you'll tell me if I've gotten it wrong or or where I've where I've overstepped so open AI begins in 2015 and it starts as a nonprofit and in a blog post introducing open AI to the world in December of that year you wrote this open aai is a nonprofit artificial intelligence research company our goal is to advance digital intelligence in the way that is most likely to benefit Humanity as a whole unconstrained by a need to generate Financial return since our research is free from Financial Obligations we can better focus on a positive human impact and this was a huge aspect of the brand then fast forward four years in 2019 open AI moves to what it called a hybrid model with a for-profit arm that got a billion doll investment from Microsoft in that year since then Microsoft has ped something like $1 13 billion it might be a higher number more into the company and Elon was one of the co-founders as I mention since the beginning but his relationship with the company soured over time because he disagreed with the shift that I just described the shift from this nonprofit model to a hybrid model and he eventually leaves the company and steps down from the board and that takes us to this year in which Elon has sued you and openai on several different occasions so far this year and he has gone given many interviews and posted countless amount of tweets or X's or whatever we're supposed to call them about this conflict all of the lawsuits claim that you were in some kind of contract violation by putting profits ahead of the public good in the move to advance Ai and then last month and this is the most recent of vment Elon asked the district judge in California to block open AI from converting to this for-profit structure okay that was a mouthful did I summarize it properly and is there anything crucial that I left out or you summarize it properly but uh I mean it was Elon that most wanted to convert not not even convert it was Elon that Most Wanted open AI to be a for profit at one point and had made a bunch of proposals that would have also things like opening I being part of Tesla but mostly just create a new for-profit that he was going to be in control of and you know so other than that I think a lot of the summary there is is correct I have a bunch of thoughts and opinions on it but as a statement of facts that was otherwise mostly correct give us like the 10,000 foot version what is the fundamental conflict between Elon Musk and his various allies meta being one of them and you guys like what what is the disagreement fundamentally about look I'm I don't live inside elon's head so this is a little bit of of speculation uh Elon definitely did a lot to help open eye in the early days and in spite of all of this I'm very grateful and I think he's just a sort of legendary entrepreneur um he's also clearly a bully and he's also someone who clearly likes to get in fights you know right now it's me it's been Bezos Gates Zuckerberg lots of other people and I think fundamentally this is about opening eyes doing really well Elon cares about doing really well Elon started uh and now runs a very direct competitor that's trying to do exactly what open AI does uh and I'll point out is a structure uh you know as like a public benefit Corp and I heard Elon has majority ownership and control and seems like a reasonable thing he would do I think a lot of the press has been misreported we're not like the non even if we go through with of the any of the conversion ideas or Evolution ideas we're talking about it's like the nonprofit goes away the nonprofit doesn't like stop being nonprofit becomes a for-profit we've talked publicly about maybe we evolve our current LLC into a PBC but anything we do would strengthen the nonprofit the nonprofit would continue to exist would continue to serve hopefully better serve the same purpose and the overall mission of the company that you talked about which is develop this incredible technology do it in a way that we think is maximally beneficial to humans and get it out into the world for people we keep doing that I'm incredibly proud of our track record on doing that so far people as you were saying earlier use chat GPT and love it there's an incredible free tier of chat GPT uh we lose money on it it's not ad supported or anything we just want to put AI in people's hands we continue to want to deploy this technology uh so that people co-evolve with it understand it that the world is going through this process it's going through right now of contending with AI and eventually AGI and thinking how it's going to go and everything we're doing I believe Elon would be happy about if he Wen in control of the company he left when he thought we were like on a trajectory to certainly fail um he and also wouldn't do something where he had like total control over open AI but I think it's like a little bit of a sideshow and the right thing for us to do is just keep doing incredible research keep shipping products people love and and most importantly like keep pursuing this mission of AGI to benefit people and getting that out into the world for someone who's just sort of tuning into this topic why is it important Sam that open AI has a for-profit arm or converts in the way that you've been talking about why why is that essential to your growth when we started openi we thought it's hard to go back and remember how different things were in 2015 um that was before language models and chatbots it was way before chat gbt we were doing research and Publishing papers and working on AIS that could play video games and control Rob about of hands and things like that and we we were supposed to get a billion dollars but ended up not we thought with a billion dollars we could make substantial progress towards what we were trying to do as we learned more and got into the scaling language model world we realized that it was not going to cost 1 billion or even 10 but like 100 billion plus and we couldn't do that as a nonprofit so that was the fundamental reason for it okay so like it's Bas and maybe another way to say it is like it's absolutely essential for the computational power to create okay every other effort pursuing AI has realized this and has set up in some way where they can sort of Access Capital markets you've said a lot of different things about Elon in recent days you gave this interview at dealbook where Andrew Ros sorin is sort of asking you how you feel about the conflict and you say sad and you also say that you think elon's companies are awesome and then he asked you you know do you think he's going to use his Newfound political influence to kind of punish you or punish open AI or punish his competitors and you said in that interview that you thought he was do the right thing how do you square that with what you just told me which is that elon's a bully bullies don't typically do the right thing I think they can totally be like I think I think there are people who will really be a jerk on Twitter who will still not like abuse the system of a country they're now in a sort of extremely influential political role for that seems completely different to me until now much of this battle you know for those of us who are like perpetually online and perpetually on Twitter we have been following the conflict via like tweets lobed sub tweets it's all sort of been playing out in real time on Twitter for us to watch open AI though has sort of been in like response mode sometimes or mostly kind of ignoring everything that's sort of how i' characterize it that changed a few days ago when you guys published this very very long memo on open it's like a blog post on open ai's website people should go and read it again we'll put it in the show notes and it's sort of like complete it's it's like a timeline going back to 2015 proving from your perspective that you know via emails and screenshots of texts and explanations of those screenshots and those texts that Elon wanted open aai to or or Elon rather was open to open AI being a for-profit going all the way back then I read all 29 Pages for those who don't want to do that they could go to chat GPT and ask them to summ asked chat to summarize it here's how chat gbt summarized it this article details the rift between Elon Musk and open AI leadership par particularly Sam Alman stemming from musk's dissatisfaction with open AI shift from a nonprofit to a hybrid for-profit model this Feud is crucial chat told me because it underscores the broader ethical dilemma of how AI should be developed and controlled whether it should prioritize public good or corporate profit especially as powerful AI Technologies become increasingly influential in society in the economy I thought that was pretty good what do you think no but on your general point we you are right that we do not sit there and like throw tomatoes back and forth on Twitter um the reason for this one was we had to make a legal filin and we wanted to provide some context we published about this once before also when we had to make a legal filing I've lost track of how many times that Elon has sued us I think it's like four you know withdraws changes goes for this preliminary injunction whatever our job is to build AGI in a way that benefits humanity and figure out how to safely and broadly distribute it um our job is not to engage in like a Twitter fight with Elon but when we have to respond to Illegal filing we will Pro we will and sometimes we'll provide context I think we've only done this twice in the early days of open aai the brand like the way I encountered the brand of it was transparency and nonprofit like those were the things that it over and over emphasized and the reason you said that you couldn't take any equity and the reason you took such a small salary is because you said I you know I don't want to be conflicted I want to always be motivated to do the thing that's best for Humanity the day after open AI launched in December in 2015 you described it to Vanity Fair as a nonprofit company to save the world from a dystopian future you also said that trying to make open AI a for-profit would lead to quote misaligned incentives that would be suboptimal to the world as a whole I guess I want to ask like do you still agree with that but simply you've had to adapt to the reality which is that developing these models takes billions and billions and billions of dollars two things one I I think I was like a little bit wrong about that um and I have been although I have had concerns um I have been impressed by how much not just us but the other AI Labs even though they have this like wild sort of Market or economic incentive have really been focused on developing safe models I think there's many factors that went into that we did get a little lucky on the direction the technology went but also if you deploy these models in a way that is harmful to people you would like very quickly I believe lose your license to operate if it was an obvious one now there are subtle things that can go wrong like I think social media is an example of a place where maybe the Harms W so obvious at the time and then there was an emergent property at scale and you could imagine something happening but the incentive problem has been better than I thought at the time and I will cheerfully say I was like a little bit naive about how the world Works 10 years ago and I feel better now naive how oh the the the pressure the societal pressure on big companies and the sort of the power of researchers to push their companies to do the right thing even in even in the face of this gigantic profit motive have been pretty good but there is something that I don't feel naive about that I felt at the time too which is it continues to be fairly crazy to me that this is happening in the hands of a small number of private companies to me this feels like the Manhattan Project are the Apollo program of our time and those were not done by private companies and I think is like a mark of a well-functioning society do you think that we need a Manhattan Project here I think the companies are going to do the right thing and it's going to go well and I I don't think government effort in this current world would work at all I don't think it'd be good if it did honestly I just I I wish we were in a world where I said this is you know where I felt like that was the way it should and was happening meta right now is also siding with Elon a few days ago meta asked California's AG to block open AI from becoming a for-profit this is what they said in their letter open ai's conduct could have seismic implications for Silicon Valley if open ai's new business model is valid nonprofit investors would get the same for-profit upside as those invest in the conventional way in for-profit companies while also benefiting from benefiting from the tax writeoffs bestowed by the government this Echoes what musk said last year when he said I'm confused as to how a nonprofit which I donated to somehow became a market cap for profit in other words if this is legal like why isn't everyone doing this I don't know why I Med accept that letter but I do know they know that's not how it works I I know that part's in bad faith if if you in any of these worlds our nonprofit will keep going and the people that invest in the nonprofit uh don't like you don't get to have a benefit from a nonprofit donation ACW to a sort of for-profit equity of course and and they know that too you can imagine lots of other reasons that meta might have sent this letter you can imagine they wanted I mean you can imagine they wanted to Curry favor with Elon you can imagine that they felt like it would help them compete with us um you could imagine that they were like annoyed with us for a perceived anti-open Source stance which I don't think is accurate or something that I feel I don't know you should ask them what the reason was for the civilian who's hearing how does a nonprofit become a for-profit what's the answer it doesn't like the nonprofit stays as the nonprofit I believe that the opening eye nonprofit is on a trajectory I hope if we do well to be the largest and most impactful nonprofit of all time that nonprofit doesn't become anything else like many other things this our world our ecosystem can have a for-profit business also but that doesn't the nonprofit does not convert the nonprofit does not go anywhere the nonprofit does not stop doing nonprofit things at the end of the day Sam who is going to profit most from the success of open AI everyone I'll tell you what I hope everyone gives their analogy for um What technological Revolution this is most like you know it's the Industrial Revolution it's like electricity it's like the web the thing I hope for is that it's like the transistor we discovered a new important fundamental physical law whatever you want to call it um we did a bunch of research so did others and it it will seep into all aspects of the economy products everything and you and I today uh are using many devices with transistors in them to make this podcast possible computer has some your microphone has some the all of the internet equipment between and me has a lot but we don't sit here and think about transistors and the transistor company does not sit here and make all of the the money it is this this new Incredible scientific discovery that's seeped into everything we do and everybody made a lot of money that's what I hope AI will be like and I think there's many reasons why it's the best analogy will you have Equity or do you have Equity or what kind of stake do you have in this new capped for profit well so we haven't formed a new entity yet um we have obviously considered uh forming a new entity or maybe converting our existing ALC into one is more accurate um I have a tiny sliver of equity from a old YC fund um I used to have some via sequa fund but that one turned out to be easier to like sell and not keep the position in um so I have a very small amount that was like quite insignificant to me in terms of what I will or won't have going forward I don't I know it's not like there's no current plan or promise for me to get anything I I will and I if I got anything it would not be there were like outlandish rumors about some number that would not happen do you get why people are fixated on that for sure as I've said many times before if I could go back in time I would have taken Equity I think again I understand more about why my earlier misgivings were misplaced I also get that it's weird for me to take it now after not earlier on the other hand I would love to never have to answer this question again and be like normal company I run it I've got some Equity investors don't have to worry that I'm like misaligned there does the whole like a of Suspicion of not having any is one of the decisions I regret the most of opening eye structure things but I understand why people are fixated on it uh that makes sense if you could go back in time how would you have done this from the beginning like let's wind back the clock to 2015 if an oracle had said to me on what was it November of 2015 before we set out number one you're going to need 100 plus billion dollars number two even though you have no idea today how you're going to ever productize this and you think of yourself as a research lab eventually you're going to become a company that does have way to productize it and business model it so you can explain to investors why they're not just funding a research lab um and number three that the incentives of people working on this are going to be more naturally kept in check because it's not going to be what I and many others thought at the time of like one effort that is way far ahead of everyone else but something more like the transistor that seeps out and so there will be better equilibrium Dynamics if an oracle had told me all three of those things that turned out to be true I would say great let's be a public benefit Corp how essential was Elon to getting open AI off the ground like if if the Oracle also told you about this fight that would ensue with someone that you regarded as your close friend would you have said you know don't need him can do it myself no he was really helpful I'm super appreciative I think it was the first time I ever saw Elon Musk was on stage at a conference you were interviewing him you guys had a wonderful Dynamic you seem like you were really good friends he has said some really harsh things about you he's compared you to Littlefinger in the Game of Thrones most devastatingly said I don't trust him and I don't want the most powerful AI in the world to be controlled by someone who isn't trustworthy why is he saying that I think it's because he wants the most powerful a in the world to be controlled by him and again I've seen elon's attacks to many other people many friends of mine you know everyone gets their period of time in his Spotlight but this all seems like standard behavior from him I'm trying to put myself in a position of a former friend a former co-founder of mine saying those kinds of things about me you you seem relatively calm about it no I'm upset by it for sure I I was talking to someone recently who uh I did think of as close and they said like Elon doesn't have any friends Elon doesn't do peers Elon doesn't do friends that was sort of a sad moment for me um because I do think of him as a friend but I I don't know I can look at this like somewhat dispassionately I remember what it was like when he said opening I has has a 0% chance of success and you know you guys are idiots and I'm ping funing and I'm going to do my own thing there were moments since then where it felt like he kind of wanted to reconcile and figure out a way to work together and I remember moments where he's just like you know off doing his thing on Twitter but if it were only towards me I think it'd be much more painful but you know I think you see who he is on Twitter and so I can like hold it somewhat impersonally and just be like this is about Elon this is not about me it still sucks um I've had a long time to get used to it I guess this recent blog post um the that that went up on open AI site said that Elon should quote be competing in the marketplace rather than in the courtroom and the cynical view of course is to say and you've alluded to this in this conversation that Elon who now owns an open AI competitor himself called xai is suing you not out of some concern over AI safety or anything else but really just to get in on the competition what do you say to that you know is this really is the cynical view true is this really just a fight to be the first to dominate the market or you should ask him I hope yeah I hope to I invited him on great you're not just known as one of the most important AI CEOs AI developers in the world you're also a very very well-known proponent of AI regulation and the cynical view here right is that in the very same way that you could cast dispersions on elon's motives you could look at the way that you have lobbied for AI regulations as a way to stifle competition and benefit your company obviously you've heard that argument before I I think too much regulation clearly has huge negative consequences in society right now and many places we have too much I mean Elon has also been a lot of proponent of calling for AI regulation as have as has the heads of most other large efforts when you step on an airplane you think you you know very high likelihood it's going to be a safe experience when you eat food in the US you don't think too much about food safety uh some regulation is clearly a good thing now I can imagine versions of AI regulation that are really problematic and would disadvantage smaller efforts and I think I think that would be a real mistake um but for some safety guard rails on the most powerful systems that should only affect the people at the frontier that only affect opening eye and a small handful of others I don't think we're at the level yet where these systems have huge safety impli implications but I don't think we're like wildely far away either but the argument that some of these startups are making startups like um there's an AI startup called hugging face which is an unbelievable name um the founder of ay company called stability AI they're basically saying what Sam and the other big guys the incumbents are trying to do open aai Google and apple basically asking government to kind of build a moat around you and stifle the competition through regulatory capture what do you say to those people and this is sort of like the the argument between big Tech and little Tech we can frame it in all kinds of ways what do you say to those people who are saying we want to get in on the competition the regulation that people like Sam and others at many other times are pushing for will hurt us and benefit them well if what they're saying is we're behind opening eyes so it doesn't matter and what we're calling for is only regulation at the frontier like only like only stuff that is new and untested but you know otherwise put out whatever open source model you want I don't think it's reasonable for them to make that argument I I I don't know I'm curious what you think if we if we do let's say we succeed and make a super intelligence you know we make this computer program that is smarter maybe more capable than all of humanity put together do you think there should be any regulation on that at all or just they just say none I definitely think first of all I don't even understand what we're talking about when we talk about super intelligence like you understand what that means and the implications of it in a way that I just don't um so that's number one and number two you know if if this technology is as powerful as people like you and Elon and so many others that are closer to it say that it is of course I think it should be regulated in some way how and when is obviously like the relevant question for sure how and when matters a lot but but uh I agree with that and and I could easily see it going really wrong recently Mark andrion was on this show and he talked to me about his perception of what the Biden Administration was trying to do around AI technology he came on and made the argument and told a story really that he experienced he says that the Biden Administration was trying to sort of completely control Ai and what they were aiming to do was to make it so closely regulated by the government that in his words there would only be sort of two or three big companies that they would work with and that they were trying to ultimately protect them from competition is that true do you know what he's referencing was open AI one of those companies I don't know what he's referencing I also will say very very clearly I think regulation that reduces competition for AI is very bad thing that's so openi was not one of those companies no I don't actually know what that's about but it's we've certainly as far as I know have never you weren't in a room ever with the Biden Administration other AI companies no I don't I don't I don't even think like the Biden Administration is competent enough to I mean we were in a room with them but never and other companies in the administration but never like here's our conspiracy theory we're going to make it only you few companies they can build Ai and then you have to do we say never anything like that what was your feeling in general about the Biden Administrations posture toward Ai and Tech more generally you just you just said like you didn't think they'd have the confidence to I think Gina Rondo was is fantastic you know every conversation I had with her I thought she kind of got it um overall I would say the administration was not that effective uh the things that I would most that I think should have been the administrations priorities and I hope will be the next administration's priorities are building out massive a infrastructure in the US having a supply chain in the US things like that when Mark was on I asked him to kind of Steelman the Biden administration's perspective or Steelman the perspective that this should be heavily regulated and he basically drew the analogy to the Manhattan Project and the development of the atomic bomb when the government felt that it needed to make sure that this new science and Innovation remained classified first of all do you think that that's a good analogy and if so if if it is as powerful as nuclear weapons wouldn't it make sense for this to be not open Ai and Gemini and Claud but rather a project of the federal government I think all the analogies are tough cuz they work in some ways and don't work in other ways like you can you can point to things that are similar to the nuclear era you can talk about like you know it takes enormous resources and huge amounts of energy to enrich uranium on one hand or to produce these models on the other um so you can find things like that that work and then the use of one of these models and the use of a new nulear weapon are like quite different things and sort of the geopolitical implications are also quite different things I think to Steelman the argument of people who say things like it's like nuclear weapons I think what they mean is that it's it's extremely expensive and ex has extreme geopolitical consequences we don't know exactly what those are or how to think about them but because we don't know exactly what they are shouldn't we have like a principle of letting the government decide I can imagine other governments at other times in history where we would have we should be very thrilled about that outcome I think putting the current United States government in charge of developing AGI faster and better than our competitors would not likely go well I think the the kind of the decline in state capacity in this country is not a new observation but a mournful one at the beginning of the nuclear age we had people in this country who functioned almost like Chief science officers right I'm thinking about people like vanav Bush who helped launch the Manhattan Project and came up with a National Science Foundation and kind of guided American policy for those first few like very crucial years of n of of nuclear energy does that person right now whether or not they're in DC or not does that person exist like if we wanted to have someone like that who sort of understood the technology had no Financial stake in it and could talk whether it's President Biden or Trump or whoever comes after him sort of the pros and cons not just of the development of AI here but the competition with China like does that does that person exist actually right now in America like could you be that person arguably I think the the willingness it's coming back a little bit but for a long time the willingness of the American public to be excited about future developments in science and technology has been gone I sort of think it went away with the nuclear weapons actually if I had to pick one moment in time there was sort of a you know a weird like few decade hangover before it there was the generational change when the bomb was dropped kind of got older and in power like I don't think America ever embraced the excitement and belief in science and technology driving the world forward to the same degree as as we used to you read these stories about what people like that used to do and how revered they were and how people believed that scientific technological progress more broadly was going to make the world better um that seems missing now and I don't think it's because we don't have an individual who could do that I think it's because the government doesn't want it and the public doesn't want it I mean what do you make of not just the political Vibe shift but the cultural Vibe shift that we've been experiencing since November 5th like if you made that argument to me 8 weeks ago i' would say yeah Sam's probably right now it feels like a different country there's a huge cultural Vibe shift and I think there's a very positive there's positive momentum in many ways I'm not sure that it exists for hey we think science is really important again and science is what's going to save us and you know solve all of our problems do you think that or do you think it's like that's the one area where I haven't felt it I just think that there's a shift in the direction of growth is a good thing technological progress is a good thing nihilism is feels like it's p and falling out of favor like I feel that change happening in a dramatic way now maybe it's because I spend a lot of time on X and like a lot of it's sort of like fomenting there and sort of Leaping from the online into the real world so you know if if you went and like talked to the average PhD student uptown at Colombia I don't think that they would have the same experience I do because everything's so vulcanized as I said earlier I think it is getting better even on S like I I strongly agree with you on the kind of General shift towards excitement about growth and success and having the country and the economy do well um I some I I do somewhat agree as I was saying earlier that I think even excitement about science is in a better place than it's naugher but I when you talk about those people who were like the scientific ambassadors of the country and who people like really listen to and were excited about and you know preach to a willing audience I'm still not sure I feel that I think there's that excitement for business but not for science well one of the companies that I feel excited about um perhaps it's controversial to say this but I just think the founders is one of the most interesting people in the country is Paul mer lucky and his company and um andal Industries and open AI recently entered into an agreement with andil to develop AI with military applications now previously open AI had had a Prohibition against using its technology for weapon now with the caveat of course that you're concentrating on defensive systems at the moment the sorts of things that could you know guard us against attacks like drone swarms perhaps like what's happening in New Jersey right now we don't have time to talk about that what made you change your mind fundamentally about integrating your company's technology um into even a defensive Weaponry system yeah so we have a set of principles that we established and we approved this one for some use cases that comply with those but I think if the leading United States efforts do not help defend the United States and our allies against our adversaries we're going to be in a very bad place and so we need to figure out how to do that a year and a half ago when we were talking part of our conversation was whether or not we were like where the AI arms race with China was I think now it's like well and definitively clear that we are very much in that arms race with China um and you know I think even people who worry about the power of AI in this country feel like well if it's a choice between us and China it's got to be us we got to win spell out for us Sam in your mind because I'm sure you're thinking about this all the time like what it looks like if China wins the AI arms race like what what happens to America what happens to the world whatever China wants and do you think the possibility of that happening is a real one them winning uh I mean we intend to work our hardest to make sure they don't how do we know if they are winning given how much they lie and also steal stuff from us this is the hard thing right we we know what they publicly release we don't know what they don't publicly release um we have a lot of signals and we have a intelligence system but it's my own stance on this is we have got to try to be cooperative and uh arms races are bad for everybody involved we've learned that lesson again and again throughout history but we need to be able to win if we need to I am hopeful that this can be a great moment for world peace and I believe that if there's ever a time for Humanity to come together this seems like a good candidate and I want us to get there but we can't be naive about that President Trump is talks a lot about you know peace through strength is your is the Sam Alman open AI version of peace through strength we have to crush get ahead and win on AI so it's not even a question that China could do whatever it wants not crushes we have to be ahead and then we have to be as willing to work together as possible and I think that is somewhat similar to peace through strength it's like if there's an arms race we'll win it but we don't want to meaning if there's an arms race we want to win but we don't want the arms race period yeah but well it's not even that it's more like if there's any path towards doing this as a collaborative effort we should but we have to be C we can't control what other entities do you mean collaborate with our enemies we collaborate with China yeah actually I'll say that directly I I I think we collaborate with people we don't get along with all the time in areas where it's in our strategic interest to do so and this is one where I think the interests of the world and certainly the mission of our company would dictate that if it is possible to be truly collaborative we should do that are we doing that right now with China on AI like you know more than I do I was going to say you might know more than I like that that will be a big question for the new Administration but that's not going to happen at the company to company level that's going to happen like the presidents at the two countries level if Trump called you tomorrow and said hey Sam I want to make you like aiar AI regulation Chief you can do whatever you want in this position what's the first thing that you would do what's the most important thing that the person in that position would do us infrastructure and supply chain add a little bit more for people that don't know what that means build our own chips here build enough energy to run data centers here change what it takes to build data centers here but be able to like build the very expensive complex supply chain very expensive infrastructure in the United States bias and censorship in AI is a enormous topic and one that we think a lot about here at the Free Press and you know the most obvious example of this the one that trended for days and everyone was laughing at that was when Gemini generated those images of like black George Washington and like a trans naazi and it was hilarious but in a way it was really serious because it felt like only the most sort of like exaggerated hyperbolic obvious example of a much much deeper endemic problem which is the bias that is baked into these Technologies both because of the people programming those Technologies and because of the information that they're sort of scraping online talk to us about how you're thinking about it at chat GPT because obviously the system that is closest to reality it seems to me will will win in the end of the day if if a chat gbt is giving me images of you know he's telling me George Washington was trans I'm like I'm not going to rely on this we don't do that so okay fine but you understand my point how do you think about the problem of bias and how you're solving for it I think there are two things that matter uh one is what flexibility a user has to get the system to behave the way they want and I think or we think there should be very wide bounds like you know there are some things like you don't want a system to tell you how to create nuclear weapons fine we can all agree on that but if you want a system to be pretty offensive and you ask it to be I think part of alignment is doing what its user asks for for Within These broad bounds that Society agrees on the second thing that really matters is what the defaults are so if you don't do any of that which most users don't and you ask whatever controversial question you want how should the system respond and we put a ton of work into both of those things um we also try to write up how the model should behave we call this the model spec such that you can tell if it's a bug or you disagree with us on some stance is it possible to build a like chat GPT or any other technology in this Lane that we can't even conceive of yet that doesn't have a political point of view isn't that inevitable I think no matter how neutral you try to write the thing it will either be useless because it will just say I can't answer that because there's politics and everything or it will have some sort of point of view which is why what we think we can do is write down what we intend for our default people can debate that if there's bugs in there we can look at the bugs if there's problems with how we defined it we can change what the definition is and retrain the system but yeah I don't think any system can be per no two people are ever going to agree that one system is perfectly unbiased but that's another reason why personalization matters so much do you believe that AI or chat GPT has a responsibility to fight pernicious ideas let me give you an example of what I mean like if you knew that by putting your thumb on the scale in the teeniest tiniest way you might be able to usher in a world where there's less racism less anti-Semitism less misogyny and maybe would even be invisible to people because you know they wouldn't know you know at a certain point as we've just talked about this is going to be you know I don't know if this was Mark or somebody else the control layer of all of our information how do you think about that actually here's one thing I've been thinking about recently as a principal like open AI has not adopted this at all but this has just been an idea that I think gets at what you're saying like let's say let's say we discover some new thing where it's like if you do this people learn way better if chat GPT responds always with the Socratic method or whatever students using it learn way better um but let's say user preferences are not to get the socratic message users just say like I just want you to answer my question then like how should we decide what to do there as the default behavior and one idea that I have increasingly been thinking about is what if we're always just really clear when we make a change to the spec and so you'll never have our thumb the scale hiding behind an algorithm which I think Twitter does all the time for example and all sorts of weird things there like We'll always tell you what the behav what the intended behavior is and if we make a change to it we'll explain why but if we do discover something like what you just said uh or like what I just use as an example and we say okay when people are using it for Education we are going to use the Socratic method um because it does seem to have this measurable effect and here's why we're doing it we can debate that publicly maybe we change our default if you convince us otherwise um um anyone can of course change that in their user preferences because the AI is like a tool for you and should do what you want but I think the thing that would be wrong is if we changed that and didn't reflected in the spec and didn't tell people we were changing it um you know I think the like black box of the Twitter algorithm for example it's like doesn't feel good to me Sam you've donated a million dollars to Trump's inauguration and it turned some heads because in the past you've called him a racist a misogynist and a conspiracy theorist among other things you've been a prolific donor to democratic candidates and causes over the years but now you say that Trump is going to lead us into the age of AI and you're eager to support his efforts to ensure America stays ahead is this a change of heart a political Evolution A vibe shift inside of you what's going on all of those things and also I I hope I mean like he's our president and I wish him every bit of success and anyway we can you know work to support this part of what he wants to do we want to do what's the vibe shift inside of you we know that there's one going on inside Silicon Valley and one going on in the culture how have you changed in the past few years I mean a ton of ways but one one is that I uh you know I've watched for the last maybe 10 12 years as I think things have gotten off track things have been good in some ways but I think gotten really off track in terms of how we think about the importance of growth uh and economic success and a focus on the right things in in the country and in the world more broadly and I think it got it got very off track and I'd say the vibe shift is a hope that as we're facing down one of these most most important moments in technological history um that can help drive a VI A vibe shift back to what I believe in very deeply which is that growth is the only good way forward do you think growth and the growth of open Ai and the growth of AI more generally is a patriotic Duty yes I I actually wrote something like I someone just sent this back to I wrote something more than 10 years ago about how growth like I think it's my very first blog post ever about how growth was the central ingredient to democracy working well and I I think the world got badly confused about that and I'm happy to see it re recognized I'm going to use my 30 seconds on a lightning round Sam lightning round what are the Drone things what are the flying objects flying over New Jersey right now I have no idea I'm really interested in this question do you know um no we're reporting on it a lot I find it interesting that various electeds are saying it's the Iran Mothership or China do you think Twitter has become better or worse since Elon Musk took control worse you're having a baby will you will you let your kid have an AI friend yes will you let them go on social media at some point will you let them have screen time yes what's your favorite sports car you love sports cars there's a lot of good ones I can't pick one what's your favorite of yours no I can't pick a single favorite I'm mcar f one favorite movie The Dark Knight do you have any normal Hobbies I like have dinner with my friends I go hiking I like you know exercise I just like sit around my friends doing dumb stuff I I don't know yeah it feels pretty normal you built a treehouse recently why did you do that why' you do that um it was Thanksgiving and uh we looking activity for like the adults and the kids that we all thought everybody was at our Ranch and we want an activity we all thought would be fun and was not just sitting around drinking all day and it was great would you box Logan Paul no will we enter World War II in 2025 I hope not what's your New Year's resolution I know what it does Sam Alman thank you so much for coming on honestly thank you"}], "I played Pokemon, but with 50+ New Types": [{"content": "some of you may have heard but a while back Jacob and their team created a little something called Pokemon too many types it's just a normal game of Pok\u00e9mon Emerald but you might notice some things are just a little bit different everyone has talked about how chaotically fun this game is and I finally found the time to play it myself let me tell you everyone was right this was the most insane backwards chaotic playthrough of Pok\u00e9mon I've had in quite some time Professor Birch is being mauled by a dog in the grass right outside my house for the millionth time and after digging around in his stuff I helped myself to a poo but not just any poo for you see this one is silly obviously I blast the Angi poo to Smither and with that have now earned the freedom to explore the world on my own yep it's Dawning on me that I have no idea how anything works anymore trainers had all these new Pokemon they're pulling out moves I've never seen before types don't line up the way I'm familiar with it genuinely felt like I was playing Pokemon for the very first time again weasel is water fluffy snam is ice Bean Machop is fighting stinky Papio give that thing a shower it reachs up that's super effective I suppose that Mak makes a lot of sense going into this game I decided not to look at the type chart at all which by the way looks like this I'm going to have to unfortunately use my brain and logic to figure out how to win every single battle until I'm the Champion which may be a bit trickier than you would even guess for example this impidimp he has now been turned into a dark gamer Uno reverse type naturally I figured gamer might work similarly to the stinky type and be weak to water that makes sense right all right look I'm I didn't make the game all right guys I'm just figuring out the riddles however let's see what gamer has to say against water type take a shower what it's got what so so here's the logic you got to think about not only is it a gamer type it is an UNO reverse gamer type oh oh yeah of course yeah as much as I might think I know how things would work you're going to witness that it doesn't always pan out the way it seems I scooped myself up a krabby which as you can see is no longer water type and entered the rustboro forest only to be challenged by a te-a grunt with a single snow runt I wasn't worried until I realized the hard way that despite its silliness poo still doesn't like grass and crabs don't mix well with ice wiping this early in a Pok\u00e9mon game is genuinely so humbling and I don't like it at all honestly I didn't even want to tell you that it happened in the first place but we run it back because this isn't a Nuzlocke and my little pets are perfectly healthy after being dunked on this time we sent the grunt hight tailing it back into the bushes despite feeling dumb and stupid and pathetic for wiping to a level 9 Team Aqua grunt with one Pok\u00e9mon I waltzed into the first gym with my chest out like I earned being there as most of you know Rox San is typically a rock type trainer but she's now studied the ways of the ancient type Pok\u00e9mon which I didn't think would be much different until she completely steamrolled me oh I got doggy but doggy isn't going to do too much here God damn it waste of a turn no well her leip is crazy sus ancient grass type how was I supposed to predict that one what even is that now aware of the threat she had in her back pocket it I was able to navigate the rematch with Krabby the fossil killer earning us our very first gym badge also Geodude is boring ugly Rock I just wanted to share that one with the room we took a nice boat ride over to duford where we got to meet lots of new friends in the local cave like clink the prime steel guys and sabaly the gamer who has a little bit of an Easter egg attached to it someone just pointed something out okay will you trade yeah will you move yep yep yep yep yep yep after saying hi and bye to Steven doing who knows what all alone with his cave rocks we headed off to challenge Broly I opened the gym doors and was immediately hit with a wall of pure disgusting stench trying not to physically Decay we crawled through the Gym's Maze and were met by stink Master Broly and his smelly ass team Krabby knocked his Mankey out cold but sock came out and started overwhelming my Pok\u00e9mon one after the other get so much knowledge oh my God like why wouldn't it be so you can imagine I am not happy that this stupid thing is sweeping my team right now luckily the Vulpix I caught earlier was able to tank its hits with its new fluffy type and together with our own little stinker of a stunky we were able to finish the fight I snatched that badge from Broly's greasy hand and basically swam to slate Port myself to wash the fumes off my clothes to dissociate that disgusting experience away I went and caught myself a new friend a ground friend to be specific as well as a little fluffy Stuffle we also got to kick out Team Aqua who was holding the poor slate Port museum lady hostage with a gun thank God they were only armed with a harmless water gun horsey because I do not want to know what dealing with an actual threatening gun type Pok\u00e9mon would even look like battling more trainers on the route up to mville got got me back into the mood to take on the third gym so I shoved Wall-E out of the way and headed up to Watson things started off normal but quickly spiraled out of control because after Sab ey beat up his Porygon Watson immediately brought out his Ace you know his famous star Pokemon whimsicott and this thing just starts spamming its new move called hype train yep Watson specializes in Prime type Pok\u00e9mon which would definitely be a funny little hint hint nudge nudge if I was streaming this on Twitch but alas here we are unfortunately one cannot gift Prime Subs to a YouTube channel sadly all you can do is click the Subscribe button isn't that just a shame well whether you clicked the button or not I got wrecked by this cotton ball hype train works the same as roll out where each consecutive use gets stronger and stronger and this whimsicott must have had some sort of Aimbot on because it didn't miss a single one it literally demolished my team the second it stepped onto the battlefield I was also a bit unprepared and forgot to level everyone up so let's just forget that fight even happened like it wasn't even fair so it doesn't count round two was where the real action was this battle was much more neck and neck knowing what Watson's tactics were going into the fight I was able to play around it much more easily jayen doesn't fall for the same charade more than one time I'm like an AI learning machine all right I never lose the same fight TW I I don't stop no don't look at that with Watson defeated I was now one step closer to becoming the strongest trainer in all of this Topsy Turvy hoen I caught myself a hone Edge swablu and Skarmory while scaling the mountain and even had time to smack the Team Magma nerds around while heading to Lava Ridge to avoid another embarrassing underleveled situation I got everyone up to Flannery's level cap for a nice clean fair fight that's like I think the thing that is hardest for me wow oh God reading comprehension I think that's what I've decided I'm my god oh wow my crabs buddy holy what is the downside of this Mystic fire I'm just so weak to Fire and warm hugs do I have to wipe to every gym plannery she's mean I don't know what that Ninetails is eating but that thing is not normal after changing absolutely nothing about my strategy and just running back into the gym directly from the Pokemon Center I demanded a rematch like her team of fluffy Pokemon aren't that tough I just wasn't ready to be paralyzed and incinerated the whole fight by her puppies and kitties and that roed up monster of a Ninetails this time Kingler and sand slash were able to double team Flannery's fluffy team with much more ease earning us the heat badge with half the gyms now defeated it was finally time to deal with some long overdue Family Matters it's well known that our father Norman abandoned our home to be a gym leader in pelberg leaving me and mom back at the house alone little root has like five people living in it and one of them's a huge nerd that's like a social death sentence for a growing child and I am filled with a burning desire for justice after all this time all those days of having nothing to do but watch Birch go into the same stupid patch of grass and get mauled by the same stupid dog over and over and over I get to walk into Norman's gym and challenge him as an equal whatever reasoning he had to leave us is going to be revealed in our battle and it better be good good I walked through the doors the air was weird and tense each trainer I battled looked at me like they already knew exactly who I was and shifted their glance away out of embarrassment shame what is going on here not even the helpful gym guy had anything to say to me he wouldn't even tell me the type theme of the gym I burst through those final doors to find Norman looking out his windows at the horizon he's been waiting for me all right you ready to fight Norman and his very fun and normal gym have you not picked up on it yet I haven't picked up on anything he say just keep going what is he going to do what What Pokemon do do he have flil okay I don't get it I don't what is this riddle I'm just watching glump is it because he's only using attacks that are acute and family like no lony he can't be a fluffy trainer that was what flanner was cuddle slam no fluffy furry he said is he a furry trainer yes Norman can't a man have hobbies nor look there's nothing wrong with being a furry many of my friends are furries but to abandon your family to selfishly pursue your passion for lucariosklaw board if I was ever going to defeat a divorced man and his beloved furry I switched around some of my Pok\u00e9mon headed back in and again began the onslaught of devastating glumps and cuddle slams weaving through paralyzing warm hugs and even the occasional cute charm eventually it came down to our last Pokemon uh okay slash oh oh no sh sneak please you have a chance here please yes now that my furry father has been vanquished I was filled with an overwhelming energy and drive it was like I unlocked a version of myself that was capable of defeating anyone and everything in front of me I beat up all of Team Aqua that was infiltrating the weather Institute cleared Winona's song and dance gym without breaking a sweat Maxi stole the blue orb to awaken Groudon like an idiot so I had to teach him a lesson again you're going to use the wrong orb idiot why do you think the blue orb would be good for Groudon he hates Blue Team Aqua rats did the exact same thing so I had to do the exact same thing to them I was on a roll it felt like I was back in my element finally familiar with the types and was absolutely thriving we pulled up to tan Liza's gym ready to let nothing slow us down except it looks like they specialize in a type I hadn't seen up until this point space well time to slow down since I didn't have anything that could really handle space Pok\u00e9mon I went and caught myself a Balon because everyone knows ohot type is good against space I mean come on as well as prepared sand slash and stunky for the fight I wasn't too confident but made my way to the gym leader room to find clay what where's ton Liza aren't you supposed to be in you Nova I wasn't anticipating this but I guess I should just start expecting the unexpected from this game it's a double okay it's still a double battle 41 oh we lose we we definitely [Music] lose gun T this is in space wait what the is going on here I don't know what's good against gun how is song sure we'll go do that shoot what the he just shot my my my Goyle he shot my guy again that's so much damage for not very effective we'll do do that oh he's dead he's so dead bare arms oh my God what is this dude this guy's fighting me with the Constitution so this isn't actually a space type gym like I was originally led to believe this is in fact a gun gy and I am being blasted to Kingdom Come by Mr red white and blue over here I failed to expect this level of unexpected but how could I we switched around my team again this time for a high noon dual battle headed back in but honestly it wasn't looking that great on attempt to of fighting a man with assault weapons and air cutter again no no take it take it boy Bo take it stop how can you do this there's nothing I can do about it there's nothing this fight sucks [Music] balls whatever I genuinely think this clay fight was the most out of left field chaotic flabbergasting Pokemon battle I've gone through in my entire career as a Pok\u00e9mon player I don't think anything Pok\u00e9mon could create would ever elicit the kind of shock and despair I felt fighting clay the drift Veil gym leader in my Pokemon Emerald game okay I don't think this is enough yet I just oh he went again sorry that's so lucky oh is this it yes clay you you rooting tooting fing  I'm not talking to you I'm not talking to you get me out of here I basically crawl out the gym with my badge like I just got out of a fight with Wy coyote because I basically did and now I got to go deal with whatever commotion Team Magma is creating in the space station withered and tired I walk up the stairs to Maxi and Steven arguing about God knows what at this point girls girls I'm going to give it to you straight I'm having a bit of a day we all know we're just going to Hash this out in a battle so let's just cut the chitchat Steven get your ass over here and after all of what I just went through you guys are not going to believe what Steven put me through in that cursed Space Center yeah so the first fight I went into it unprepared because I didn't know the team Maxi and friend were packing so that was a bit of an unfortunate disadvantage it's all right we run it back but this is where things go drastically downhill hyper voice Steven you're just leaving me in the The Trenches why even try at this point Thunder oh my fuing God Steven you idiot why do they only go against me why do they only go against me Steven Steven you are theing Elite 4 champion of ho don't use Thunder on Agron it doesn't it in no world is it ever good I swear to God [Music] Stephen you're actually you're actually joking Stephen you're my guy I've always had your back I've been team Steven since day one but what in the world was that this is not the play style of the region Champion buddy get up don't talk to me I need some hair to blow off some much needed steam I infiltrated Team Aqua Hideout because Archie apparently didn't get the memo of my threat of a knuckle sandwich if he ever tried to use red on Blue Kyogre knuckle sandwiches delivered but unfortunately too late Kyogre is already halfway across the Pacific Ocean to go drown Groudon can any of you adults do anything right hello Earth to Archie's brain it's Hollow I scaled the sky Tower to wake up Rayquaza and rat on the siblings duking it out in the middle of suolis off he flew and after a bit of non- gentle parenting the two slink back in their rooms in shame yay Land and Sea across the globe are safe once again if I catch your asses playing with these fing orbs again I swear even though it was the eighth and final gym before taking on the Elite 4 Juan wasn't an issue for some competitors he could have been difficult because he actually ran a full-on crab t team very intimidating but I've also got a crab and my crab is stronger than his crabs so we won after all I've gone through this was the hardest I've had to claw my way to the Elite 4 in a long time but that just means Victory is going to taste oh so much sweeter I entered Victory Road anticipating an easy dungeon clear but from the Shadows emerged Wall-E I've beaten you before I can be you get holy mother okay maybe that one was a bit of a fluke I wasn't ready this time will turns out Wally's tough I didn't expect it but he really is this made me realize I'm not prepared for everything with my current roster of Pokemon and if I'm going to become the champion I need to make some big changes I've been swapping around my team pretty casually over the course of my adventure but after locking in I decided on this as my final roster primarina Kingler AIS slash beware and two new members of darmanitan the Angi fire monkey and duraladon the you guessed it gun look I've seen the power of being able to pull up with a gun I think I'm going to need that in the fights that are ahead of me with these changes I was able to rematch Wall-E and take home the win this time this is it these guys are my team and they're the ones I'm going to become the best with while doing some final training before the Elite 4 there was just a little tiny unexpected hiccup along the way what was that oh my Freddy you would never he would never have to teach it too I'll get rid of earthquake for a bite of 87 five nights what what what the it's Parish song Oh My Freddy what am I turning into you're not like this Freddy for those of you who are unaware five nights is basically an unhinged version of Parish song where after five turns both Pokemon on the battlefield are hit by a 250 base Power Attack and Bite of 87 is straight up a brand new on hit KO move where if you get bit you die instantly but there's only a 30% chance of it actually connecting highrisk High reward Type move equipped with a plethora of new threatening moves and mons my team was now prepared for the final Gauntlet I flashed my badges to the guards and took a step into Sydney's domain I like that look you're giving me let's have a good match as someone once taught me you got to be prepared for the gun Phoebe was up next with her gamer Pok\u00e9mon and even though I was able to take down her team with my loaded deroon and shower giving primarina the highlight of the fight was when she brought out a Freddy Fazbear of her own huge win beware oh it's Freddy V Freddy here it's got to be Freddy V Freddy and you know what Freddy [Music] does oh he got the first one he got it that's mying Freddy right there doesn't miss he hits that frontal lobe everying time I walked into glacia room Larry the third fight was against Larry who normally is a gym leader and Elite for member in Pala but looks like he also works for hoen jeez man three full-time jobs that cannot be legal you got to talk to HR he led with a Golem that only knew stealth rocks and explosion which is chaotically hilarious but right after that he sent out a W of a slacking that took out my duraludon primarina and AIS slash finally Kingler was able to knock it out after pulling off a few Dragon dances sorry crab Raves since I basically had a crab bazooka on my hands Kingler just swept him from there after the Larry jump scare I didn't even know what to expect for the final member it could genuinely be anyone Cynthia red clay butt again what no you're joking he's in theing vent no wait I F facing the wrong way I didn't know that's what happened when you click the VIN from that side I'll be honest Not only was I flashed for simply going about my E4 challenge but Elite 4 sussy Baka and his sus team went dummy mode on me I'm scared so I shoot oh no wait this isn't good this thing's got to go no he's faster he's dead what I thought no where is he oh nice crit no they all vent stop venting well now I guess you get another crab Rave I do no he he avoided it no Krabby I'm sorry a I was not anticipating a battle where their whole strategy was to hide in vents scattered all over the room and when they popped out they hit like trucks not even my five nights attack could hit them in the vents which meant I was just getting my own mons attacked by animatronics admittedly we were vaporized but shut up there's nothing in the rules about trying again it's not a Nuzlocke I refused to call it there because I will not let my legacy end with see that girl she was on her way to become the next hoen Champion but she couldn't get past Elite Four member sashy Boda poor thing the Second Battle went much better because not only was I able to play around all the venting but halfway through the fight Kingler finally learned a better crab move this whole time I was being forced to use 50 base power vice grip but now that we've got 100 base power crab hammer it was lights out for these imposters and with that it was time all that stood left in front of me was the champion Wallace which makes sense because Steven was definitely fired after that Maxi fight out came his first Pok\u00e9mon which was a simple why not now let me explain something here because this thing is far more threatening than you could ever believe this game has introduced so many new types into the world of Pok\u00e9mon many of them are extremely powerful but there's one type that Reigns amongst them all the absolute worst and that's baby type it's pretty self-explanatory which Pok\u00e9mon fall under the baby category Igglybuff cleffa Budu bonley literally the baby Pok\u00e9mon you know what this type is weak to literally like all of them think about it if you hit a baby with anything they're going to be taking damage it's a baby so now you're probably thinking okay so why does the final boss of hoen lead with why not the baby type it's also Uno reverse that's right every single type Effectiveness against this thing is now flipped it's a defensive monster just waiting to counter and mirror coat your whole team to death for simply breathing on it too hard I was genuinely so so scared of this thing I dragon tailed it out of there that's a nightmare for another time his lattos got dragged out which is crazy to say I was relieved to see compared to that masochist of a why not but of course it's still a lattos so it takes out my duraldon with a single Thunderbolt Kingler comes out to finish it off so next came syali which I'll just let you watch how this one played out I don't think I get past this thing what even is this type type Emerald he's a type type type type I don't even know what that is but it's got to be weak to Monkey so type type is more of a modifier than anything it just doubles all type interactions oh  so okay so I die here what so here's a little glitch sometimes uh let's say the max damage you can deal is 999 if the game register doing more it overflows into one HP so this is Dar manit tan died and came back to life basically pretty much yeah well that gives us enough time to Banana Blitz again yeah we take those all right nope I don't want to hear it we take those the next chunk of the battle plays out pretty normally Wallace did have some pretty scary Pok\u00e9mon including Steven's Metagross he very likely lost custody of until I was face to face with this stupid little bomb again it tried everything it could to bait my AIS slash into attacking it but we stayed patient I knew I had a single shot at this so AIS slash charged up as many swords dances as it could muster before slicing that thing to the Moon a final Beast entered the ring a through Abomination called Sans Shedinja this is the only Pokemon in the decks with the Sands type and it can only be touched by a very small selection of types A tried and true nightmare unfortunately for you Wallace I've played the genocide route of undertale I know exactly what it's going to take to bury this thing for good and he's right here it's a beautiful day Sans shinja but insects like you should be burning in [Music] hell I did it the strongest Pok\u00e9mon trainer in this Wacky World of hoen is me it took so much more than I thought it would to be here way more humiliating wipes than I'd like to admit I lost a lot of battles but shut up you try playing it then with the power of friendship logic determination and maybe just a little bit of love we've beaten Pokemon to many types and with that we're pretty much wrapping up on 2024 this was a big Jaden year for many personal and content related reasons and I just wanted to say thank you guys as always for watching after all this time I'm forever grateful for my team and friends who are always supporting me and I'm looking forward to working even harder in 2025 I still can't believe I really just love making videos this much even after 10 years I hope that you'll continue to support me no matter what projects I'm working on next year and I promise to do my absolute best until next time bye-bye"}], "25. Health Economics": [{"content": "[SQUEAKING] [RUSTLING] [CLICKING] JONATHAN GRUBER: So\ntoday, we're going to have sort of a\ndifferent kind of class since it's the last class. Today, I'm going to talk\nabout essentially how we bring to bear the set of\nissues we've talked about this semester to a\nreal-world topic, and actually, how it plays\nout in policy and practice. And I'll draw on some\nof my own experience, having applied the kind\nof tools we learned in 14.01 to the field of health\ncare economics for 25 years, and how that has led\nme to be able to help in the development of health\ncare policy in the US, and talk about sort of where\nhealth care policy stands at this point. So let's get a little\nbit of background about health care in the US. Basically, when we're talking\nabout health care in the US, we have to recognize that\nthe US spends, by far, the most money on health\ncare of any developed nation in the world. We spend about 17 and 1/2%\nof our gross domestic product on health care. That amounts to almost $10,000\nper man, woman, and child-- every man, woman,\nand child in America. That dwarfs the\nrest of the world. The typical European nation\nspends about 2/3 as much as a percent of\nGDP on health care. England spends less than\nhalf as much on health care. So basically, we spend\na lot on health care as a share of our economy. And what do we get for it?"}, {"content": "Well, the evidence here-- the first fact is clear."}, {"content": "The evidence that we get for\nit is a little bit mixed. So if you look at the\ntypical thing on the web, you know, US health\ncare is terrible. Our money's wasted. You'll see that on things\nlike infant mortality, we rate, like,\n20th in the world. Or life expectancy, we're,\nlike, 20th in the world. So by those metrics,\nwe don't do very well. But in fact, those metrics\nare misleading because we also have-- we have the most unequal health\ncare system in the world. So the right way\nto think about it is to think about the\nhaves and the have-nots. The haves, which is us and\nmost people in America, people who are\nwell-insured in the system, actually get probably\nthe best health care in the world, Now that might\nbe disputed by many people. But I think about\nthis like an economist would think about it,\nwhich is, how would you decide whether you would prefer\nproduct A versus product B, whether they buy product\nA versus product B? Every year, one million\npeople come to the US to get treated for their\nhealth care problems."}, {"content": "No one leaves. No one's going to\nEngland for surgery. No one's flying from the\nUS to England for surgery. They're coming here."}, {"content": "If you're in the system, we\nhave the best health care in the world. Unfortunately, if you're\nout of the system, we have some of the worst\nhealth care in the world. So a white baby born\nin America today, there's roughly a slightly\nmore than 0.5% chance the baby will die\nin their first year. That's comparable\nto northern Europe. If you look at a black\nbaby born in the US, the odds they die\nin the first year are about twice that, which\nis worse than Barbados. So the problem in the US is\nnot that our outcomes are bad. The problem is\nthey're very unequal-- that we're spending\nall this money. We're delivering good,\nbut not exceptional, outcomes for people\nin the system and bad outcomes for\npeople out of the system. So clearly, we're not\ngetting a lot of value-- it's not like we deliver\nexceptionally good outcomes to people in the system. We're slightly better,\ndespite spending a lot more, and we're worse for\nmany Americans who are left out of the system. So that's sort of the setup of\nwhere we are, which is really, you have two\nfundamental problems in health care in the US. Our spending is too high, and\nour access is too unequal. Now so I want to focus today's\nlecture on those two aspects and think about how can we\nbring the kind of lessons we've learned in this course\nto thinking about addressing those problems. So I'm going to focus on the\naccess problem and the cost problem."}, {"content": "Let's start with\nthe access problem. Now in America, before 2010,\nwe had about-- or before 2014, we had about 50 million\nuninsured Americans. 50 million people\nwho did not have health insurance in the US. We're the only\nnation in the world-- only developed\nnation in the world with a significant\nuninsured population. Now the fact that 50 million\npeople are uninsured, is that a problem? On its face, if I just\nsaid here's a fact. 50 million people in America\ndon't have health insurance. Based on that fact\nalone, can you tell me whether there's\na problem or not? You shook your head no."}, {"content": "Why not? AUDIENCE: Because it might be\nbetter for you not to have-- JONATHAN GRUBER: Yeah. You know, many more people than\nthat don't have flat-screen TVs and don't own homes. Why do we think\nthat we should care if people don't have something? The answer would be, we\nwould only care if what? Under what condition? When do we-- yeah-- AUDIENCE: [INAUDIBLE] JONATHAN GRUBER:\nIf-- well, they'd be better off if\nthey did have it. Now they could be better off\nbecause they could be richer, but that's not our problem. Given their budget,\nthey're not buying it. What-- under what condition\nis the market not-- under what type of conditions\nwould the market not deliver the best outcome? AUDIENCE: If there's\na failure, like-- JONATHAN GRUBER: If\nthere's a market failure. So the fact that\npeople aren't insured doesn't matter except A, if\nthere's a market failure, or B, for redistribution purposes. Remember, that's\nthe two reasons we want the government involved. So if health insurance markets\nwere perfectly functioning and people who were\nuninsured were roughly equally distributed in\nincome as everyone else, there'd be no cause for worry. But in fact, that's not true. We've talked in\nthis class about why markets like health insurance\nwon't function well, which is a problem\nof adverse selection. The problem is\ninformation failures which will lead health insurance\nmarkets not to function well. And the people who\nare uninsured tend to be much poorer than the\npeople who are insured. It's also\nredistributional concern. So the reason we care\nabout the uninsured are both because\nof market failures and for redistribution, that\nthey tend to be lower-income. What's interesting\nis the uninsured don't tend to be the\npoorest in society. They tend to be the near poor. So here's the way sort of\nhealth insurance coverage works in the US. For the vast majority\nof Americans-- 60% of American-- 60% of Americans\nhave what's called employer- sponsored insurance. So like your most\nof your parents, like me, they get health\ninsurance from their employer. The typical upper-income\nAmerican gets health insurance from their employer. The typical average-income\nAmerican does. About 60% of Americans. Then-- and I'm going to\ndo this sort of pre-ACA. So before 2014,\nbefore the big change that was put in place by\nthe Affordable Care Act, you had about another,\nmaybe, 6% that bought into what we call\nindividual or non-group health insurance. That is, they went out on\ntheir own and bought insurance. But that's a tiny\nmarket compared to ESI. And the reason is because of\nexactly the adverse selection problem we talked about. Think about yourself\nas an insurer. And you're worried about\nyourself as an insurer. And think about\nwhat your goal is. Your goal as an insurer\nis to essentially absorb risk in a way that allows\nyou to make a profit. So what you want is\nyou want to live off the law of large numbers. You know that with a\nlarge enough group, you could be able to predict\nwhat their costs will be. And therefore, you can\njust make a profit on top. So insurers love-- when\nMIT comes to an insurer, they're delighted. They're like, look, you got--\nbetween MIT and Lincoln Labs, you've got about\n10,000 employees. I, with great\ncertainty, can predict what the costs will be next\nyear for a group of 10,000 employees. And so I, as an\ninsurer, can know I can just charge that, plus\nX percent, and I'm golden. But when Jon Gruber\nwalks in the door, they're like, why are you\ncoming to me, individual? Maybe because you know\nyou're sick, maybe because you love skydiving. I don't know."}, {"content": "But I'm wary of you, so I'm\nto charge you a lot of money to get health insurance. As a result, most-- very few\npeople bought health insurance on their own. And in particular, the\nreason they didn't is because insurers would not\noffer health insurance to people if they were at all sick. They would do things\nlike having what we call pre-existing\nconditions exclusions. These were features of insurance\ncontracts which said, look, you walked in the door, Jon,\nand you want health insurance. But I know, in the past, you've\nhad cancer or asthma or knee surgery. I'm going to tell you,\nI'm going to insure you, but not for any expenses that\nmight arise from recurrence of those past injuries. So you had cancer in the past. Anything that comes up in the\nfuture because you had cancer, I'm not going to cover. Anything that comes\nup in the future because you had knee surgery,\nI'm not going to cover. Anything that comes up in the\nfuture because you had asthma, I'm not going to cover it. So I'm going to give you,\nessentially, partial insurance. So it's going to be\na market failure. I'm going to insure you, but\nonly for part of what you need. Alternatively, they could\nuse pre-existing condition solutions-- they could\nuse what is called medical underwriting,\nwhich was basically saying, OK, Jon, come in. I'm going to give you an\nexam, and if you look sick, I'm going to deny you insurance. Or if you look sick, I'm going\nto charge you 100 times more than someone else. So these were not\nillegal or even immoral. These were just ways\ninsurers came up with to try to deal with the\nadverse selection problem. As a result, this\nwas a market that did not function very well. Question about that? AUDIENCE: [INAUDIBLE] JONATHAN GRUBER: No, totally\nlegal in every state-- virtually every state,\ntotally legal and not immoral. I mean, this is just they're\nmaximizing their profits."}, {"content": "It's what companies do. And the point is that when\nthey did this, what this meant was if you didn't have\nemployer-sponsored insurance or insurance from the\ngovernment, which I'll come to next, then you\nwere subject to the fact that if you got\nsick, you might not be able to get insurance,\nwhich is sort of weird. Insurance is supposed\nto cover if you're sick. But in fact, if\nyou were sick, you might not be able to get it. So that was the fundamental\nmarket failure we had here through adverse selection. Now we also-- that was\nemployer-sponsored insurance, so that was about 2/3\nof the population. You also had on the order\nof 15% of the population had government-sponsored insurance-- probably more like 20%. 20% of the population\nhad government-sponsored. Insurance. The two big programs here are\ncalled Medicare and Medicaid. Now if you ever\ntake my 1441 class, I will only hold you\nresponsible for one thing if I ever meet you\n10 years later, which is remember the difference\nbetween these two programs. Medicare is health\ninsurance for the elderly. Medicaid is health\ninsurance for the poor. And those are our two big\npublic insurance programs. And about 20-- and if\nyou're in those programs, you're also set. They don't have any\nof these features. If you're in, you're\ncovered for everything. So about 20% of\npeople are there. And then finally, if you add up\nthe numbers, we had about 15-- the numbers don't quite add\nup, but you had about 15% of the population was uninsured. 15% uninsured. So you had about 2/3 private,\nabout one fifth public, and about one sixth uninsured. And those are individuals\nwho typically were not the poorest because the\npoorest people got Medicaid. The typical uninsured\nperson is, like, what we call the working\npoor, someone who's got a job, but it's a crappy job that\ndoesn't offer health insurance. But they make enough\nmoney that they can't qualify for being\nin the low-income program. So your family is struggling at,\nlike, $40,000, $50,000 a year, high enough income\nthat they're not qualifying for Medicaid but not\nin a good enough job they're getting health insurance. That's your typical\nuninsured family. 2/3 of the uninsured\nare in families that are headed by a\nfull-time, full-year worker. They're not typically the\nunemployed down-on-their luck people. They typically\nare the people who are trying to play by the\nrules, as they say in politics, but typically can't get a\njob with health insurance. So that's your basic landscape. And what we know\nfrom that landscape is that a lot of\nthe access problems were because of this\ngroup and this group because the people who couldn't\nget in this market, and as a result, were often uninsured. That was a lot of what\ndrove the access problems. So that was sort of the first-- one of the two big problems\nthat faced our system. And for many, many years,\nwe knew we had that problem. And for about 100\nyears, we've tried to reform health care in\nthe US to deal that problem. And probably about\nevery 17 years, on average, there was a big\nattempt to reform health care, and they always failed. And they always\nfailed because they failed between two extremes. There were two extreme\nviews that could never quite meet in the middle ground. And they come to what I\ntalked about last time, which is how do we solve the\nproblem of market failures in insurance markets? Well, one version of\nsolving that, I described, was subsidization. You could-- remember,\nwith my MIT program, if I paid the healthy guys\n$500, they'd all buy two, and I'd solve the problem. So one version\nwas subsidization. The problem is\nsubsidization only works if it's big enough to\novercome these problems. And no one ever proposed\nsubsidization big enough to overcome these problems. In my MIT example,\nI was going to give $400 to every\nhealthy-- first of all, it means giving money\nto healthy people, which is sort of\npolitically difficult. Like, hey, the healthier you\nare, the more money you get. It seems a bit weird. Also, it's just hard\nto solve these problems by just subsidizing people. Insurance companies\nare still too good at trying to get\nrid of the sick people. And even if you subsidize\npeople who come in, insurance companies will\nalways have an incentive. They'll say, great, healthy\npeople come, we'll subsidize. They'll still want\nto avoid the sick. So it doesn't solve the\nproblem in insurance companies. I didn't talk about\nthis last time, but as MIT's\ninsurance company, I should try to shed\nthe sick people. And that problem still\nexisted under this solution. The other extreme, which is\nsort of back in style again, is the single-payer model,\nwhich is saying, look, let's just have the government\nprovide health insurance to everyone. We have the government provide\nSocial Security to everyone. The government provides\nhealth insurers to every elderly in\nAmerica through Medicare. Everyone over 65\nin America, boom, gets government-provided\nhealth insurance. Talk about socialism. Every American gets that. In Canada, everybody gets\ngovernment-provided health insurance. Why not just do it here? Let's get rid of all the\ncrap with insurance companies we don't like. After all, insurance\ncompany administrative costs are about 15% of\nmedical spending. So, boom, we could lower\n15% of medical spending. That is, you know, that's\nlike $500 billion a year. Boom, it's gone. So basically, why not-- so\nsingle payer is something a lot of people\nhave advocated for. Let's just have one giant\nuniversal health insurance program. Now the problem with this-- the problem with the\nsingle-payer approach is largely-- there's pros and cons to\nthe economics perspective. But the problems here\nare largely political, which is that to make\nsingle payer happen, you have three enormous\npolitical barriers, which come back to economics. Everything comes\nback to economics, but they play their way out\nin the political system. The first problem\nis paying for it-- paying for it, which\nis that single payer-- to have the government give\neveryone health insurance means a massive expansion\nin the government, which means a big increase in taxes. And we know taxes\nhave deadweight loss."}, {"content": "We know taxes are\npolitically unpopular. Now here's what's\nmisleading about that."}, {"content": "Here's the fundamental thing. So I worked for the\nstate of Vermont. The state of Vermont wanted to\ndo their own single-payer plan. If any place can do\nit, it's Vermont. They're, like, super lefty. They essentially have\none insurance carrier, which is Blue Cross anyway. They're a small state. It seemed like if anyone\nwas going to do it, Vermont was going to do it. So I worked with them to\nput the numbers together, what it would cost them. And I had good\nnews and bad news. The good news was,\nI said to Vermont, if you do single payer you will\nlower the cost of health care in Vermont total by at\nleast 10%, at least. That was conservative. The bad news is to\npay for it, you're going to have to more than\ndouble the entire amount of taxes collected\nin state of Vermont. And that second sentence\njust killed everything. What's the problem? The problem is that right now\nhealth insurance in America is paid for by\nessentially a hidden tax. What's the hidden tax? It's the fact that when\nyour employer gives you health insurance, they pay\nyou less wages as a result. Remember our tax\nincidence discussion. And we said that essentially\ntaxing the employer falls on the\nemployer-employee depending on basically elasticities. Well, you can think of health\ninsurance the same way. When your employer gives\nyou health insurance, he doesn't just\neat the whole cost. He says, look, I'm paying you\na total set of compensation, part of which is\nhealth insurance. So I'm going to pass the\ncost of that health insurance on at least partially\nto your wages. That's essentially a hidden tax. So at MIT-- right now, I\nhave a health insurance plan through MIT, which costs about\n$18,000 a year for my family. I pay about $6,000 a\nyear out of my paycheck. MIT pays $12,000. But the truth is, MIT\npays me $12,000 less. They don't just give me\nthat health insurance out the goodness of their heart. They take it out of\nmy wages, or least partially out of my wages. That's essentially a hidden tax\nthat Americans pay every year to finance health insurance. If we went to single payer,\nthat hidden tax would go away. I would get a $12,000 raise. That's great. But I'd also face\na high new taxation to pay for the\ngovernment-sponsored plan. Now given that the total\ncost would fall-- we should be able to net this out\nin a way that most people win. The problem then\nbecomes the politics, which is you're tracing a hidden\ntax with a non-hidden tax. And that's very\nugly politically. So people don't believe their\nemployers will pay them more if you don't make the employers\nprovide health insurers, like, oh, the employers\nwill just pocket it. And I could teach them tax\nincidence till my face is blue, but they just won't believe it. They'll say employers\nwill just pocket it. But I have to pay this\nnew tax for single payer. So that's the first\nproblem single payer faces is that people don't\nreally understand that trade-off between\ngetting rid of the hidden tax and adding a new non-hidden tax. That's problem one. Problem two is the problem\nwe talked a little bit about, behavioral economics,\nand about loss aversion. There's a general feature,\nwhat we call status quo bias in human thinking. Status quo bias, which\nis, essentially, it is harder for me to\ngive up what I'm used to than to grab something new. We talked about the mug example."}, {"content": "Remember, I talked about mugs. So basically, you\nhad to pay me more to get the mug away from me than\nI was willing to pay to buy it. That once you have something,\nyou value it more than if you didn't have it yet. Well, right now,\n60% of Americans have employer-sponsored\ninsurance. And if we say to them, give\nthat up for Berniecare, they're going to be,\nlike, eh, I don't know. I kind of like my\nemployer-sponsored insurance. You know, yeah,\nyou might tell me Berniecare is\ngoing to be better, but that's just you talking. I know what I have\nright now, which I have employer-sponsored insurance. I don't want to move away\nfrom that status quo. So status quo bias makes\nit hard, in general, to do radical changes\non an economic system. And this is a perfect example. It's going to be hard\nto get people to give up what they have for something\nthat they don't really know about yet. That's the second problem. The third problem is,\nonce again about money, but really beyond the\nscope of this course, which is the problem of the insurance\ncompanies and lobbying, which is that the insurance business\nis big business in America. Health insurance companies\nmake about $900 billion a year. If you said to them, hey,\nhealth insurance companies, would you guys mind just\ngiving up your $900 billion to begin a single-payer\nhealth care, they'd actually say, yeah,\nit's been a good run. Go for it."}, {"content": "No. They're going to lobby and\nfight that because they want to keep their business. And that's going to be a\npretty hard force to overcome. So single payer has\nalways struggled with dealing with these\nkinds of political problems. And that's why we've been stuck. We've been stuck between\none alternative, which is subsidization, and the\nother alternative, which is single payer. And that's where\neconomists have come in-- came in the 2000s,\nfolks like myself, to talk about a\nnew alternative way to do it, which was\nessentially to try to bring in some of the\nbest features of these two approaches. And the solution we proposed-- so if you want to\nread more about this, I've actually written a\ncomic book to explain it."}, {"content": "It's a graphic\nnovel, technically. It's called Health Care Reform. It's, like, $9 on Amazon."}, {"content": "And so I like to think of\neverything in terms of images. Now I'm not going to draw one."}, {"content": "I'm not going to try\nto draw anything. But the way I like to think\nabout this is the solution we came up with, which we first\npioneered here in Massachusetts and then brought to\nthe whole country through the Affordable\nCare Act, is what we call a\nthree-legged-stool approach, three-legged-stool approach. Leg one is deal\nwith this problem. Deal with the insurance\ndiscrimination problem. And so leg one is ban\ninsurer discrimination. No more pre-existing conditions,\nno more medical underwriting. That is, if I walk in the door,\nand you have offered anyone-- you have to offer me health\ninsurance at the average price for my age. And you have to offer it to me. So any 40-year-old who walks\nin the door wanting insurance, you have to sell it to them,\nand you to sell it to them at a fixed 40-year-old price. You can't say, you're sick. I'm not going to sell it to you. So the first step is to\nban insurer discrimination, to try to solve that problem. Now the problem this\nraises is you have simply-- if you do this alone, you've\ncreated a new problem, which is if you tell insurers\nthey can't discriminate against the sick, you don't\nsolve the adverse selection problem. You're just making\ninsurers go bankrupt."}, {"content": "Now here's the way I\nlike to think of it. I'm sure none of you\never gambled on sports. But if you had\ngambled on sports, you might know the way\nsports gambling works is that there's a guy in the\nmiddle, called the bookie. And the bookie's\ngoal is to not-- is to get exactly the same\nnumber of bets on either team. So they take no risk, and just\nmake their profits off the top. So what bookies do is\nthey set point spreads. So the Patriots played the\nDolphins this past weekend. I am-- sadly, I'm\na Dolphins fan. The Patriots played\nthe Dolphins. The point spread was\nsomething like-- does anyone know what the spread\nwas in the Patriots' game? I think is was, like, 8 points. So that spread was chosen. The Patriots were favored by 8. What that meant was\nyour bet was either the Patriots win by either\nmore, or the Dolphins win, or the Patriots win by\n8, or by less than 8. So one side is Patriots\nwin by 8 or more. One side is Patriots win by\nless than 8, or Dolphins win. And the reason you\nhave that bias thing is because people think\nthe Patriots are better. They are better. And as a result,\nyou want to get-- if you set an even bet,\nPatriots win, Dolphins win, everyone would bet\non the Patriots. You'd lose money. So you want an equal\ndistribution of risks. So what you want is you\nwant to set the point spread so the distribution\nof risk is equal. Then having done that, you just\nmake your money off the top. Now imagine I passed a law\nwhich said all sports books have to reopen at halftime and make\nthe same bets available they made before the game started. Well, for those of you who\nwatched the exciting game this weekend, you\nrealized at halftime, it became pretty obvious\nthe Patriots weren't going to win by 8, that\nit was a lot closer game than people thought. So if they reopen\nthat, a bunch of people would suddenly bet\nagainst the Patriots. The Patriots ended up\nlosing, and the insurers would have gone bankrupt-- the\nbookie would've gone bankrupt. Insurers are just bookies. That's all they are. They just want a predictable\ndistribution of risks. So if you tell them, you have\nto offer health insurance to everyone for the\nsame price, but only the sick are going to buy,\nthey're going to lose money. So that's why we need the\nsecond leg of the stool, which I talked about last time, which\nwas the individual mandate. The individual mandate,\nwhich is to say, OK, insurers, if you offer\nhealth insurance to everyone at a fair price, we will,\nas our part of the deal, make sure everyone\nbuys health insurance. So when the 40-year-old walks\ninto your office wanting insurance, you can know it's\nnot because they're sick."}, {"content": "It's just because they have to. So we say to insurers, you\nprice insurance fairly, and in return, we'll\nmake sure you get the fair distribution of risks. So you say to me--\nmy MIT insurance, you price insurance\nat $1,500 and don't try to keep out the sick,\nI'll make sure everyone buys. And you'll make\nyour $100 profit. So that's-- the mandate was\nessentially trying to bring-- was trying to allow--\nget rid of discrimination by bringing in the\nentire pool of people so insurers could fairly price. The problem with that is you\ncan't mandate something people can't afford. So in Massachusetts,\nwhere we were creating this plan\nin the mid 2000s, the typical family\nhealth insurance policy was about $12,000 a year. The poverty line for a\nfamily was $22,000 a year. We couldn't exactly\nmandate people that they spend 55% percent\nof their income on health insurance. That was not really feasible. So the third leg of the\nstool we came up with is subsidies to make health\ninsurance affordable, saying, if you're\nlow-income, we will offset the cost of your insurance just\nlike the subsidy approach here. We'll offset the cost\nof your insurance to make it more affordable. We'll do it on an\nincome-related basis, so it doesn't cost so much. So we're not going to\nhave to pay for everyone's insurance like single payer. Remember, single payer,\nessentially taking someone like me, who's\nhappy with my insurance, swapping it out for new\ngovernment insurance. This is saying, no, if you're\nhappy with your insurance, stick with your insurance. But if you're low-income and\ncan't access the employer market, this gives\nyou a new place to go. And that was the idea\nthat became Romneycare, the plan here in Massachusetts,\nand eventually then became Obamacare, or the\nAffordable Care Act. So this is essentially\nthe idea of that plan. Now, did it work? Unambiguously, yes."}, {"content": "Now you won't find\nanyone more biased than me on this question. But I think what-- I think if I've tried to teach\nyou one thing in this class, it's that we need to rely on\nreal facts wherever possible. And if not, we could\nturn to theory. But here we have a set of real\nfacts that we can turn to, which is that\nessentially what we did in Massachusetts with this\nlaw is we covered about 2/3 of the uninsured population. At the federal level,\nwe covered about 45% of the uninsured population. It was a lower number because\nthe federal law did not apply to undocumented\nimmigrants, which are about a quarter\nof the uninsured. It's not an issue\nin Massachusetts, but a big issue in other places. That's about a quarter\nof the uninsured because the federal\nlaw did not apply to undocumented immigrants. So as a result, the\nshare cover was lower. But a large number\naren't covered."}, {"content": "Yeah. AUDIENCE: If there was\nan initial mandate, then how was there anyone\nwho was left uninsured? JONATHAN GRUBER: Great question. So there are three reasons why\npeople were left uninsured. The first reason was a\nquarter of the uninsured were undocumented\nimmigrants, and the law didn't apply to them. So right now the upper bound\nwas 75%, just to start. The second reason is that the\nindividual mandate contained exemptions to make it both a\nlittle more humane and, quite frankly, politically feasible. So if you could not\nget-- if your income was below the poverty\nline, you were not subject to the\nindividual mandate. And if you could not get\ninsurance for less than 8% of your income, you\nwere not subject to the individual mandate. So there were exemptions. And the third thing was\nthe individual mandate was not, like, we're going\nto throw you in jail. It was a tax penalty. And many people decided they'd\nrather just pay the penalty than buy health insurance. So for those three\nreasons, a number of people did not get health insurance\nunder the Affordable Care Act. Now there's a bunch of\ninteresting questions, like should the mandate\npenalty be bigger? How should we handle that?"}, {"content": "There's a lot of-- I could go for hours on this. But that's basically the\nstructure of what we had. So basically, that worked. It didn't get us to\nuniversal coverage. It wasn't as effective as\nsingle payer would have been, but it was the largest\nsingle insurance expansion in American history. And the evidence is clear. It brought many\npeople into insurance. It improved people's\nuse of health care. It improved health. So basically, that was kind\nof the step forward on access. Now the problem with\nthis is it's only a step. There's still many\nuninsured, and this has been politically really\nchallenging, because these two answers are quite simple. Just give people money or\njust have single payer. This is super complicated."}, {"content": "I can talk about these\nin about 15 seconds each. These took five\nminutes to go through. And people thought it\nwas just too complicated. It didn't make sense. Lots of reasons-- we could\ntalk lots reasons people didn't like it. So it's never really been\nas politically successful as people like myself,\nwho helped develop it, would have hoped. And it's left a lot\nof people uninsured. So we haven't solved\nthe access problem. We made a big step forward,\nbut we haven't solved it. And that's the ongoing debate\ntoday we see, particularly in the Democratic Party. The Republican Party\nreally doesn't focus much on insurance coverage. But the Democratic Party does. And they're-- that's why there's\na lot of energy behind single payer right now is like, look,\nyou tried the kind of halfway ground. That kind of worked, but\ndidn't work all the way. So let's just go all\nthe way to single payer. Yeah. AUDIENCE: The initial\nmandate, does that-- I guess, for the\nmore people living in poverty, does that work\ntogether with Medicaid or-- JONATHAN GRUBER: Yeah. Basically, a lot-- actually,\nit's quite interesting. It worked quite\nwell with Medicaid. A lot of people\nwho aren't insured, actually, are people\nwho are already eligible for free\nMedicaid coverage and just don't take it. Now we don't quite know why."}, {"content": "It could be language barriers. They don't understand. A lot of people-- a lot\nof even legal immigrants just don't understand\nthey're eligible. It could be people just don't\nwant a government handout. They're embarrassed taking\nhelp from the government. It could be people\nthink, I don't need it. I'm never going to be sick. We don't know why. So part of what\nthe mandate did was say, look, you already\nhave free health insurance. Just pay attention and take it. That's part of the effect it\nhad was bringing people in. So a large part of\nthe coverage increase is actually bringing in people\nwho were already eligible, just weren't taking it up before. So that's kind of where we are. So where we stand\nnow in coverage is we've taken a\ngiant step forward. We've covered probably\nabout now, probably, between a third and 40% of\nthe uninsured in America. But we're sort of right now\nkind of stuck at that point. And the question is, do we\njust sort of stick there, or do we try something\nmore aggressive? With the political\nproblems, I don't know."}, {"content": "But that's going to be the\nchallenge going forward. So that's-- questions\nabout that-- because that's where we\nare on problem number one, which is access in coverage. Now let's turn to problem\nnumber two, which is cost. Cost is way harder."}, {"content": "What I just did\nwas the easy part. It's way harder to get\nyour health care costs, and here's why. Two facts that are seemingly\ncontradicted if you think about it. Fact one. Since 1950, US\nspending on health care as a share of our\neconomy has quadrupled. We've gone from--\nmore than quadrupled. We've gone from 4% of our GDP\nbeing health care to over 17%. And it's been worth it. If you look at the\nimprovements in our health, and you value them in\nthe way economists do, which is we have statistical\nvalues of life we apply, or statistical values in\nimprovement in health, the improvement\nin our health has been worth the money\nspent on health care. You guys don't realize it. Health care totally\nsucked in 1950. Babies born in 1950 were\nfour times as likely to die before they reached\ntheir first birthday. If you had a heart\nattack in 1950, you were four times likelier\nto die within the first year. To put it in terms all young\nhealthy people care about, if you hurt your knee skiing\nin 1950, tore your ACL in 1950, or tore your cartilage, you\nwere in the hospital for a week. You were on crutches\nfor six weeks and had arthritis the\nrest of your life. Today, you go to an\noutpatient center. You get arthroscopic surgery. You're back on the slopes\na couple weeks later. Health care is just way better,\nand our health is way better. America is a much better off\nnation, spending 17% of GDP on health with how healthy\nwe are than we were in 1950. And once again, do\nthe economists tests. No one ever advertises, hey,\nwould you like 1950s health care at 1950s prices? No one out there is offering\nthat because it's worth it. That's fact one. Fact two is we waste a huge\namount of money on health care. By some estimates, about a third\nof what we spend on health care is totally wasteful, does\nnothing to improve our health. Now how can those two\nfacts be consistent? It's worth it,\nbut it's wasteful. Well, the answer is that the\nother 2/3 is super awesome, that basically the\nincrease in health care, where it's been productive,\nhas been amazing. But we dragged along all this\nunproductive spending too. So it's good news and bad news. So the good news is,\nwell, that's great. We just cut out the one\nthird that's unproductive, we've solved our problem. Literally, if we could\njust simply cut out the one third\nthat's unproductive, we'd spend the same amount as\nEurope does on health care. We'd solve our entire\nlong-run fiscal problem. The bad news is that it's\neasy to look back and see what the one third was. It's hard to look forward and\nsay what it's going to be, that health care comes with\na huge amount of uncertainty about what's going to work and\nwhat's going to be worth it. And as a result, it is\nvery hard to say, OK, fine. We'll cover this. We won't cover\nthat, because it's hard to know what's going\nto work and what's not. And so, essentially, you're\nin this very difficult spot. So what are the\nkind-- that's the sort of fundamental\ntrade-off that we face. So what are the potential\nsolutions to this problem? So essentially, there's a\ncouple of different solutions to the problem, two different\npaths we can follow. Path one is the regulatory\npath, which is basically the path that Europe follows. What Europe does is they\njust much, much more heavily regulate the delivery\nof health care. And they do that in two ways. One is they actually have\nregulations about what health care you can get. So for instance, England has\nthe euphemistically named NICE, the National Institute for\nHealth and Care Excellence, which actually tells people\nthey can't get some things. It literally rations. So for example, for many\nyears-- it's no longer true-- in England, if you're over 75,\nyou could not get a transplant. They said, look, we got a\nlimited number of kidneys. You're going to die soon anyway. Let's give the kidney\nto a young person. Actually, kind of makes sense. The idea is, look, we have\nsome limit on our kidneys. Why should it be determined\nby some random fact, like when you got on line? It should be\ndetermined by who gets the most value from the kidney. It's going to be someone who's\n30, not someone who's 75. So one regular route\nis to literally have regulations like that. That's actually pretty rare. Most countries don't actually\nregulate in that way. Most countries kind of let\nyou get what your doctor says you should get. There's three routes."}, {"content": "So one route is\nsort of regulatory. The other route of regulate--\nso one route is sort of what we call sort of\nregulating, you know-- I don't want to call\nit access-- sort of technological regulation,\nregulating which technology you can get. The second kind of regulation\nin Europe is supply regulation. So they basically don't\nlet there be many doctors. And there are not many\ndoctors and hospitals. So there are as many\nMRI machines in LA as there are in Canada. Basically, just not many place\nto go get an MRI in Canada. So if you' hurt\nyour knee in the US, you go, you get an MRI,\nlike, the next day. In Canada, you get\nit six weeks later. So the only way to control\nit is to actually regulate the supply of medical care. Just give people less\nstuff they can use. And the third way to control-- the third regulatory mechanism,\nand the most important, is price regulation. We are the only nation in\nthe world which essentially lets the free market determine\nthe price of health care services. Every other nation\nregulates the prices that people pay for their\nhealth care services. Now the question we\nhave to ask is why? Why does that make sense? Well, the answer would\nbe that we think-- it would make sense if we think\nthere's a fundamental market failure in the determination\nof health care prices. And in fact, it\nturns out there are numbers of market failures in\ndetermination of health care prices. So one market\nfailure, for example, is imperfect information. I don't know-- I can't shop\neffectively-- when I'm in the back of the ambulance\ndying from a heart attack, I can't be, like, you know\nthat hospital looks expensive."}, {"content": "Take me over there. I want to shop there. You can't really shop. It's a hard market to shop. And if you could,\nprices aren't posted. You don't really\nknow what it costs to get your heart attack\ntreated in different places. So imperfect information. There's also\nimperfect competition, which is if you have your\nheart attack on Cape Cod, there's, like, one-- or Nantucket, which is\nan island, with no way off but a ferry,\nthere's one place to go. There's one hospital. They have a perfect monopoly. You can't get off the island. You're going to die otherwise. So it's imperfect competition. There's even\nimperfect competition where you think the\ncompetition might be perfect. So take Boston. There are so many\nhospitals in Boston, you cannot literally fall down\nwithout hitting a hospital. Yet there is an\nenormous dispersion in the prices hospitals charge. In particular, the\nvery famous hospitals, like Mass General\nHospital, charge multiples of what less\nfamous hospitals charge, even though less famous\nhospitals are really nearby. Why? Well, because they\nhave essentially what we call a reputational\nmonopoly, that even though they don't have an actual physical\nmonopoly, people are like, I want to go to MGH. They're the best, even if\nthey're not necessarily the best. They just have this\nview of being the best. And they can charge\nhigher prices as a result, even if their outcomes\naren't necessarily better. In other markets, we\nthink perfect information would allow us to get rid of\nthese kinds of inefficiencies. It doesn't exist in health care. As a result, perfect\ncompetition simply does not work in\nhealth price setting. And as a result,\nall other countries regulate health care prices--\nand then not other countries-- even the US can\nregulate health prices. So the Medicare program\nhas regulated prices. That covers millions\nof Americans. It's just for the\nnon-government, private health insurance in the US, there's\nnon-regulated prices. Now I am not, despite my tone,\nsaying that regulating prices is the answer. It's not clear. Regulating prices comes\nwith a huge number of additional problems\nlike we talked about. We talked about\nregulated monopolies, which is the government may not\nknow the proper price to set. The government may\ndo a terrible job. They may get lobbied. They may be corrupt. Indeed, in the US, the\n1970s, virtually every state did regulate hospital prices. And every state\nwent away from that because they thought\nthe system was broken. So it's not like\nthere's any-- it's not like the European\nsolution's an easy answer. That's why the other route that\npeople have been pushing lately is a different route, which\nis the incentives route, which is basically to say,\nlook, we don't want to regulate supply or prices. What we're going to\ndo is we're going to say, doctors and\nhospitals, you get together and form these units we\ncall Accountable Care Organizations, ACOs. This is a big innovation\nof the Affordable Care Act of Obamacare, set up these ACOs. These are hospitals and\ndoctors all get together to be basically,\nlike, soup to nuts, all the health care\nyou need in one group. And we say to them, we are\ngoing to pay you one flat amount of money to care for Jon. And then within that,\nyou decide what he gets. You decide what prices\neverybody pays and makes. You figure all that out. But we're going to\ngive you a flat amount. In particular, that flat amount\nis not going to rise much. And that's going to bring\nthe costs of health care under control, where\nbasically every ACO will get an amount that's\na flat amount, and it just won't rise much. And that's how we'll\nbring health care costs under control. That has a number of\nwonderful features. First of all, it's\nmuch less evil sounding than things like\nnot letting [INAUDIBLE] rise or regulating what prices. Second of all, there's much\nfewer regulatory tools. We just say, here's a flat\namount we're giving you per person, and we're done."}, {"content": "So that sounds great."}, {"content": "The problem is we haven't\nbeen able to get it to work. And that's because it turns out\ndoctors and hospitals aren't very good at figuring out how\nto set prices and set supplies. They're just not--\nthey don't know how to really figure this out. And the ACOs so far have not\nactually performed very well. They've not saved much money. So really, we're stuck\nbetween a route which seems a lot easier but we\nhaven't really figured out how to make work,\nand a route which has worked all around the\nworld but seems politically nightmarish. And that's kind of\nwhere we are right now in terms of controlling costs. And that difficulty is\nwhat we find ourselves in. But let me be clear. This is not like, oh, that's\nvery interesting, Jon. I'll go home and\nforget about it now."}, {"content": "This is the entire future. Health care costs are\nthe key to determine the entire fiscal\nfuture of the US. As I mentioned last\nlecture, the US is currently estimated about\n$75 trillion in deficit over the long run. $70 trillion of\nthat is health care. Health care is the\nsingle determinant of the US fiscal\nbalance in the long run. Literally, it's the single most\nimportant government problem facing-- health care cost\nis the single most important government problem\nfacing your generation and the next generation. I like to say that all\nthat matters when we think about the future\nis health care cost and global warming because\neither way we're under water. Basically, those are\nthe two big issues we have to face going forward. So this is a serious issue\nthat your generation is going to have to struggle with-- sorry-- as you go on. So that's health care\nin the US in 40 minutes. So this class--\nyou know, there's a famous skit from\nSaturday Night Live, which is what you remember\nfive years after college."}, {"content": "And it's five minutes, and 3\nand 1/2 minutes of spring break. I don't expect you to remember\nthe formula for-- if you're not going on in economics,\nI don't expect you to remember the formula\nfor deriving cost function. What I expect you to\nget out of this class is A, an interest in economics. And I hope you'll go on. I sincerely hope that. And I'm available\nto anyone who wants to talk about the pros and\ncons of going on in economics. Obviously, I'm more pro."}, {"content": "But I'm happy to talk about it."}, {"content": "So always feel free to\nreach out about that. But B, even if you don't\ngo on in economics, I want this to make you\na more educated consumer of the newspaper. This is-- we are in an era, as\nI said in my very first lecture, where truth and facts\nand the scientific method are, themselves, under attack. And MIT is the last bastion\nof fighting this war. We are the place that explains\nthe scientific method, that uses the scientific method. And we need to use\nthe methods you've learned here to\nthink intelligently-- whatever your conclusions--\nbut to think intelligently about these economics topics. And fundamentally, that\nmeans being annoying. And to illustrate that,\nI'd like to end with a joke that some of you may have heard."}, {"content": "Sorry, I apologize if you have. So the joke is a doctor, a\npriest, and an economist go golfing. They get on the golf course-- and they hit the golf\ncourse, and they're behind someone going\nincredibly slowly. I don't know if there are\nany golfers among you, but the idea is if\nyou're very slow, you're supposed to allow\nthe people behind you to play through and\nget ahead of you. This person won't let\nanyone play through. And he's, like, 50 shots a hole. It's disgusting. And there's, like, 50 people\nlined up behind this guy. And these folks\nare so disgusted, they quit after nine holes. They go back to the clubhouse. They're pounding their\nbeers like, what an asshole. I can't believe he wouldn't\nlet us play through. It ruined our day. And someone comes up to\nthem and says, excuse me, are you new to this club? And they said, yes, we are. He said, well, I can tell\nyou're new to the club because if you weren't new, you\nwould have known the person you were playing behind is blind. And actually, it's\na miracle he can get the ball in the hole at all. And usually, it's an honor to\nbe on the same course as he is. And the person walks away. And there's, like,\na deadly silence. And the people at the\ntable are like, wow. I feel terrible. And the doctor goes, I can't-- I feel terrible. I can't believe I'm-- myself, a man of healing, would\nbe so insulting towards someone who's blind. I'm going to dedicate a wing\nof my hospital to the blind. And he turns to the priest. And the priest says, I\ncan't believe myself, a man of the cloth,\nand that I'm supposed to care for the less able\nin society, would do this. I'm going to set up a free\nsoup kitchen for the blind. And they turn to the economist. And the economist says, well, if\nhe's blind, why doesn't he just play at night? And-- makes sense, right? And basically, the point is\nthat the job of the economist is to sort of be\nannoying and look for the basic\nflaws in arguments, to understand them, to ask\nthe difficult questions, but to have responsible answers. And that's what I hope\nyou'll get out of this course that I hope you'll\ntake forward with you. So thank you very much\nfor sharing it with me. And good luck on the final. [APPLAUSE]"}], "Online Degree = Scam? | IIT Madras Seminar 2024": [{"content": "hi everyone and welcome to a new video today we have my online session with the folks at I Madras online bs degree let me set the context this is not the folks from the standard IIT offline degree this is the online degree which is a lot of them from non- Tech backgrounds a lot of them from a dual degree a lot of them only here for the name of an IIT so the discussion was around is this online degree similar to the offline degree what kind of differences exist U Can you call yourself you know a real graduate from it Madras if you're doing this degree um will the outcomes from both the programs be the same how can the outcomes from this online degree which is very easy to get in be compared with the offline degree eventually because right now there's a lot of iits coming out with online degrees U fairly easy barrier to entry how can you outshine and have similar outcomes as the offline degree U if you're from this batch a lot of discussion around online versus offline how it's very easy to get into these online degrees but the number of people who are actually able to graduate is really less so how difficult is it to get through these degrees some discussion around research whether or not you should get a job follow research do entrepreneurship and a lot of random doubts that we have on a day-to-day basis as Engineers it's a fairly raw talk a lot of raw questions from students and I've tried to answer them to the best of my abilities with that let's get into my online session at it Madras we do want to become someone who is worthy of the name iitm so we are kind of in a place where we don't know what to do I think the phone that you're feeling is probably no different than someone from a tier three college or a tier 2 College who probably feel similar how do you hold up to the it name I think one good thing that happens in this program is the number of people who get out is really less like I saw the numbers and you know it's very hard to pass out there's something there like some level of respect you will get you know you're able to finish the degree how do you see uh the BS program for the students that are pursuing it cently I would say one thing okay think of it like any other BSC or BS program um do not associate yourself you know at all to college in super 30 I've seen two people fail interviews because of DSA only they were very good at web 3 but let's say the company asked DSA and then they so we had to start DSA classes then we saw no one here ised in DSA they're all here for Dev we stop DSA classes and then whatever referals are happening right now are happening purely based on dev should you do it interview you will need it thank you for making time for us so to begin with let me ask you something though what would you like to be a trist sir kirat or by yeah right I'll k so uh all of the students here are to saing the itm BS degree a remote degree uh I'd like to start off by setting the agenda that uh this won't be a Q&A session at all as much as we would love to have one with you but we want to just have a discussion for now uh being someone who is so uh into remote jobs remote structure the entire idea of remote looking at this uh particular degree and the students in this degree what would you suggest them like like I mean networking to important of course we have a lot of uh essential factors with networking uh you you keep speaking that uh networking is very crucial for students to do because that is the way to grow so what do you think uh for people pursuing this degree what kind of uh outlooks to should these people have for networking yeah that's a great question by the way can you hear me well I'm sorry yeah it's all it's totally fine cool uh great question I think especially for you guys considering it's an online degree you probably have to do it more than other people I don't super suggest this in campuses because you know in campuses you have on-site companies and that's part of part is sort of sorted of course not for a remote job but most other jobs you'll at least get a campus placement so you don't have to aggressively uh Network even though it's a very overused term networking um I think what that means over here is just being in small technical groups lead CTO Founders sometimes very technical events meetings toward Asia things like these rather than you know um know what the other way to network is um but long story short U for you guys considering it's an online degree I'm un sure what kind of placements you know um you guys can sit for at the end um and if the answer is you don't have an official placement thingy then it definitely makes a lot of sense remote job or otherwise for you guys to look out H I mean uh the the entire curriculum does have a you know placement cell industrial uh cell that does bring opportunities to students uh but the entire idea of you know placements is totally different for the people following this curriculum right and uh when we look at your videos you know it's all oriented towards corporate the corporate world networking through the corporate world climbing the ladder in a smart way and building your knowledge in a smart way but uh this entire degree deals with data and AI data science in AI mostly uh and this opens up opportunities for research more than corporate in the current world looking at the way these are AI is moving forward now uh considering the remote aspect and also the fact that research is a huge area of uh moving forward what do you look at uh research or corporate because there's a lot of things for us to look at through this degree so that's a tough choice um I have struggled with it uh half of not half of my batch like 20% of my batch went for Masters or phds I'm assuming that's what you mean by uh research so like it's a tough choice it's you earn first or you learn first is the question generally the idea first ear later whoever whichever of my friends have gone for a PhD or a masters in machine learning or AI are very well positioned for this upcoming bull to you know um Sprint past everyone else when it comes to not just monetary outcomes but the network the places they're at the people they're meeting the alpha they'll have in the next 10 years um so specifically in machine learning and yeah probably five years ago was the best time to go for a PhD today also not the worst time um anything else like cyber security or if you're just going you know for uh for a job in the US or for an H1B then these are tricky times uh because a lot of H1B holders are not uh getting jobs have to come back so on and so forth for ML if you get up basically you should go for it like that's research is great research is great for ML I'm I'm myself I'm an ml researcher as of now uh I work in ml in biomed data and uh you know medical side of applications uh when I look at it you know looking at your videos you yourself being someone having such a large fan base and you yourself having a lot of followers uh the guidance is very less the kind of uh there's no structured way of going through research at all uh there's there's a lack of structure there's a lack of hierarchy and I'm a True Believer of this you know if there is no hierarchy it leads to Anarchy that is what I that is what I believe in and the entire research area is totally H so uh when you yourself being someone so structured when you look at your videos it's always structured there's a pathway there's a road map or there's a laid out uh brain map for you and your and you have peers that you suggest to so looking at research and also applying that entire structured concept to it what do you think uh we should should we like stick to the structured concept that exists that has proven to be right or is there any other way to go ahead accordingly I think there's some some structure in research like you have to publish a lot of papers you have to have great cgpa try to go for an internship early find a good professor get in touch with them eventually your professor is going to you know get you in if you're looking at a Stanford or or a UBC so there's some structure there um there is also competition um because you know the structure is very well defined for folks at it Bombay Folks at it Bombay are you know probably the on-site people uh because a lot of these people are getting picked for internships from their second year itself going for you know University internships rather than corporates but there's some level of structure U that said uh of course it's fairly ad hog like in the end it's very hard to tell who got in how they got in um there's no examination that you're giving yeah but I would say you know like there's some level of luck involved and everything but the basic things you should check off like your cgpa needs to be really good some paper if you have published that's great and be in touch with professors if you want to get into uh UBC then make sure you know you know the best professor over there and makes sense I mean okay uh research there is a particular structure now assume that some person goes through all of the structured road map the basic structure that exists does well uh and is in a good position right now would you as a you know would you as someone who's a senior developer right now or a senior software engineer right now who's in CS would you hire someone into corporate research and if you would what kind of criteria would you look at uh for this person assume it's me and I'm into research what kind of criteria are you looking at as of now to hire someone and to corpor interesting so I've talked to a few Founders when this ml Rush came I was talking to Founders understanding who they're hiring for and tldr was they were avoiding researchers of course open a will hire you an anthropic will hire you all phds eventually land here only from the best but if you look at you know General startups building on top of llms um one they can't afford you because you know as an ml researcher you're probably going to make a mill at the very least from open AI so how would a startup that's a starting afford you one two they also might not need your expertise because they're not building at the foundation level they're not building llms they're building on top of LMS so all the companies that I am sort of involved with if I think about it even in web 3 rarely does any of my companies hire a blockchain researcher we work with companies uh who hire blockchain researchers for example if we want to get our contracts audited we'll hire and consult they'll charge us very heavily and they're working with they're hiring or usually are group of phds who are doing cryptography so you know you don't need a job long story short if you're going for research you know you people will pay you for you have value people will find a way to reach you and compensate you and you don't really need to stuck to the stick to the 50 40 hour week right you can provide value other ways to many companies compared to one but if you are looking for one there'll be an open AI or you know someone working who be happy to at least interview you if not hire makes sense makes total sense but there's this one thing that you said you know about startups that you've talked to a lot of startup people and uh you ask them about their ml engineer hiring uh that is another issue now that is a like every company has a a next to them now I mean if you enter Bangalore you'll probably run into someone in the Metro and you ask them what they're doing they'll tell you that they're having a company with a next to them so what should we look out for because as someone who is there who who are studying this data and AI uh when the Bull Run is actually happening when there's a proper hike in Ai and unsure as to what is going to happen in the near future uh what what need what do we need to look at because there's a lot of options out there we don't know which business is going to stand Which business is not going to stand and uh um being researchers or being let just say people who are into corporate how do we look at a company and say okay you know what this looks good I can be here interesting I think that's true not just for AI but generally high growth startups okay you can't judge which one's going to do well until they reach a Tipping Point for example by now you know GTO is probably going to do well zato is public profitable they're going to do well everything else is a gamble um specifically for you know high growth startups but you can look at their funding run Runway things like these and and usually bet on the founders if there are solid Founders one business guy one Tech Guy you can learn from them your worst Cas is you learn a lot from the company U your medium cases the company survives you get a salary your best Cas is the company does really well your Equity grows a lot so specifically for a companies as youve said a lot of B companies in Bangalore not all of them would require or you know most of them wouldn't probably require an AI engineer as I said most of them have delegated all AI things to uh either libraries or to llms they're not doing this in house it's referred uh but I think generally companies are looking for you know whatever that product needs let's take an example uh a good example of an something on top of llm might be what Zoom recently added you know meeting summarizers so they if they're building on top of llms their existing fullstack Engineers can build that specific problem now if they want to fine tune the model they want to bring the model in house bring the pricing down they'll probably negotiate with openi but in the worst case if they do want to build it in house is when they will hire you know a COR ml person but again probably you know contractually things like this so I don't know that answers your question of you know find I it does answer to it does answer my question more or less but it brings up a question you know being someone who's in the tech World for for like a decade or over a decade now I would say um why haven't you gone into the AI bulletin because you've been analyzing the market you've been into the you know economic and financial status of the market and I'm pretty sure you were aware of the fact that AI is going to eek because your your your circle specifically was pretty good I would say you were you were having a one of the top bunch at I right so what made you take a step back and look into web web 3 and you know blockchain I think not everyone from it is following one specific R if I'm honest uh if I'm being honest most of them are following you know trying to get into Fang um rarely of you are following you know hype markets like these and you know making sure they're making their way there it's either you go for a m or a PhD cryptography information security machine learning and then you eventually join the company there hope that the market has survived by then um or you know the market is doing well U or there are very few people like me who are you know changing markets left right and Center U for me uh why have I not done this I followed these markets for a while now for example web RTC followed by web3 I'm not saying I'm not following AI in fact recently I've joined you know U one of these a Rappers that's doing really well as a consult so not necessarily shying away from it that said U since I'm not working at the model there right I don't know I can't I can't extract as much value as I can being a core let's say smart contract writer because I probably have more value as a core smart contract engineer during the bear of the web three than a full stack engineer in AI during the bull so that's a high the learning curve of ml is fairly High compared to you know um let's say something like web3 orc resources are less very limited there's a circle of you know researchers which is why it's really good if you get in like that's sort of a filtering Benchmark for a lot of companies if You' have done you know some sort of research in machine learning unless you've done done that very hard to or you know compete with those people I mean yeah you make a point but uh why is it that if it's if it's so obvious that we need to get into research and if you do get into research that the pay will be absolutely amazing and you have a great future ahead even though this fact is established why is it that the market or the majority of the students in India struggle to find out the fact that okay I need to get into research I need to build my knowledge first work on uh you know cases which are compet which like ask me questions that I need to think in a unique manual for why is that this this mindset is not inculcated and I've been a follower of your videos since you were posting you know uh those sequential videos lectures so Le you had a proper pink and violet layout if you remember that three years ago I guess I've been following you since then and uh even when I look at your videos there haven't been a lot of talks with how research is uh you know giving you a lot of value uh so this entire culture is not inculcated in students and even from your point of view haven't really you know that path I mean it's a given that you are not into AI but why why is this factor missing what do you think it's a personal choice for people right uh for example when I was going for my masters I Accord to Tamu NYU one more Inu I forgot all of them were charging fairly High there was I had to take a loan for sure plus there was covid so there were a bunch of factors that made me not go there U it's not necessary everyone has to do research people can do well without it like entrepreneurship is one part being a engineer C someone is good part being an employe is also a great part and one uh there are a lot of factors that come to the picture like you have to be willing to bet on a market for 5 years for example I was talking to my one of my friends recently who did a PhD in computer vision he was telling me the problem statement I was working on U for five years after you know graduating from it bomb so 9ine years almost that problem statement can now probably be solved by llms so we were looking at it the wrong way we probably should have solved it a different way there's some risk that comes with u you know unless you love the problem deeply uh and you know want to follow it through don't care about mon outcomes as well considering you know if you're giving 9 years as a medical practitioner if you give 9 years you know there's a lot of monetary outcome at the end like infinite money at the end but that's not true for you know most research there's some money and if you do very well then it's more than it but I don't think this needs to be forced on everyone uh people are smart enough to you know figure out U whether or not they want to go into research also there are family situations sometimes you can't afford it if you go for a PhD also you know you're barely subsidized U so you're not living a super Lish lifestyle of course you can save some money um but if you look at your counterparts with working in fan you know they're probably making more there's a lot of uh compromise that you have to do initially uh which is not for everyone that's probably one of the reasons I did not go for it uh as I said I applied never applied for PhD honestly did apply for Masters nothing too specific computer science U goal was to move to the US honestly there's no other research well there a little bit of ml like one of my seniors who went for ML research in NYU is doing really well I started by uh ni company they raised like 80 mil if I did go to NYU he' probably you know be guiding me I was would be doing that that would have been a great part what I'm doing right now was a great part it's very hard to judge thankfully life is very single threaded so you know you don't know what would have happened if you went down the other part maybe my flight would have crashed what knows yeah everyone has like a personal choice yeah I mean I remember watching a video of yours and you mentioning the senior who was like into ML and you looked at him you talk to him and then you realized okay you know what the next buun is V3 and I'll get into that but uh don't you think the ml buun is still going on because that video came came out one year ago I guess this EML bullrun thing that you had with your senior that video came out one year ago or N9 months ago more or less and U your talk was like two years prior that or three years prior that I'd assume so you had that talk four years ago and ml buun is still going on I'd say it's at its peak and I can for sure say that it's going to be at its peak till 2026 so uh how long do you think this is going to run oh I can't predict that would be nice if I could but I there's sentiment shifts though right initially there was a lot of euphoria everyone was getting funded left right and Center now the companies that are getting funded are people who have found real use cases on top of AI who have real Revenue so when move towards business of course open has a metric all llm companies will do really well my talking about they're also building their own llm for companies that's why they're raising at whatever valuation they want but other than that you know um now the next set of value is going to AC or Investments are going to ACR to real businesses will build on top U so will it continue it will honestly the whole problem is whether or not you can find real use spes on top of these things blockchain be AI or you know something else um if you do then you know companies will do well I don't see ml going anywhere honestly it's much better much more sticky what's come out via LMS compared to what you know web3 sols or web RTC sols say so we'll see but of course this is going to do well for the next few years makes sense totally makes sense now uh you being a webc andb web web3 and also blockchain person your content is totally I mean it it it of course deals with uh these Technologies these Landscapes when we when we put out our link you know the link for registration to this and we put a small category saying ask your questions all of these students enrolled in this I madas degree make uh by making sure that they're going to be involved in data and AI okay but all of the questions or not all I mean 40% of the questions said uh is is this a good time to get into web3 is a good time to get into blockchain now these students mind you these students are in the remote degree and half of them are pursuing like two degrees like I am I go to an offline College in Hyderabad and then I do this simultaneously so it is through interest that they they took this in but they're still considering web 3 and you know blockchain because they are thorough Watchers of your content now what there's this fine line you know that you tread when you're a content creator that you should not misguide people you know uh you should not uh you know give the wrong uh idea to people and guide them through to an interest which they which they shouldn't probably look at of course number games number game is huge in India 15 LPA they set uh so when you post videos what kind of uh you know logic runs in your mind that okay I shouldn't probably put out the wrong agenda because these students are data science students and they're still giving questions like these so it's it's kind of a personal concern to be honest I think generally it's not just for your degree any degree people are interested towards course yes if you go to an IIT look at mechanical engineering people 50% 60% are taking coding jobs so it's not just that your degree has data science and people are looking at other CS options every degree you look at most people are looking at CS options getting placed in CS companies rarely are people you know actually taking core jobs um when you join a degree you're fairly young you don't know what you're getting into honestly uh when I was getting in I was struggling between it Bombay mechanical and RI CS pretty randomly I took RI CS but I know eventually I had CS was a good option I have core interest in CS it's also had a decent monetary outcome which might not have been the case at mechanical so if I join mechanical day still a very high probability I would wouldn't have done Justice to my degree would have gone for a CS job so I don't think there's anything bad around uh choosing a different path in CS or otherwise a lot of people in CS eventually become engineering managers PMS so on and so forth though it's very you're fairly young to decide I've done a data science degree so I have to stick to data science U I think it depends on a lot of factors including you know where do you think jobs are where do you think uh how long does it take to get a job how hyping the job is so on and so forth which is why people from all degrees flock towards CS not just this one uh how do I decide whether or not I'm misguiding people on YouTube I thankfully do not think about it uh all the videos that are coming out are pure natural what is happening in my life right now what I'm trying to build right now and I'm putting that out what kind of let's say money I'm making and putting that out U two people get influenced probably uh would that lead to a bad thing for people at it Madras uh data science degree considering that took a data science degree I don't know maybe so you guys should think before you know you decide looking at any content creator H if he's getting into X should I get into X or not do your own research and then you know decide based on that makes sense makes sense and you know the journey for like getting into Cs and also like into and film filming videos putting them on YouTube uh it it it was an exciting one clearly because I've seen you since you're what 55k follow 55k subscribers or something and early subscriber it so uh I'm sure the journey was like amazing and throughout this process I've seen you change your stance a lot of course I mean opinions change especially in the Cs landscape that is totally understandable now one of the most controversial opinion of course considering the Indian market is the DSA one there were videos saying DSA okay no and then videos saying okay DSA you know what maybe and then there's a super 30 now uh in the middle there was a thing saying okay you need to so all the I guess we got like 60 to 70 questions on this just just address that one one thing for like for a minute what do we do for DSA uh in super 30 I've seen two people fail interviews because of DSA only they were very good at web 3 but let's say the company asked DSA and then they so we had to start DSA classes then we saw no one here ised in DSA they're all here for death we stop DSA classes and then whatever referrals are happening right now are happening based purely based on dev should you do it there were interview where you will need it for example for me my very first remote job they asked fairly solid BSA also you guys are in college so you have a lot of time to explore you can do a bunch of things two degrees like you are I don't know why you're doing that you know you're not able to manage DSA but personally I've seen for example you good colleges people do ICPC good sort you know your DSA sorted for a while you have to practice before your interviews U for most the common answer is can do it it's just another two hours every day you have to give U and should not avoid it uh for extremely you know for example you you said you want to go into research so does it make any sense for you to do DSA probably not because similarly there are people who know I want a remote job a very high growth remote job where I get selected based on my skills and nothing else but then you know you don't need H I mean okay coming to the fact that I'm puring two degrees I realize them into data and math a bit too late my original degree deals with blockchain cyber security and IOD and uh I I study blockchain and cyber security daily while I you know go to college but it doesn't involve data and math so I'm not enjoying it as much so I thought you know what I'll just try this out and it worked out for me so I thought okay fine and then it led to research so I'm glad I took this decision uh moving forward though uh when you look at you know DSA there are a lot of Landscapes to look at but uh as someone who is into tech for so long and uh you must know that there will be newer technologies that are coming up for sure I mean it it is an analy cycle at this point is it essential for us to stay in the loop especially with with llms man I mean like every week we have a new announcement from from open AI every week every month we have a new announcement from Gemini anthropic comes up with something insane and then suddenly the CEO changes his drama so is this like uh should we keep up with this or should we just leave it all alone work on our Concepts you know do our research or do are corporate and move forward what do you suggest so so when you say should we focus on this do you say should we focus on the standard DSA or should we focus on the funding anoun yeah should we like focus on standard DS I mean that is again of course a personal opinion for sure but the kind of Bull Run that AI is in the kind of announcement that you're getting on it on a weekly or monthly basis it's insane I mean the growth is insane and if you look at the Nobel prizes that the people won I'm not sure if you're aware of this or not all three in maths physics and chemistry dealt with AI there were AI applications in their respective fields that that shows the kind of influence AI is going at right now and should we like keep up with the pace that AI is moving at is it important to keep it up because when you are in your youngest ages I'm sure there were Technologies which were coming up even in web development or web 3 or blockchain and were you keeping up with the latest you know technologies that were kept that were like bringing forward or did you just mind your own business short answer no I wasn't um there in it as I said there's a very standard what everyone is doing which back then at least was everyone was going to Google us uh everyone was following the path of getting to Google us that's where your invion was limited right there was some people Bombay has a culture of research thean was focusing on Research over there I would doubt they were also looking at funding announcements they were looking at CV uh you know computer vision papers that are coming out so on and so forth so the fancy funding announcements or you know who's getting hired whereare or llm came out um I think more than that smart people focus on what research underlying research is coming out U that will eventually maybe lead to something a better you know 10x outcome like llms did chat GPT something new for sure like everyone tries it and gets allowed what is that next thing that is coming I think most people are focusing there so rather than looking at you know funding announcements and these things if you want to go for research you should focus there research paper for me never did that honestly was going through the but nothing outside that had friends who had to go forar fair enough as much I was allowed to like keep talking to you I think uh over to you all right uh I will come in in a bit again uh hello hello [Music] sorry uh so uh we were talking about our uh curriculum so can you please uh give your review on our curriculum uh yeah yeah yeah I haven't gone through it yet well a little bit I did um looks like a lot of it is your your videos are just on YouTube and also in your Erp is what I could tell I looked at some of your YouTube videos uh it's decent like it's much better than you know traditional College honestly uh or you know whatever TI three colleges U is IT industry oriented probably like Pro again this is fairly relative but compared to other colleges it's decent U also I haven't looked too much into it so you know I'm not the best judge right now we have foundational course which talks about I mean which teaches about mathematics and statistics a little bit in um English also yeah and do we have a little bit of programming python so tell me this what when you when you say little bit of programming in Python how deep are we going here uh this is the foundational course which means it is like stepping stone it is like a basement so this just deals with everything in uh in a very very basic level so from there we are taking to I mean we are taking us in a diploma level which deals with programming completely so here we have pdsa dbms mad projects Java system commands and then uh there is also another diploma degree here which is for specifically called data science where we have six courses and two projects and other diploma also had two projects and six courses as you can seen so it deals with MLF uh machine learning is split into three different courses here foundations techniques and practice we also have a project on it and there is BDM uh here we actually go and talk to a business and do a project analyze your data give them some suggestions that kind of thing happens that is the project and how to do it is the course and uh business analytics and tools and data science are related to the same thing so and then comes the BS degree so here we have two parts we have to choose from this one or this one which is related to uh deep learning and Ai and the other is related to software and software testing engineering and testing so there are some elective courses me sorry I was scrolling too fast these are some some of the elective courses we can [Music] choose is it all right uh I've seen this before uh it's very good uh I mean compared to a different colleg is pretty good uh is there scope for improvement there always is uh but you know love it and and uh I think the bigger problem is this not just proof for you guys like in any college people are rarely learning anything from you know the whatever classes you learn from the community and the problem with you guys might be you guys mean don't meet enough because most people if you ask so you know that might be missing U cabus wise of course it is much better than you know what you would find in a traditional College people are sleeping in a traditional College you guys have an Erp just studying one day before the examination um which is also fun so it seems like you know whatever you guys are paying for you're getting some value from it you have you can learn from it are there better resources maybe for learning the same things from you know maybe yeah that's that's a high level but you know happy to dive deeper uh one thing uh there's a better question that I wanted to ask why um how do you see the BS program like after the promotion that uh she did how do you see uh the BS program for the students that are pursuing it I would say one thing think of it like any other BC or BS program um do not associate yourself you know at all to a colleagues and I say that to everyone even from go to it offline okay you know your degree is your people who the people that you found around you and you know who are now doing really Val because usually colleges if you look at a lot of coding YouTubers which are they from they're all from D because start follow what you guys should really look for is this you know people possible this is having similar outcomes at the out of ID Delhi so what is that thing that you guys are doing like or you're going to do Consulting will probably be the first aliz from from this online program um that's something we'll figure out in a few years U but is that possible in an online degree uh yeah I've seen your stats lot lot of people go to level two there's less you know pressure on you guys to graduate everyone passes together everyone very Clos net I'm seeing you the age Gap is fairly big and you know some are very lower going to level two are going to level three so it's a good experiment and you guys are you know the first batch probably one of the first batches will come out the experiment so I'm excited to see what comes out of it but yeah very early to tell okay you know whether it will be better than let's say you know a real it col what a real this a real it college but onsite I or onsite tier three or onsite tier say better time will yeah I just like to but in here I mean really resonated with with the point on College like bomb is going to research that he's going to entrepreneur um so a couple of points with the online degree obviously because it is online you don't meet people obviously the guys at Chennai few of them do this full-time so they they have a community they have a desha they have a i and Research Park they have a couple of professors they work with they have the entrepreneurship and they have the library of ss right um but that's not there for the people online so that's number one uh number two harking back to a point previously about how personing two degrees might be stupid I don't think it is because with with this with the online degree you get the tag of the stamp of IIT on your resume you get like a tier three college and the I on your resume as well as you get I mean you get the content but then you can get the content from other YouTubers as well but I think the major thing is uh first of all the stamp secondly like this Society this entire meeting we're doing today is is a part of this huge Community Driven effort to get people together otherwise 174 people on this call wouldn't be together if not for the society so I I just wanted to share a few thoughts I mean no no particular question at this point great point I think there's no downside if you can manage two degrees you should do it U my yeah yeah I agree because not not the worst idea in the world if if you're on tier three college but does it really give you you know the credibility of an i is my question it's very easy to get in that is that is exactly so I talked to a professor from Colombia and I put IIT on my resume uh so he thought I was actually offline IIT uh and then I'm like no it's on BC degree and we're doing it and it's but it's official and all I have ID card and it's like oh no you should put online on it so I I think message for the other 1 170 people there on the call I put a bracket online on it you know but I share a story here when Yash came to the office for the first time he was wearing an IT madas degree sorry t-shirt I was like cool it madas then he said and from online I like degrees associate fake it till you make it I think is the vi it'll hurt you really bad if you use fake it make it if you you know eventually someone realizes agre High L advice for people here speaking of uh you did mention about offline iatm I and online I uh we are doing it in online so we do have this kind of foro because uh most of the people here are either working or they are having their own ENT uh they are entrepreneurs and they have their own companies or uh they have dual degrees and I I have come across so many people and I myself uh I'm doing it as a standalone degree so we really feel foral and I not even in Chen so I feel very left out and I don't know uh how we should come come over this thing and uh we do want to become someone who is worthy of the name iidm so we are kind of in place where we don't know what to do and we do have some people to look up to but we're not sure how we are going to make it or it's kind of what should I say maybe cat and a all kind of situation here interesting so I think the phone that you're feeling is probably no different than someone from a tier three college or a tier 2 college would probably feel similar um how do you hold up to the it name I think one good thing that happens in this program is you know um the number of people who get out is really less like I saw the numbers and you know it's very hard to pass out there's something there like some level of respect you'll get if you know you're able to finish the degree uh so I wouldn't say it's you know completely uh someone would deny can you go to it Madras an online degree if you finished the it Madras degree considering I've looked at the stats I would respect you it's a hard degree to go through you know they Grill you really bad I know anyone can join but not everyone can graduate so that's one and two it goes away with time and years you know uh I did not make it to whatever in xyc but you should you should you guys should meet offline just say 174 people this happens on steroids when you're on campus you're every day having you know meetings like these smaller groups yeah yeah that is super important nothing else onl but if you if you filter out the smart people and put them together they 100 times better than taking smart people but keeping them in their houses so one thing you guys can figure out somehow consider your society and you know um you guys are saving a lot on your fees also so why not spend a little on meeting I don't know Goa meet up Bangalore meet up and you everyone sort of reaches them yeah that is happening actually but uh it's not that everyone can go and meet on the specific time in so there are some uh drawbacks one more thing by which I wanted to ask is um um which I wanted to convey is that to all the people um one of my cousin was watching anime okay so I told him that oh you are watching cartoon so I I did that because I wanted to see the reaction so see U here's the agenda If he if he wants you know uh to prove that anime is not a cartoon he will be in the aggression or he will be in Lost Sensation that no no it's not a cartoon thing it's a anime or it's a different kind of class but he was not about that okay uh he said that okay no worries uh it's it's a cartoon for you it's a anime for me let's be Let's uh you know accept that so here's this thing for all the online people who are you know in the it Madras if you are know defending that I'm I'm in know online you are doing the wrong things on the on that step itself what's your call on this yeah I mean be Elling towards it bro like someone saying let them say I mean if someone is taking out the time and beting your degree or spending two minutes thinking it any ITB let them be people will say whatever want what difference will it make in your Liv arguing with them or agreeing with them we can do it as about that so um we cannot let like people openly ask because so guys like we got your questions We shortlisted Them Say set them into proper questions so we'll be asking soon since you have only like 12 more minutes we'll we'll get can I ask something not really please uh try to like you know keep it contained we have your questions here don't worry we'll cover everything which is which is repeated which was important so sh start so uh so the first question is there are two categories here so uh first category is people who are who are knowledgeable I mean they do have some coding uh knowledge but they don't have any formal degree in the coding aspect as not they not from the computers background so they want to get into Tech and the other situation is that there are people who don't have any work experience they are from uh Tire three college and they have a age Gap and they are doing this degree and they want to get into Tech so what advice will you give for these two category of people you're saying most people here are in these two categories madas this degree is not going to help you too much help you crack some prerequisits in a Google or somewhere else you need to work extremely hard on your skills you anything you work really hard and do DSA Dev and get really good at it and second choice is you get into a degree and chill you take the first one to that less paid or free thing of looking at a lot of videos and working really hard should be your goal that's 99% of the effort a degree will help you 1% here are some prerequisites and sure I'm sure there are placements and other things that you get from you this needs to be done even more for people are from nonch makes sense I mean yeah that totally makes sense I mean the next question though uh people most of the responses were were likey most of them are in the first year offline College Med dual degree people uh they're like okay there are multiple Pathways for me to look at they came into this thinking key half of them came into thinking madas well enough good enough for me but now they're like okay there are hackathons there's compet programming and then there is you know increasing my knowledge in data or AI should I like go into research as I asked and uh when you are already occupied with two degrees one offline one online and there's a lot of things laid out for you what is your suggestion like like is there any other option that you can suggest colle course have to manage a lot of things U yeah to drop out from one would be first advice U I don't think I mean unless you find something very good in your degree I don't know um but within a year figure out this one makes more sense for me and and you know probably because handling academics handing making sure you're passing through your examinations considering it's so hard you'll spend all of your time there only and what with what final outcome um is that really worth it maybe I don't know man I mean everyone has their own situation so think about it I I think you have to spend a lot of time working hard yourself on things that are happening in the industry which is most probably not being taught in your col makes sense now the next question though uh do you think decentralizing AI is possible from like a blockchain perspective out of honestly I mean this is no can try lot of people have tried it lot of somec money there but no one has been able to figure it out I think web AI exists very well on web2 traditional systems web3 is all something else merging them people have tried no sticky use case until now no stick use Cas until now I mean would you suggest people exploring though like your personal suggestion no makes sense now uh so many questions and I mean it so many questions asked about chok it is the Talk of the Town right now in CS for all the tech people and uh it is it goes without saying most of J is for web developers people with JS skills or like you know Java now dealing with data and AI most of the people like here what is your advice for people for G in data and specifically so many questions on yeah generally I would say don't stick too much to a degree as I said don't sck yourself you should look at standard things that you know lead to sometimes research outcomes sometimes ICPC sometimes D sometimes a Google job sometimes a startup job sometimes open source so said most of the companies uh do come in web def python JavaScript Java the three common stacks and this is look at college and judge it the best way to Jud iture um so you guys should do it you know you'll probably as I said be the first ones full pioneer when I was at it RI there used to be five people or six people who got into G by the time I graduated we did 42 we were the highest college you know doing G you should do that next madas online is 50 selection you have so many people but you know the real people you have to figure out what the bestes andus that makes that makes sense I mean conc on the standard stuff to while moving forward with your own with your own uh things so guys to a suggestion by the man himself make sure that you Explore More don't restrain yourself to data and AI sounds good anything else sounds good that's that's pretty much it yeah thank you thank you so much congratulations [Music] here"}], "Stanford CS236: Deep Generative Models I 2023 I Lecture 3 - Autoregressive Models": [{"content": "all right so let's get started the plan for today is to talk about Auto regressive models which is going to be the first uh type of first family of uh generative models uh that we're going to uh consider in the class this is the kind of technology behind large language models things like CH GPT um so yeah just as a recap uh remember sort of like this high level overview whenever you want to train a generative model you need data so samples from some IID unknown probability distribution P data and then uh you need to define a model family which is going to be a set of probability distributions uh over the same space over which you know your data is defined mind and uh these probability distributions are typically parameterized somehow um for example using uh it could be conditional probability tables in the case of a ban Network as we have seen in the in the last lecture for the most part we're going to be thinking about probability distributions that are defined in terms of neural networks so you can think of theta there in that picture as being kind of like the parameters of uh the neural network that you're going to use to define Define this probability distribution and then you're going to Define some sort of notion of similarity or Divergence between the data distribution and your model distribution and then we're going to try to optimize the parameters of the neural network to make your model distribution as close as possible to the data distribution uh the caveat being that you only have access to samples from the data distribution right so you don't know you can't evaluate the probability of an image under the data distribution the only thing you have access to a bunch of samples and uh once you have this probability distribution then you can do several things you can sample from it uh so you can choose a vector x with probability uh you know there's many different axes that you could choose from each one of them is assign a probability by your model and you can choose one uh with the the probability uh according to this probability distribution you samples from it uh and this is what you need to generate new data uh we're going to be interested in evaluating probabilities um for several reasons one is that evaluating probabilities is useful for training the models so if somehow you have a way of figuring out How likely is any particular image according to your model then that gives you a pretty natural way of training the model kind of like solving this optimization problem of trying to find the point that is close as possible to the data distribution and one way to do that is to just do maximum likelihood you can try to find the parameters of your model that maximize the probability of observing a particular data set the other thing you can do if you have access to probabilities is you can do things like anomaly detection so you can given an input you can see you know is this input likely or not so kind of like but we discussed in the last lecture one advantage of generative models compared to discriminative models is that you can reason about the possible inputs that you have that you might be given access to so you might for example try to detect adversarial examples uh because perhaps you know they they are different from the kind of like natural images that you've used for training your model and so if your generative model is good you might be able to identify that something is odd about a particular input maybe the likelihood is lower than it should be and so you can say okay this is this is perhaps an anomaly maybe I should I shouldn't I shouldn't be very confident about the kind of decisions uh or the kind of predictions that I make about this particular data point and as we discussed another thing you can do is potentially on supervised representation learning uh and so in order to do well uh at learning a good a good approximation of the data distribution you often need to understand the structure of the data and so in some cases uh it's going to be a little bit tricky for auto regressive models which is what we're going to talk about today but for other types of model models is going to be pretty natural there's going to be a pretty natural way of extracting features um as a byproduct basically of training a good generative model so the first question uh is kind of like how to represent this proability distributions uh so how do you define this set in a meaningful way and today we're going to talk about Auto regressive models right which are built on the idea of using chain rule essentially and uh next we're going to talk about how to learn it so recall that U there is this General result that you can take any probability distribution Define over a arbitrary large number of variables n and you can always Factor it as a product of conditionals so if you have four random variables X1 through X4 uh you can always write it down as the probability of X1 the probability of X2 given X1 and so forth and uh this is just fully General you don't to you don't need to make any assumptions on the on the distribution every distribution can be factorized this way exactly and in particular you can also use any ordering you want so in this case I'm factorizing it based on the ordering X1 X2 X3 and X4 but you could choose a different ordering so you could decide you could write it down as the probability of X4 times the probability of x3 given X4 and so forth and here you start to see that yeah in general you can always do it but perhaps some orderings might be better than others um so if there is some kind of like natural causal structure in the data then perhaps modeling the data along that direction is easier but chain rule doesn't care it works regardless of whatever ordering you you you're going to use uh baset essentially exploit this uh this idea and uh they make progress by basically simplifying these conditionals so we've seen that in general representing even when the random variables are discrete representing those conditionals as staes doesn't scale doesn't work and so B net ban networks essentially make some kind of conditional Independence assumption they assume the certain things are conditional independent from other things and uh and then that gives you potential simpler factors that you can represent as tables and the other way to go about it is to use a neural model well instead where instead uh you're going to give up on the tabular representations it's no longer a lookup table now it's going to be some kind of function parameterized by a neural network that you're going to use to uh map different kind of uh assignments to the variables you're conditioning on to parameters uh for the conditional distribution over the the the the next variable in this ordering that you're using so in this kind of neural models what we're going to do is we're going to start from chain Rule and then we're going to try to approximate the true conditionals uh using neural networks and this works to the extent that the neural network is sufficiently powerful that it can well approximate the conditional probabilities which could be potentially very complicated if you think about those as tables there could be really complicated relationships between the entries in the table and this kind of factorization using neural models Works to the extent that the neural network is sufficiently flexible that it can capture the structure of what you would get if you had a you know a fully General tabular representation and uh the good news is that uh efficiently deep neural network can in principle approximate any function and so that's kind of like where the magic of deep learning comes in if you can use very deep neural networks there's a good chance you might be able to actually come up with a decent approximation to these conditionals and that's why this these models tend to tend to work in practice so remember that you know the the Machinery we're going to use is going to be the same as the one you use using regular let's say classification so you want to predict a binary label give it a bunch of input features uh you just care about the conditional distribution of a single variable given a potentially large number of other variables but the important thing is that you're just trying to predict one thing at a time a single variable Y and so you can use things like logistic regression or Ral networks to do this kind of things and uh in particular we've seen that logistic regression is kind of like assuming a relatively simple dependency between the values of the covariates x or the features that you're conditioning on and the conditional probability of Y given X it's basically assuming that there is a linear dependency that then is fed through a sigmoid uh to get a a non- negative number that has the right kind of like normalization and uh you can make things more flexible by assuming some kind of nonlinear dependence and there that's where you use NE networks right so you can take your inputs act you can transform them by applying linear Transformations nonlinearities you can stack them in any way you want and then at the end of the day you still have some sort of transformation that gives you the parameters of this conditional distribution over what you're trying to predict given what you have access to and so maybe at the end you use a some kind of sigmoid function or a soft Max function to to basically normalize the the output to a probability distribution so it's more flexible you have more parameters which is good because the model you know you can capture a richer set of dependencies between the variables the price you pay is that you have more parameters to learn you need more memory and you might imagine that you might need more data uh cool so that's the building block and then basically the whole idea what Progressive models is that once you know how to predict one thing using a neural network you can kind of like combine them and you can always think of a high dimensional output let's say uh an image as a number of individual components and chain rule gives you a way of predicting individual components given the the previous ones and so then you can plug in your neural network to get a generative model and that's what neural Auto regressive models do right so for example uh let's say that you wanted to learn a generative model over images so just for Simplicity let's say that you wanted to work with a binarized mnist so mnist is kind of like a classic data set of handwritten digits um so that if you binarize them so that every pixel is either zero or one black or white um then they might look like this so you see that they kind of like look like handwritten digits and each image has uh 28x 28 pixels so you have 28 * 28 random variables to model and uh the variables are binary 0 1 Black or White and the goal is to basically learn a probability distribution over these 784 random variables uh such that uh you know when you sample from it they that you get hopefully look like the ones that you have in the training set or that in other words you're hoping that the distribution that you learn is a good approximation to the data distribution uh that generated these samples IID independent identically distributed samples that you have access to in the training set and again this is challenging because there's a lot of possible images you need to be able to assign a probability to each one of them and so uh recall the recipe is uh you define a family of probability distributions parameterized by Theta which we're going to see in this lecture and then you define some kind of learning objective to search over the parameter space to do some kind of optimization reduce the learning problem to optimization over Theta over the parameters that Define the distribution to try to find a good approximation of the data distribution which is going to be the next lecture uh so the way to use an auto regressive model to Define this probability distribution is you first need to pick an ordering so remember if you want to use chain rule you have to pick an ordering and for an image is not even obvious what the ordering should be um there is not an obvious kind of causal structure like you're not modeling a Time series where you might expect that you know there is some causal structure and maybe predicting the future the past is easier than going backwards but any ordering Works in principle and so for example you can take a raster scan ordering and so you can go from um top left to bottom right you can order the 784 pixels that way and then you can apply chain rule to this probability distribution and so you always you know that without loss of generality there's always a way to write down this distribution that way basically as the probability of choosing an arbitrary value for the first random variable then choosing a value for the second given the first and so forth and so that's how you break down a generative modeling problem that is tricky to a sequence a small number of classification regression something we know how to handle each one of these conditionals is only over a single random variable and that's the kind of setting you know how to deal with from or you typically consider when you think about classification regression those kind of problems and uh you know uh you cannot do tabular form so a ban network is is out of the question here and so instead we're going to try to basically model these conditionals using some kind of like neural model some kind of functional form that will allow us to map the different config figurations of the pixels we're conditioning on to a probability distribution or the next pixel that we need to to work with in this particular ordering that we've Chosen and uh so in particular I mean if you think about the first uh probability distribution you know you can represent it as a conditional probability table that's just a you know binary random variable you just need one parameter for that so that's why I'm saying pcpt here means that you can actually store that one set cly uh but the other ones become complicated and so you kind of have to make some sort of approximation and one simple thing you can do is to just lose logistic regression so you can try to use logistic regression to basically predict the next pixel given the previous pixels and that gives you a generative model basically and uh if you do that notice that you don't have a single classification problem you have a a sequence of classification problems like you need to be able to predict the second pixel given the first one you need to be able to predict the third pixel given the first two you need to be able to predict the last pixel the one in the bottom right given everything else so all these classification problems are basically different and separate do you even have a different number of covariates or or or Fe variables that you're conditioning on and so in general you're going to you can potentially use different parameters different models for each one of them and this is kind of like what I'm alluding here there is a different Vector of coefficients Alpha for your logistic regression model for each classification problem and uh so more explicitly for example you would have the first uh prior distribution over the first pixel which is just a single number it tells you how often do you choose the first pixel to be white versus black so if you think about the structure of these images you know the top this pixel here the top left is almost always black so you probably would want to choose this number to be to be close to zero assuming zero means black sort of like you want that pixel to be often black uh and then uh you know you need to be able to specify a way of predicting the first pix the second pixel again the first one and you can do it using a simple logistic regression model and and so forth right and uh that you know that's a modeling assumption whether or not this type of generative model works well depends on whether or not it's easy to predict the value of a pixel given the previous ones in this particular arbitrary order that I've chosen for the pixels and uh you know whether this works again depends on how how good this this how good this approximation is so it might work well or it might not work well because maybe these dependencies are too simple maybe regardless of how you choose this Alphas there is not a good way of figuring out how you should choose the the value whether or not a pixel is white or black in this case and uh but you can think of it as an AO regressive model and that's what because essentially what you're doing is you're trying to regress you're trying to predict the the data PA the structure of the data itself right so uh you're regressing on yourself like you're trying to predict parts of each of each data point given other parts of the data point and uh that's you know this kind of a modeling assumption has been tried before um this kind of model is called a fully visible sigmoid belief Network it's kind of like a relatively simple uh early type of generative model that as we will see is not going to work particularly well but it's kind of like useful to to work it through so that you get a certain level of understanding of exactly what it means to model a joint distribution in terms of a simple kind of like classification models so when you think about what we're doing here when you think about chain rule uh we have all these individual pixels that we're modeling conditionally and all the ones that come before it in the order and so when you model the probability of XI given all the variables that come before it in the ordering let's say using a logistic regression model uh you know you're basically outputting the conditional probability of the pixel being on or off given what you've given the values of the previous pixels and uh we're often going to denote this using this symbol here x minus I smaller than I uh which basically means given all the indexes I that are strictly smaller than than all the indexes J that are strictly smaller than I and uh which you know in the case of logistic regression that conditional probability is given by this relatively simple expression linear combination and then you pass it through a sigmoid now how would you evaluate you know if somebody gives you a data point and you want to know How likely is this data point according to my model which is the kind of computation you would have to do if you want to train a model by maximum likelihood how would you how would you evaluate that joint probability given that somehow you have all these values for for Alpha so what you would have to do is you would have go back to back to chain Ru so you will basically just multiply together all these factors and so more specifically you know the the first pixel X1 will have a value well I guess here I have an example with the let's say imagine that you only have four pixels uh there's four random variable and let's say that we are observing the value Z one one zero um then you basically need to multiply together uh all these values which are basically the predicted probability that a pixel takes a particular value given the others and these predicted probabilities depend on the values of the previous pixels in the ordering right and so they depend on so X hat I which is the predicted probability for the I pixel depends on all the pixels that come before it in the ordering so a little bit more explicitly it would look something like this um where you would have to compute the conditional probability of the second pixel when the first pixel is zero you would have to compute the conditional probability of the third pixel being let's say on in this case given that the previous two are zero and one and so forth and then you would basically replace that expression here for xat with the standard sigmoid logistic function thing and that would give you the the number how would you sample from this distribution so let's say that somehow you've trained a model and now you want to generate images according to this model the good thing about an autoregressive model is that you can basically it also gives you a recipe to sample from it like in general it might not be obvious how you do this like okay you have a recipe to evaluate how like different samples are but then how do you pick one with the right probability right so would you randomly generate one image probability and then do some sort of rejection sampling you could do things like that uh that seems you could use generic kind of like inference schemes if you have a way of evaluating probabilities you could try to you even brute force and kind of like invert the CDF and try to do something uh something like that that of course would never scale in to to the situation where you have hundreds of random variables the good news is that you can basically do it you can use chain rule again and kind of like decide the values of the pixels one by one so what you would do is we know what is the prior essentially probability that the first pixel is on or off and we can just pick a value for the first pixel now once we know the value of the first pixel we know how to figure out a value for probabilistically for the second pixel so we can plug it into the previous expression you could do you know something like this just to be very pedantic you have there's some prior probability and perhaps you always choose it to be black because all the images are like that but then you pick a value and then you basically sample the second random variable given the conditional distribution and this conditional distribution you can get the parameter by fitting it by using this expression so the the logistic regression model will try to predict the second pixel given the first one and uh you're going to get a number from this and then you can sample from it then you can pick you know you're generating two you have two pixels now that you've chosen values for then you can fit it to the next logistic regression model and you can keep generating the image one pixel at a time so that's the recipe and and it's good news because uh you know sampling is to some extent uh easy I mean it's uh not great because you have to sequentially go through every random variable that you're that you're working with but it's better than Alternatives like having to run out you know using Marco chain mon Carlo methods or other more complicated techniques that we might have to resort to for other classes of models the good news is that for these kind of models sampling is relatively is relatively easy conditional sampling might not be so if you wanted to sample pixel values uh based on you know if you wanted to do in painting because you have some you already have a piece of the image you want to generate the rest depending on what you know about the image it might be easy or it might be hard so it's not straightforward the fact that you can do this efficiently is a nice benefit of these type of models okay now how many parameters do we have so you know we have a bunch of alpha vectors these Alpha vectors have different lengths because there are different they are logistical regression models of different sizes basically any guess like for this model that's say two parameters and this one three and then four then five it's uh in n squ like it's n squ one plus roughly n squ right so you know potentially not great but maybe manageable cool now as as I kind of mentioned before this doesn't actually work particularly well so now I don't have the results on Mist but if you train it on this data set of uh the CCH 101 so the samples are on the left and you can see that they kind of have shapes like there is like objects of different types and then uh you know you can kind of train this simple model based on logistic regression classifiers then you can sample from it and you get this kind of blobs so not great and the reason is that basically the logistic aggression model is not sufficiently powerful to describe this potentially relatively complicated dependencies that you have on the pixel values so how can we make things more comp better let's use a deeper neural network right that's the that's the natural thing to do um when and if you do that you get a model that is called n Neo regressive density estimation and the simplest thing you can do is just use a single layer Neal Network to replace the logistic regression classifier and so what would it look like uh basically what you do is for every uh index I so for every pixel you take all the previous pixel values and you pass them through uh first a linear layer then some nonlinearity and then uh uh you pass the nonlinearity um what you get these features this vectors that you get through a logistic regression final output layer that would give you the the parameters of this B random variable so it will tell you how whether or not what is the probability that the I pixel is on or off and as you can see now we have a slightly more flexible model because uh you don't just have the alphas the parameters of the logistic regression classifier of the final layer of the network but now you also have the the first layer so you have a slightly more flexible model and uh and so it would look something like this so you would uh and again the issue here is that you you know you have if have n random variables you have n separate kind of classification problems and so in general you would you could use completely sort of like decoupled models and so the first model would have let say a single uh input X1 and so the the shape of this Matrix would be just a column Vector basically and then if you have two inputs X1 and X2 to predict the third pixel then this Matrix would have two columns essentially and and so forth and uh yeah do we have Sig like why do we have a Sig over H and then Sig over like the second Sigma makes sense but why do we have a sigma for H I don't think don't necessarily have it to have it it's just yeah here I'm having an external linearity there but yeah you don't necessarily need it yeah if you don't have that Sig wouldn't just lar layer yeah so it's better to have linearity yeah but you know this is just for illustration purposes you could imagine different architectures different doesn't have to be a sigmoid could be a Ru could be other things it's just yeah um so over here you have three rows in your a matrix like are we trying to predict three separate features for why I thought it was just one probility um I have oh I see what you mean so this is just like the there's basically a hidden Vector H which could have it's not necessarily a scalar that hidden Vector is then passed to a logistic regression classifier and so it's then mapped down to a scalar through this uh expression here which might be so there's a DOT product there right and so this you know in principle all works but you can kind of see the issue is that you are basically we're separately training different models for every pixel which doesn't seem great perhaps there is some common structure at the end of the day we're kind of like solving related problems we're kind of like trying to predict a pixel given part of an image given another given the previous part of the image and so there might be opportunity for doing something slightly better by tying the weights to reduce the number of parameters and as a by product speed up the computation and so what you can do here is you can basically tie together all these matrices A2 A3 A4 that you would have if you were to think of them as separate uh classification problems what you can do is you can basically just have a single Matrix and then you kind of like tie together uh all this uh the the weights that you use in the prediction Problems by basically selecting the corresponding slice of some bigger Matrix right so before we had this the first Matrix that we used to call A2 and then A3 and then A4 and they were completely you know decoupled you could choose any values you want for the entries of those matrices what you can do here is you can basically choose the first row the First Column to take some uh set of values and then you're going to use that for all the subsequent kind of like classification problems so you're equivalently kind of like trying to extract the same features about the first about X1 and then you're kind of like going to use them uh throughout all the classification problems that you have in the in the you know when you're trying to model the full image yeah um is reducing overfitting also motivation for this yeah so the question is reducing over over is you know overfitting also potentially a concern yeah reducing the number of parameters is also good uh for uh overfitting issues tying together the classification problems might be good uh you might learn a better solution that generalizes better and as we see it also makes it faster I'm curious like empirically it makes more sense to invert your X's you're saying like you always depend the same way based off the last thing you predict instead of like saying the n term should have the same weight for X1 for example uh what's the suggestion sorry I didn't quite so I guess over here we're always multiplying the first W1 X1 W1 X1 for every single x i we're predicting instead of that would it make more sense to invert your X's so that W1 looks at X IUS one W2 looks at X IUS 2 and so on and so forth you're just looking at one preceding entry two preceding entries and so on oh that could also work yeah that's a different kind of parameterization that is more like a convolutional kind of thing I would say that we're going to talk about that too this is what they did in this particular model a question about notation what is the w dot uh comma smaller than 9 mean what is the DOT it's just D Matrix yeah I don't think yeah probably didn't need to dot or I guess it means the piece of a bigger Matrix I think that was the intended notation but yeah you got the idea sure uh and the good news is that this can reduce the number of parameters so if you have size d uh for this hidden Vector H that you're using to uh make the predictions how many parameters do you need 2 * uh it's not longer quadratic in N that's the kind of big takeaway before we had something that was quadratic in N uh now it's basically linear because there's b a single Matrix that you have to store and then you canate reuse it all the time right um so that's good um now the other advantage that you have with this kind of model is that you can evaluate probabilities more efficiently uh because basically whenever you go remember if you want to evaluate the probability of a data point you have to evaluate all these conditionals so you have to go through every conditional and you basically have to evaluate the kind of computation if there is no structure on the matrices and you have to redo the computation because there is no nothing shared but if you have some sh shared structure and you can kind of like reuse the computation so if you've already computed this dot product this product here this Matrix Vector product here and then if you are adding an extra uh column then you you can reuse the computation that you've done before you can just add in an extra call is the is factor C also shared amongst all the hidden layers yeah I guess it could be or it doesn't have to be I think you could you could make it either way yeah I think I actually forgot to because it didn't fit but yeah you would have there should be a c and you could change it yeah are the um W columns like updated in each step so like in the fourth step here it also updates the first and and second column uh what do you mean update like minus setting was with the weight Matrix you basically you build it um column by column and but you try to learn right like over seeing like many examples so I was wondering like as a model learns it basically updates also in every step all the previous columns that it has learned right yeah yeah yeah so it's all tied together and then we haven't talked about how you would do learning but yeah so then you can see that kind of like the First Column matters for all the prediction tasks so you would be able to learn it you would get some signal from every sing learning problem yeah yeah yeah just to clarify um in this model you are sharing weights uh right yeah and does that uh imply any like assumptions you're making about the data you're looking at there is an assumption again you're kind of saying that these conditional probability tables we could could be arbitrary somehow can be captured by prediction models that have this sort of structure uh so somehow that there is some relationship between the way you would predict one pixel different pixels in an image whether or not it's reasonable it's it's it becomes an empirical question uh I think I have the results here and it tends to work significantly better than let's say uh the previous logistic regression model so it does seem like this kind of structure helps modeling natural images or toy kind of images like amist um and so here you can see some examples you have amist binarize the oh no actually I don't have I don't have the samples from here what you have here is samples from the model train on Mist on the left and the conditional probabilities corresponding to the samples on the right so remember that when you generate samples Auto regressively you actually get probabilities for each pixel given the previous ones and then you sample from them to generate to actually pick a value and so the images on the left are binary 01 the images on the right are kind of soft because for every pixel you got a number between 0o and one that then you sample from to generate a color in this case 01 and so you can see they kind of look a little bit better because they are a little bit more soft but that you can see that it's doing a reasonable job at capturing the the structure of these images what are the numbers on the like on right uh on the right a table look just almost exactly like that one why aren they why don't they just create some variation I mean some other kind of they are so the numbers are corresponding to the to the samples that you see so basically what this is saying is that what what you would actually do when you sample is you would take the first pixel you have a probability then you plot it on the right then you sample a value from that on the left then you go based on that value based on the actual binary value you come up with a probability for the second pixel which is just a number between Z and one you plot it on the Right image then you sample from it and you keep going so the right is notes doesn't come from like the things what factor learning yeah yeah it does it does so it's basically these numbers the predicted probabilities for every pixel which are the X at I so the probability that that pixel is on or off and then but they are matching so that's why look the same because the sample that you see on the left is what you get by by sampling from those distributions yeah I am noticing that like it is agnostic of what the label should be is that like the right call to make for Generation so the question is should we take advantage of the fact that maybe we have labels for the data set and so we know that you know there is different types of digits that there is maybe 10 digits and then we want to uh you know take advantage of that so here I'm assuming that we don't have access to the the label why if you had access to the label y you could imagine trying to learn a joint distribution between X and Y and perhaps you would get a better model or perhaps you can assume you don't have that kind of structure you just learn a model and you can kind of try to use the model to see whether it indeed figured out that there are 10 clusters of data points and that you know there's a bunch of data points that kind of have this shape of a that look like a c kind of like an oval and that's a zero and that's the kind of third point of how do you get features out of these models like presumably if you have a model that can generate digits that have the right structure and it generates them in the right proportions it has learned something about the structure of the images and what they have in common and so that was kind of like the third point of getting features and supervis learning we'll talk about how to do that but uh yeah there's two ways to see it you can either do it unsupervised or if you have access to the label then perhaps you can include it into into the model you can do conditional generation or you can jointly learn a distribution over X and Y yeah so in this case when you sample you can get any one of the 10 digits well if the model does well yes uh you know for example you to check whether the model is doing a good job you could try to see what is the proportion like if in the original training set all the images come they're uniformly you know you see an equal proportion of the different digits then you apply an Mist classifier to your samples and you can see does it generate uh digits in the right proportion if it doesn't then there's probably something wrong with the model if it does it's doing something right whether it's correct or not it's it's it's hard to say so like here it seems like you're injecting the stronger prior into the model so if you had an infinite data set would you expect the original approach to per better than this one meaning the that one's less structure imposed by us right so the representation it should learn should theoretically be richer that one is actually more structure like you're imposing like you have less parameters is less flexible uh if you had infinite data and infinite compute the best thing would be conditional probability tables ban Network that one would be able to in principle capture any relationship with infinite data you would be able to learn that table that would give you perfect module overfitting I mean but if you have infinite data you don't have to worry about that either uh something so on the left picture is the the actual samples generated from the model the right is we somehow code the conditional probabilities into a scale yeah just between zero one like the conditional probabilities would be numbers in between zero and one and it's just the gray scale yeah cool so that's the N um now you might wonder what do you do if you want to model uh color images let's say so if uh the B the variables are no longer binary but if they can take let's say k different values how do you maybe pixel intensi is ranging from 0 to 255 how do you do it now what you need to do is the output of the model has to be a categorical distribution over however many different values the random variables can take so you can basically do the same thing you first get this kind of hidden vector or latent representation H and then you instead of applying some kind of mapping it down to just the the parameters of a berol random variable you can use some kind of soft Max output layer to map it down to a vector of um if you have K different outputs that you care about a vector of K probabilities uh Pi i1 through p i k which basically would represent the probability that the I random variable should take one of the K different values that the random variable can take and uh that's the natural generalization of the sigmoid function we had before it's just one way to take K uh numbers which are not necessarily non- negative and they might not be normalized and it's just a way to normalize them so that they they become uh a valid probability distribution so specifically you just do something like this if you have a vector or arbitrary numbers you apply the soft Max operation it produces another Vector you apply an exponential to every component to make sure it's not negative and then you divide by the sum of these exponentials which is basically making sure that uh the entries are normalized so that it's if you sum the probabilities of all the possible things that can happen you get one and uh so natural generalization of what we had before now you might wonder what do you do if you want to model continuous data so maybe you have you're dealing with speech and it's more n it's not very natural to discretize the the I mean even for images perhaps you don't want to discretise the the random variables and you want to model them as continuous random variables so the solution is basically again use the same architecture but now the output of the neural network will be the parameters of some continuous distribution so it's no longer the parameter of a ber the parameters of a categorical it could be the parameters of a gaussian or a logistic or some U continuous probability density function that you think should work well for your data set and so for example one thing you could do is you could use a mixture of K Gauss so what you have to do is you need to make sure the output of your neural network gives you the parameters of K different GS uh which are then mixtured together let's say uniformly to obtain a relatively flexible kind of probability density function like you see here an example where there's three Gauss with different means and different standard deviations then you combine them together and you get a nice kind of GRE uh red curve where you're kind of allowed to move the probability mass and you're allowed to say maybe you know there is two different uh values that the random variable can take two modes one here and one here and you're allowed to move the probability Mass around by changing the mean and the standard deviation of the gut in this Cas have like 2 vales so I think I have the the the more precise thing here so you would say the conditional probability of XI given all the previous values is a mixture of K gos each one of them having a different mean and a different standard deviation and as usually you have to to basically use the neural network to get the parameters of this distribution so in this case as was suggested you could use the same trick and then as an output layer you can no longer use a soft Max or a sigmoid you have to use something else that gives you the parameters of these random variables and so you need 2K numbers you need K means and you need K standard deviations and you know the uh I guess you know you need to be careful about if you use depending on how you parameterize like if you parameterize a variance then has to be no negative but that's relatively easy to enforce okay now as a way to kind of like get a deeper understanding of what these kind of models do you might notice that they look a lot like out encoders like if you look at this kind of computation graph that I have here where you have the the data point X1 X2 and X3 and X4 that is been mapped to this predicted probability X1 hat X2 hat X3 hat and so forth it kind of looks a little bit like an out encoder where you take your input X and then you map it to some kind of predicted uh reconstruction of the input and so more specifically an out encoder is just a a model that is often used again in unsupervised learning it has two components it's an encoder takes a data point and Maps it to some kind of latent representation and then uh for example it could be again a simple neural networks a two layer net like this and then there is a decoder whose job is to try to invert this transformation and the job of the decoder is to take the output of the encoder and map it back to the original data point and uh you know in this case in this graph that I have here it could be another neural network that takes the output of the encoder and Maps it back to some reconstruction of the input and uh the loss function that you would use would be some kind of reconstruction loss so you would kind of like try to train the encoder and the decoder so that uh for every data point these kind of when you apply the decoder to the encoder you get back something close to the original data point so depending on whether the data is discreet or continues this could be something like a square loss where you try to make sure that at every coordinate your reconstructed I variable is close to the original one if you have discrete data it's more like does the model is the model doing a good job of predicting uh the value for the I uh let's say in this case it's binary here or the I random variable that I'm actually observing so if the I random variable is true is one is the model giving me a high probability for for the value one right not super important but kind of like this is how you would you would try to learn the decoder and the encoder so that they they they satisfy this condition and of course there is a trivial solution that is the identity mapping so if the uh encoder is just an identity function and the decoder is some identity function then you you do very well at this and it's not what you want typically so typically you would constrain the architecture somehow so that it cannot learn an identity function but that uh has kind of like the flavor of what we're doing uh with this sort of like Auto regressive models we're taking the data point and then we're trying to use parts of the data point to reconstruct itself or we fit it through these networks and then we output these predicted values and if you were to think about how you would train one of these models by let's say maximum likelihood you would get losses that are very similar to this you would want to you know if you were to train these logistic regression classifiers you would get something very similar to this where you would try to predict the value that you actually see in the in the data point I'm just trying to understand put it in coder decoder kind of mechanism is it uh is it the main point for encoder uh is just to kind of com uh compress all the previous informations into a very low dimensional kind of like is that the main yeah yeah so the question is why are what are out what are out encoders used for yes the the typical one typical use case would be to learn a compressed representation of the of the data somehow if you can do this you know maybe you force the output dimension of of the encoder to be small and then in order to do a good job at reconstruction it has to capture the key factors of variation in the data and so you can kind of think of it as some sort of like nonlinear PCA kind of thing that will try to discover uh structuring the data in an unsupervised way yeah can we do sampling when TR the the question is can we do sampling with an out encoder no an out encoder is not quite a generative model so these two things not quite the same but they are related and that's what we're going to see next um so yeah this was coming up you know typically you would train this to do representation learning try to find good representations uh what is exactly the you know if you think about kind of like what we just said now if you have an out encoder there is not it's not really a generative model like how do you generate data from an outter coder we just re but what's the input to the so the the suggestion is okay let's throw away the encoder let's just use the decoder what do you feed into the decoder to generate data some just handcrafted effect yeah that's the solution for a variational out encoder actually so the variational out encoder will be let's try to learn a simple generative model to feed inputs fake inputs to to your to your decoder uh and so you can kind of fake the the process and you can use it to generate so that's the variational out encoder solution I will talk about later but if you just have you know there's not an obvious way to generate the inputs to the decoder unless you have data but at that point you're not really sampling right could you like feable yeah that's the VA solution basically that we'll talk about yeah what if you like add a regularization term that for as your hidden representation to just look like a gausian or something like that yes so again that's the solution imposed by the that's basically a variational out encoder literally a variational out encoder is this plus what you suggested forcing the latent representations to be distributed according to a simple distribution a gausian and if that happens to work well then you can sample from that distribution fit the inputs to the to the decoder and that works but you know that requires a different kind of regularization the relationship here is that uh you know although this two things look similar it's not quite the same and the reason is that uh you know we cannot get a generative model from an out encoder because somehow we're not putting enough structure on this kind of computation graph and there is not an ordering remember that to get an auto regressive model we need an ordering mini chain rule so one way to actually get uh or to connect these two things is to enforce an ordering on the out encoder and if you do that you get back basically an auto regressive model and so basically if uh you're willing to put constraints on the weight matrices of these neural networks so that there is a corresponding basically evasion Network or or chain rule factorization then you can actually get an an outo regressive model from an out encoder and the idea is that basically if you think about it the issue is that we don't know what to fit to the decoder so somehow we need a way to generate the data sequentially to fit it into this decoder that we have access to and so one way to do it is to kind of like set up the computation graph so that the first reconstructed random variable does not depend on any of the inputs if that's the case then you can come up with the first output of this decoder yourself because you don't need any particular input to do that and then you can feed your predicted first random variable into then let's say that the you know at generation time then you don't need it now if you can it's fine if the predicted value for the second random variable depends on X1 that's fine because we can make up a value for X1 then we can fit it into the computation and we can predict a value for X2 then we can take this value we can take the first two fit them into the outter quar kind of thing and predict a value for X3 and we can keep going and it's the same thing as an auto regressive model so if you look at this kind of computation graph you can see that the predicted value for X1 depends on all the inputs in general and so you know if you look at the arrows all the inputs have an effect on the first predicted value and so that's a problem because we cannot get an auto regressive model if we do it that way but if we somehow mask the weights in the right way we can get an auto regressive model and then as a bonus then we have a single neural network that does the whole thing so it's not like before that we had the different classification models or that they were tied together somehow uh if uh we can do this then it's a single neural network that in a single forward pass can produce all the parameters that we need I was wondering in some tasks for example this digit task we earlier discussed um is there not a risk of the model learning just how to shift it by like one pixel or something uh not OB obvious that you can just shift because the I mean you're not you cannot cheat right so you cannot look at the next Pi they cannot you can only use there's gonna you have to pick an ordering and you have to predict yes but you can like begin with the L and then begin putting like the pixels to the left of it for example but what do you put you haven't seen them they haven't seen the right pixel so you don't know exactly what to copy right no no you do because like if we before order goes left right you will know what the uh pixel in the input image to the left of it is so you can just put that uh and you can draw like a black line on the left so like or I was wondering if or metric need to to like really prevent that from happening no you don't you don't need to prevent that from happening and partially it's because these B would then be trained by maximum likelihood so and that's a separate thing that we're going to talk about how to evaluate so that that solution might not actually give you a good score from the perspective of a learning algorithm even though maybe the samples would look fine uh but yeah I haven't seen that happening in practice so okay the bonus would be single pass you can get everything as opposed to and different passes and uh the way you do it is to basically mask right so what you have to enforce is some kind of ordering and so you basically have to take the general computation graph that you have from an out encoder and you have to mask out some connections so that there is some ordering that then you can use to generate data and the ordering can be anything uh so for example you can pick an ordering where we choose this X2 X3 and X1 which corresponds to the chain rule factorization of probability of X2 X3 given X2 and X1 given the other two and then what you can do is you can mask out some connections in this neural network so that X the Reconstruction for X2 does not depend on any of the inputs and then you can mask out the parameters of this neural network so that the parameter uh of x3 is only allowed to depend on x2 and uh uh the parameter of X1 is allowed to depend on everything just like according to the chain rule factorization and so one way to do it yeah so that's I think what I just said one way to do it is you can basically keep track for every hidden for every unit in your your hidden layers you can basically keep track of what inputs it depends on and so what you could do is you could pick for every unit you can pick an integer I and you can say I'm only going to allow this run this unit to depend on the inputs up to the I index I and so you can see here that uh you know there's this 2 one 2 two this B basically means it's only allowed to depend for example this unit is only allowed to depend on the unit one and two this unit here is labeled one so it's only allowed to depend on the first input according to the ordering which is X2 and then you basically um recursively ask add the masks to preserve this invariant so when you go to the next layer and you have a node that is labeled one then you are only allowing a connection to the nodes that are labeled uh up to one in the previous layer and the way you achieve it is by basically masking out and setting to zero basically some of the elements of the of the Matrix that you would use for that layer of the neural network and if you do that then you preserve this invariant and you can see that indeed uh the parameter of X the probability of X2 which is the output the second output of the neural network does not depend on any input which is what we want for a chain rule factorization and if you look at the parameter of x3 which is the third output you'll see that if you follow all these paths they should only uh depend on basically the second on x2 which is the variable that come before it in the ordering and so by maintaining this invariant you get out encoder which is actually an auto regressive model you are essentially forcing the model not to cheat by looking at Future outputs to predict uh and you can only use past output past inputs to predict future outputs essenti and this is one architecture that would enforce uh this kind of invariant yeah sorry is it something that's like done during training or do you like train an auto encoder and then mask certain generation this is done during training so you have to during train like you basically have to set up an architecture that is masked so that uh it's not allowed to cheat while you train because if you didn't mask then it could uh when trying to predict the X2 you just look at the actual value and you use it right and so this is very similar if you seen language models you also have to mask to basically not allow it to look into future uh tokens to to make a prediction if you're allowed to look into the future to predict uh tokens then it's going to cheat and you're not going to do the right thing and this is the same thing uh at the level of the comput different computation graph that basically achieves the same sort of result and the benefits of single passes during training time yes good question yeah yeah so the question is is the benefit only a training time or INF time so the benefit is only at training time because at inference time you still have the sequential thing that you would have to come up with a value for for the first variable and fit it in so it will still have to be sequential that's unavoidable every outo regressive model has that kind of annoying uh flavor basically how do you choose the so the recipe in this paper Is Random so you mean the the the the the value or the the ordering oh the ordering that's also very hard I think uh you know if you have something where you know the structure and you know again that there is some causal or there is time maybe there is a reasonable way of picking an ordering otherwise you would have to either choose many orderings if you have basically have a mixture uh choose one at random but there is not a good way of basically selecting an ordering there is actually research where people have been trying to learn how to regressive models and an ordering so you can like Define a family of models where you can search over possible orderings and search over factorizations that over that ordering but you can imagine there's like M factorial different orderings to search over and it's discrete so it's a very tough kind of optimization problem to find uh the right ordering if one is not dependent on anything how does the model output one there should output it should only Al two three right should be uh you would have to I mean depending on the loss fun it cannot depend on anything but you can still basically make a a guess based on no evidence so you would basically choose the prior right so if the let's say the second variable is always true then you would still depending on the training objective you would still try to choose an output here it's a constant but you would try to match basically the most likely value in the training set or if you have a proper scoring rule then would try to match the distribution that you see in the training set depending on the loss function you still try to choose a a value that makes sense but it's fixed so you can only choose one and so you can't do much but you're still you would still try to do your best to to to capture the data depending on the training loss yeah are there redundancies like in the second to last layer there's two uh nodes which just have one as an input so is it kind of redundant to have multiple nod just have one in uh I mean the weights are different so even though there is multiple nodes that have only one an input they might be extracting different features for that input so it's not necessarily predominant I would say so the objective of the auto is to deconstruct yes so how do you reconcile the loss function it's pring one thing at how do you like make the function reconstruction yeah so the function would be the ones that we have here uh which uh would be you know basically you would try to make the predictions close to what you have in the data so the loss function wouldn't change it's just that the way you make predictions is you're not allowed to cheat for example you're not allowed to look at XI when you predict XI and you're only allowed to predict it based on previous variables in some ordering and it turns out that that would be exactly the same loss that you would have if you were to train the AO regress model it depends on kind of the model family that you choose but if you have logistic regression models it would be exactly the same laws for example in your last layer like you said you pick each of the I guess the number of times it looks previously randomly so if you happen to pick 222 how would you predict the first entry after that yeah so you would basically be you're not allowed many connections and you would do a pretty bad job because you you would be less uh Flex ible that you could be it would still be a valid model uh it wouldn't be a good one I guess so that's why people often have kind of like an ensemble of these mods where you have multiple masks and you just do it that way yeah cool um let's see now an alternative way to to approach this is to um use uh RNN some kind of like recursive style Compu ation to basically predict uh the next uh random variable given the previous ones According to some ordering right at the end of the day this is what the key problem whenever you build an auto regressive model is solving a bunch of coupled kind of prediction problems where you predict a single variables single variable given the other variables that come before it in so moring and uh the issue is that this history kind of keeps getting longer so you're conditioning more and more things and uh rnns are pretty good at or or it's one way to kind of like handle this this uh kind of situation and uh kind of like try to keep a summary of all the information of all the things you've conditioned on so far and recursively update and so a computation graph would look something like this so there is a summary H let's say h of t or h of t + one which basically is a vector that summarizes all the inputs up to that time and you initialize it somehow based on some initialization and then you recursively update it by saying the new summary of the history is some transformation of the history you have seen so far and the new input for that times step XT + one and maybe you know this is one way to to kind of implement it you do some kind of linear transformation of HT XT + one you apply some nonlinearity and that gives you the new uh summary up to time t + one and then what you can do is just like what we've done so far is then you use H to basically or you transform it Ag and you map it to either let's say uh a category the parameters of a categorical random variable or a berol random variable or a mixture of gaussians whatever it is that you need to predict uh you do it through uh well I guess you probably also would need some nonlinearities here but there is some output which is the thing you use for prediction which is going to depend only on this uh history vector or the summary Vector of all the things you've seen so far and uh the good thing about this is that uh basically it has a very small number of parameters like regardless of how long the history is there is a fixed number of learnable parameters which are all these matrixes matrices that you use to recursively kind of like update your summary of all the information you've seen so far and so it's constant with respect to when remember we had the things that were linear in N we quadratic in N this thing is actually constant the mees are fixed and you just keep applying them exactly it's it's extreme weight sharing and that you try to do everything throughout a cion yes here we're imposing a mark assumption on the addition probabilities this is still not so the question is is this a mark of assumption there not a mark of assumption in the sense that if you think about XT is not just a function of the previous XT minus one right uh it still depends on all the past random variables in again not entirely General way so you can only capture the dependencies that you can write down in terms of this sort of recursion and so you know it's not uh it's definitely not a mark of assumption this is that if you think about the computation graph it does depend on all the previous inputs and uh so this is an example uh of how you would use this kind of model to model text so the idea is that in this simple example we have only let's say four different characters h e l and O and then uh you would basically encode them let's say using some kind of one hot encoding so H is one0 0 e is 0 1 0 0 and so forth and then as usual you would use some kind of Auto regressive factorization so you write it down this case from the ordering is the one from left to right so you you write the probability of choosing the first character in your piece of text and the probability of choosing the second character given the first one and so forth and uh what you would do is you would uh basically obtain these probabilities uh from the hidden layer of this recurrent neural network so you have these hidden layers that are updated according to that recursion that I show you before and then you would use the hidden layer you would uh transform it uh into an output layer which is just four numbers and then you can take a soft Max to basically map that to uh for non negative numbers between 0 and one the Su to one and so in this case for example uh we have a hidden layer and then we apply some linear transformation to get these four numbers and uh we're trying to basically choose the values such that the second entry of the vector is very large because that would put a lot of probability on the second sort of uh possible character which happens to be e which is the one we want for the second position and so then when you train these models the game is to choose values for these matrices so that you know let's say you maximize the probability of observing a particular data data point or data set um and yeah so again the the kind of like key thing here is that you have a very small number of parameters and then you use the hidden state of the neural of the of the RNN to get the conditional probabilities that you need in an AO regressive factorization and uh and then and then you can see kind of like the recursion then you would compute the next hidden state by taking the current history then every you know the new character that you have access to you update your recursion and you get a new hidden State you use that hidden state to come up with a vector of predicted probabilities for the next character and so forth it's the same Machinery as before but instead of having multiple kind of like linear reg or logistic regression classifiers we have a bunch of classifiers that are tied together by this recursion and uh the pro is that you can apply to sequences of arbitrary length and it's actually in theory at least rnns are pretty General in the sense that uh they are they can essentially represent any computable function at least in theory in practice uh they are tricky to learn and uh you know you still need to pick an ordering which is always a problem for AO regressive models the key thing the key issue with this sort of like rnns is that they requires they're very slow during training time uh because you have to unroll this recursion to compute the probabilities and uh that's a problem but I'll just show you some examples and then I think we can end here it actually works reasonably well like if you take a simple three layer RNN and you train it on the all the works of Shakespeare at the Character level so it's literally what I just showed you just a three layer RN um and then you sample from the model you can get things like this which has a little bit of the flavor of sh X I guess not I if you think about this is at the Character level you know it's this literally generating character by character it's actually pretty impressive like it learns it needs to learn which words are valid and which ones are not Grammar punctuation like it's pretty impressive that a relatively simple model like this working at the level of characters can do can do like this you could train it on Wikipedia and then you can sample and you can just you can make up fake Wikipedia Pages like this one on the Italy that conqu conquering India really interesting made up stuff but again you can see pretty interesting how it it's able to has the right markdown syntax and it's closing the brackets after opening them which has to remember through this single hidden state right that it's carrying over uh yeah so you know he even making up links to to for for this madeup facts that it generates and uh you know train it on baby names and then you can sample from the model you can get new new names so yeah it's a it's a pretty you know surprise works surprisingly well I guess the the main issue that hopefully then maybe I guess we'll go over it next time that the reason this is now used for state-of-the-art language models is that you have this bottl that you need to capture all the information up to time T in a single Vector which is a problem and the sequential evaluation that's the main bottleneck so it cannot take advantage of modern kind of gpus because in order to compute the probabilities you really have to unroll the computation and you have to go through it step by step and that's kind of like the the the main challenge"}], "The Paper that changed everything! The Science Behind ChatGPT Fully Explained": [{"content": "hello everyone okay today to be honest this is one of the episodes that will not get view I know from the get-go I will do all the work and this will not get viewed and I'm okay with it this is one of the episodes I actually do for the community so the reason I'm doing this episode is in one of my previous videos I mentioned the strawberry test and I was saying the reason that AI models struggle with the strawberry test and Counting how many RS in the word strawberry is because of the way it does the prediction and one of the community members who is mostly smarter than me when it comes to AI corrected me and said no it's more about how it does the embedding and the tokenization um at the early stages of the training of that model so I I said this is a good opportunity for me to dig deeper into understanding artificial intelligence as someone learning and sharing my journey with everyone and this brings me to this episode so I did some homework I looked at some references I read through it I'm going to do it all over again here and this is the type of video um if you're going on a long run Long Walk Drive it's something that you listen to I guarantee by the end of this video this is my value proposition to you that you will have definitely more knowledge of how AI models work than the average person now it might not be perfect but this will be giving you maybe keys and ideas and resources that could give you some inspiration to do more than I did with this knowledge so you can research it more understand it more and hopefully share that feedback with me so today we're going to start with some Basics I've already covered a video on the highlevel uh areas about artificial intelligence the different type of networks how they work and even I shared some resources on demoing such artificial neural networks how they work so at least you can imagine what goes behind it beyond the theoretical and today I'm going to do the same but I'm going to focus on a different area today we're going to focus on two things the Transformer architecture which is the thing that revolutionized everything we know today about artificial intelligence it brought us chat GPT Cloe all of these kind of models that we love today and then I'm going to look at the future you know what could be the next phase of evolution for artificial intelligence models and and finally if you wait till the end I'm going to give you three or four tools that you can play with that can help you understand better everything that we go through through throughout the episode and even you can share it with others and help you explain to them what is a Transformer and uh visually explain it to them rather than doing an hour probably um of uh sharing links and articles and reading through text and by the way you you feel free to jump till the end and just go to these resources see the demos but I guarantee you you will not capture the full value of this spend some time maybe you can skip between chapters but spend some time to go through the flow of the video and then go to these demos it will be much more valuable for you I will be repeating certain things I'm not an expert so I found myself when trying to learn things on YouTube that there's two two types either I go watch an expert doing things things overs simplistic or I watch an expert actually complicating things things that I I don't grasp or understand but I found a different type of video not so mainstream I'm going to do the same which is someone trying to learn learning real time uh the concept that they're talking about and I found that more relatable and in many cases help me grasp such Concepts much better than getting it from the expert so to speak so let's explore the together my name is Sam if you don't know me my channel is all about AI in Tech and helping people who are not technical like myself when I started my journey learn it use it in their personal life and their careers to improve it make it better take it to the next level and with this let's just get started subscribe to Daddy's channel the mic okay before I start everything I show you here it will be in the description all the links and resources these are things created by super smart amazing people that are giving out this information to the world I will link it in the description I've created as I mentioned before a video with the AI Basics networks how they work the types of networks at least the common ones I'll also include it in one of the cards um throughout the video and I'll include it in the description so you can have everything in this video in one place for you to do your own research or expand on it if you want to do so and with this let us start with quickly I will not spend lots of time with the fundamentals and then we C we can actually move to the important part which is the Transformer architecture and why it's special and closer to the end as I mentioned we're going to actually see why it gets the strawberry uh test wrong and we're going to demo that see why and how we can fix that maybe in the Transformer architecture but in general why it get set wrong for the most models okay so here you see this visual which I showed in the previous video and I'm sure you've seen it before the whole premise for someone who doesn't understand artificial intelligence yet maybe this will help you to follow along with the video is that someone somewhere decided to replicate uh the way we think the way our brain functions into a program can we do it artificially can we do it through machines basically and what they've done uh literally they were influenced by the way how our brain cells operate and work and how the anatomy of those cells is so you can see uh the cell how it looks I will not go over the scientific terms of the cell but basically it has branches as inputs going into the nucleus which is later on AI we call it a nod and then it has an output that goes probably to interact or uh connect to other nucleus uh cells in the brain and you can multiply that by billions and billions of cells and that what gives us what we perceive as humans as intelligence or IQ so this is how our brain works now they came and they tried to replicate that through um a program in mathematics now this idea is not new it's actually pretty old but we never had the computing power we never had the data available to actually uh start working with these things uh in real life um and as technology progressed we started doing it implementing it playing with it and we got along the the years certain breakthroughs obviously the Transformer was one of the most recent and the probably the most significant one um that we can recollect today and it it moved artificial intelligence from something that researchers used to work with to something for the open public to actually make use of uh in tools that we use today everyone probably is using artificial intelligence in one way or another so you can see here this is the nod which is basically the nucleus and it gets a bunch of inputs similar to the branches that we see uh here and uh again this is all mathematics so these are computer programs on top of machines but in the core of it all of this as you will see today it's all mathematical equations that make sense of data as it flows and by the way today as humans we are not able to them all of these calculation at once we know how it's built we know how it's calibrated and we judge based on the design of the model the input the output if it's a good one or not there's methods to to do that so we have all of these inputs which are X1 X2 until xn and then we have the weights now the weights is the end result the easiest way to to describe it it's the value we care about because this is the uh the value that is assigned to a model after it's trained and this is what gets a model to operate in the way we want it and for it for us to use it later as a trained model so this is the network for training that model and generating all of these weights that we will use later on in any model for operation for use now all of these inputs go to an activation function and the easiest way to describe the activation function it's a function that will decide if that node will activate or not so when we think about something in our brains not all our cells are working at the same time it's parts of our brains address certain areas or certain questions or certain topics that or or activities we it's taken care of so it's not every time we have a thought the whole brain lights up it's just parts of that brain and the easiest way for me at least in my in my mind to understand activation function it is exactly that it is the function that tells this node oh based on that input you you're going to fire up and continue to another node or you're not going to fire up you're not going to activate basically and things will stop here um and there's a sum which is a transfer activation function basically it takes all these inputs there's a function that runs over these inputs and it is what uh puts things together to go for the output so this is the First Fundamental idea um and again let me scroll down just to show you how these networks look again this is a very simp simplistic look of of networks it is again similar to the brain billions and billions of what they call parameters so you can these parameters are the weights but you can think of it when they train these uh neural networks they actually train them through billions of nods and eventually we have those weights that we'll use later on this is just a you know a view for you just to imagine how it looks so you'll have an input layer basically this is where you push the uh training data or whatever data you need to train that model and you have so many layers of hidden layers what they call Hidden layers and then you have an output layer where you get the output this is why they call it deep learning because you you have so many layers of hidden layers within that neural network that makes it really deep so the data has to go a long way from the input layer until it reaches the output layer and you can see at each stage there's that sum and activation function shown in the equations in the bottom so this is fundamentally what we are looking at when we talk about Ai and this idea again is old now something changed along the way in terms of the architecture of these things that made it much better in reasoning of some sort and uh but more importantly in having certain uh aspects which simulate um memory or context understanding because at the end of the day if I give you a sentence that sentence is highly linked in your brain to a context so for example if I mention um The cat went out to the field and it ate the mouse so once I say it it refers to the cat this is the context that your brain will start linking it to the cat computers can't do that and the the evolution that we have in the technology actually helped us to give it the ability to understand that this it this word is actually linked to to cat and that happened through the concept of attention what words in that sentence should the model or your brain for that matter give attention more or less what are the more related or less related words in a sentence that are important for you to comprehend and or important for the machine to comprehend so this is our Focus the attention part um when it comes to artificial intelligence models but at the back end of all of this it's all artificial neural networks that in simple terms look like this now let me move to another fundamental thing which maybe not so related like other things I'm going to show you here but I think it's important so I talked about it in my first video this is the convolutional neural network and this is a type of networks or CNN um it has to do more with image or other areas less text uh but these are very similar in in one way or another but they itely benefited as much as the conventional networks I just showed you from the Transformer architecture and artificial intelligence and uh the the way those networks operate so let me just take you down again you can see always there's this kind of view of how the brain works and then they show you how the network works uh it works more more with images and you can see it's that unique area that IT addresses and it's basically a network that cuts down an image to let's say say pixels or smaller parts they call them convolutions that's why it's called the convolutional neuronal network and each part uh becomes basically a matrix of numbers at the end of the day when we look at an image uh the colors in it uh the lights it's all it's it's just a number it's a it's a numeric value that the computer translates into an image when it's shown to us and when it handles that image as well it's numbers behind it so it takes that chunk of the image as a convolution turns it to these numbers and then does something called kernels into it basically does modifications those by the way could be as simple as increasing contrast um or doing other things like highlights it helps it break down that image to uh certain kind of aspects or a basic or simple uh way of looking at that image which might not be the same way we look at the image but it eventually will help the AI Network understand somehow what it is looking at so it does all of these convolutions again multiple layers and the kernels and at some point it it flattens all of that out and pushes it into a normal neural network or feed forward neural network and then it gets an output so let me show you uh in this uh again you can read this on your own pace this is not the topic of the episode but um this is one way of looking at it and by the way in the episode that I will link I give you a chance or a link or a tool where you can do this for yourself you can build the network you can actually see how it's predicting the input image versus the output so for example this is an image of the number two um and you can see that this design is taking this image breaking it through convolutions and then and you can see here convolution one convolution two and does all of these uh needed Kern and you can see it's multi-layered at each of them and then it's flattened here and then it goes through that feed forward Network and eventually you get an output the output here is 0 1 2 each of them will be assigned the probability and if the network is done right the probability for two will be the highest and then it will tell you I predict that what's written in that image is the number two so again I know this is just something for you to keep in mind that this definitely benefit Ed a lot this type of network from what we have when it comes to the Transformer architecture again just one more resource for you uh because this might come up and be mentioned as convolutional neural networks later on while we are reading um into the uh more detailed aspects of the Transformer architecture now let's spend a little while here on the activation function because those are very important when really understanding how neural networks uh are artificial neural networks work and again I am trying to learn as I go with you so if you find me struggling it's part of me thinking out loud with you and for us both to get to a place where we better understand all of this um and this will be the theme of the whole episode so I hope you enjoy it okay let's move on so what does an artificial neuron do simply put it calculates a weighted sum of its inputs adds a bias and then decides whether it should be fired or not so basically what I already explained so this is y is the output and then you have the sum of weight multiplied by the input so this is the W multiplied by the X uh adding to it the bias and again the bias is covered in the first video um it helps out in really fine-tuning that model as you train it uh in reducing the uh losses and basically the errors as it's being trained uh to the data it's fed again it's a different area of its own we're not going to focus on it today but just understand that this is the equation behind the activation function now the value of y uh can be anything ranging from uh minus infinity to Infinity um and then neuron really doesn't know the bounds of the value so how do we decide whether the neuron should fire or not why this firing pattern because we learned it from biology that's the way brain works the brain is working test is a working testimony of an awesome and intelligent system so again it's all the same theme it's it's the way our brain works and we're trying to replicate it into technology we decided to add an activation function for this purpose uh to check the Y value produced by a neuron and decide whether outside connections should consider this neuron as fired or not or rather let's say activated or not so the same thing I just um covered before again the activation functions are simple functions and there are so many types uh for different uses or sometimes the combinations of those types uh within the same network but here let's just go over it high level I will not go over each type of function and what it does it will just give you the idea of how that function works it gets something and it gets something out in a certain way and that could tell the network yeah this is activated or not activated and um the first thing that comes to our mind is how about a threshold based activation function if the value of y is above a certain value declare it activated if it is less than a threshold then say it's not so great okay this could work activation function a activated if Y is larger than threshold else not um alternatively a equals 1 if Y is higher than the threshold 0 Z otherwise so okay so this is called a step function so this is one type of activation function so you can see whatever inputs come if it's less than zero it will the out will be zero if it's more than zero the output will be flat one so here if the Y is negative the output will be zero and basically you can tell zero means it's not fired there's nothing that could happen with a zero of some sort but if the Y is posi then the output is one so that activation or that node carries on um and considered as activated or fired so let's just quickly look at if there's something more interesting here but I I I think you get the idea of the activation function now it goes here into you know the types of activation function so here we have a linear function so these are important and nonlinear function so the sigmo function is one of the most important and it's a nonlinear one so you can see here again it's the same concept so if we have something negative again you have to see where it falls uh into the chart here and then you can uh see what what the output will be right and the same goes if it's positive so this just gives a value on the curve based on the X's coming in the x's and the Y's basically and the sum of them coming in and it will give a y uh which will be somewhere on this curve depending on the sum that it is getting so at the highest range of it if the sum is equal to six or more the output will be will be one if the sum is smaller um of minus 6 it will be zero so again this is one way of one equation of deciding what nods fire and what's not and again all of these things beyond my expertise but it's it is um the way I think about it all of these things are levers for the people designing a model to put it within their Network to get certain performance out of that artificial intelligence Network that they're building or the model that they're building and training again here you can look at the link at your own pace there's so many types of activation functions this is a linear one so anything uh negative will be zero anything positive will will work against that linear um line here so if if it gets let's say a five it will be probably visually I'm looking at a five so this is one: one kind of linear function uh so it's it just anything positive will be the same value as the sum of the input anything negative is zero so it just removes the negatives uh this activation function so there's so many types so many things that you can look at but you get the idea the activation function fundamentally is a function that we design to decide which neurons continue carry on or fire basically or activate and which ones do not now let's jump to One More Concept this is called the back propagation algorithm and this is the way where actually models started getting trained and it is an algorithm that gave the models the ability to see what's happening going forward when the data is progressing into the network and the errors that are taking place see that errors in pre previous steps in in simple terms and try to correct them somehow or modify or by the way it's running so it will reduce those errors so we'll just go through it again it's a concept that helps a lot for you to understand later on as we progress through um the session towards what what we need to aim for which is Transformer architecture so let's just go quickly through the main parts of it so back propagation algorithm is probably the most fundamental building block in a neural network it was first introduced in the 1960s and almost 30 years later in 1989 popularized by raml Hart Hinton and Williams in a paper called learning representations by back propagation errors so again I I might have butchered their names I apologize for that but again it's it's the algorithm that helps us manage those errors and get the model to actually learn and train as it goes the algorithm is used to effectively train in neural network through a method called chain rule in simple terms after each each forward pass through a network back propagation performs a backward pass while adjusting the model's parameters weight and bias so we've seen the weight and bias so the bias if I didn't stress on it actually it goes into the nod by the way so these are some levers to tweak that model and how it will later on perform so what this does it allows not only feed forward kind of path for the information through the network but also a backward path which will allow that model again to modify weight and bias to perform better supposedly in the future at least this is the concept behind it and this is um happening or being used in neural networks since that time on obviously this have evolved a lot uh definitely it has evolved a lot um in the way it is being implemented or not okay so I'm learning as I go with you but it is definitely something as I learn uh or or read into artificial intelligence like it's one of the concepts that you need to grasp to a certain degree uh to really understand how the technology work so again here is just um um a look at the network so you have the inputs again these are going through the hidden layers that we talked about before and you you can see the nodes here and uh eventually it goes to an output and the input so this is just describing what we're looking at so you have an input layer and again this is the compilation of the inputs and weights uh basically the summation I guess you have the hidden layers uh again with their own uh equations um again this I'll give you a tool to do all of these calculations if you want to do so yourself this is I think the barrier of you know how deep you want to go in your understanding of the models you know my interest is to have good understanding of the concept and how that works um maybe touch the surface of the equations I'm an engineer I enjoy these things but this will be time consuming if it's your thing I will give you what will help you really tweak these things look at them as they work step by step later on at the end of the video but just understand that each layer each function each step has some numbers and modifications or manipulations to those numbers uh is maybe the the best term to use here and um okay so here activation a uh 2 okay and A3 are computed so here it's just talking about the type of activation function uh it is using so it's giving uh three examples we talked about the sigmo there's the reu and the tan each of them as we've seen does a certain uh function to manipulate the output based on the sum of the input and weights to activate or not activate um a nod let's go just to the important part of back propagation so this is going through all of the details of the equations what I'm interested in the back propagation and Computing gradients so according to the paper from 1989 back propagation repeatedly adjust the weights of the connections in the network so as to minimize a measure of the difference between the actual output Vector of the net and the desired output Vector so those talk by the way there's two types of data that they usually use uh in the things training data and validation data so training data is the data that you actually push through the network to train it validation data is the data you use to test the output of that Network and if it matches the training so basically back propagation um maybe in a simplistic term it does one run checks the output Compares it to a validation data sees the difference and by the way it does it compare to the training and validation data so you have what is called training loss and validation loss basic basically two types of errors and it looks at those errors and how wide they are or how big the error is or how small it is it will again be fed back to the network to modify or tweak the weights and biases so it will try again and try to reduce uh the error for the next run and so forth until a stage where it's satisfactory it is set as satisfactory where it holds and stops the training for the model in other words back propagation aims to minim minimize the cost function by adjusting Network weights and biases so the cost function is mentioned in this article you can read about it the level of adjustment is determined by the gradient of the cost function with respect to those parameters one question may arise why Computing gradients so again it is part of the math of doing back propagation and uh managing these artificial neural networks I will not go through it today you will have the link for this article um for you to dig as deep as you want um though now this is just a video I wanted to show before we move on it basically looks like this so you can have the forward feed type of movement within the network but also you have the um the Red Arrows which is the back propagation so you can see that goes back feeding to these nodes and then the nodes will modify or tweak the weights and biases and again go back to feed forward and see what output that actually generates within that neural network okay so let's move to the next part here okay so now we are getting into the interesting stuff ahead of the Transformer architecture the most prevalent type of networks that were used are the recurrent neural networks and long short-term memory uh networks lstm so it I think it's very important for us to understand what was the basically the height of of the technology before the Transformer architecture um and what was the issues within that uh architecture or that technology at that time and later on we will find out what actually the the Transformer um architecture solved compared to what we we had and again by the way the Transformer architecture did not replace fully these type of networks these are networks that are still powerful tools that are just enabled to much better perform in terms of quality of output compared to just working alone through these Technologies so still uh recurrent neural networks are a very important part of the technology of artificial intelligence and how it's designed trained and delivered um to the consumers or the users of that technology but at that stage the main thing was for developing artificial intelligence Concepts was the rnns which is the current neural networks and the long shortterm memory Network which is lstm and those are variants sort of speak um of the same type of network so we'll go through that just to see what was there ahead of Transformers and later on we'll start getting to the important stuff when it comes to the version of artificial intelligence in concept basic concept that we are enjoying today in everyday life so let's go through the introduction in recent years recurrent neural network and their specialized variant so it's a variant of the uh RNN long shortterm memory lstm Network have emerged as powerful tools in the realm of sequential data processing and here I would focus on the word sequential so this is one of the areas or the limiters in this technology at that stage that things went into sequence tokens or data went into sequence and uh the context was not there because it's going into sequence from natural language processing to time series forecasting these neural architectures have shown remarkable capabilities in capturing temporal dependencies and patterns so the basics of rnns Imagine a traditional neural network it receives inputs processes them through hidden layers and generates an output however so this is what we already covered the the normal neural network however rnns add a crucial twist memory they incorporate their previous output back into the network allowing them to remember past information and influence their current processing so this is not ideally back propagation but it sounds just like it it just retains for some time the uh information or for a longer period the information within the network so um that could be for back propagation for error this could be for context but it's again limited so the the output of of one node goes back to the previous node so it can somehow return that kind of loop um uh which runs for some time I think and we'll get the details a little bit as we go through this article the architecture of rnns rnns are a type of neural network that has hidden States and allows past outputs to be used as inputs so past outputs are used as inputs so I'm just imagining with you so it runs and it takes the output at each layer as an input again for the same layer maybe we we'll look at it now and this will will get the network as it's being trained to know what happened last time basically what happened the last time the data went through that Network okay this was the outputs now it might modify differently either for certain quality parameters or to reduce the um errors that we talked into the back propagation I think this is different in that sense from back propagation so back propagation just takes the those errors back through the backward feed um and provides uh them for tweaking the weight and the bias this one just literally is recurrent so it takes the output feeds it back back as an input so we have the inputs which is the X if you remember x i or x 0 X1 until xn and then we have the weights and this network will have one more input which is the output that the same not generated pre previously so uh the next time it runs it has the previous output as one more input to consider okay so this is just a high level architecture so let's just read quickly the basic building blocks so the basic building block of RNN is a recurrent unit this unit takes two inputs the current input X which is here so this is the current input and the hidden state from the previous time step the hidden State a acts as the memory carrying information about what the network has seen so far okay so it's not as simple as the same uh output of the not so actually it's the output from the previous um um the previous state of what the network has seen so far we we hopefully maybe we can see more details around that later but basically here if you're looking this is the a so this is the memory part of it and this is the output so we have inputs we have outputs this is one hidden layer I assume and this hidden layer is getting the previous state of what the network has seen so far when it comes to the perspective of this layer so I assume this layer has much previous layers and layers to follow it so it only sees what went through the last time or the last state um of previous layers okay let's carry on so um example of RNN machine translation okay sentence I'm going to eat lunch an RNN translating the this sentence to French might initially translate it to okay I will not even try to pronounce French I'm going to definitely butcher it um but okay this is the French word of eat I assume however or one variant of eat in French however if the preceding sentence uh in the conversation was would you like to join me for dinner the RN should understand that eat refers to lunch in this context and translate it to something else in French instead this shows how rnns can use context of surrounding sentences to improve the accuracy of translation so they have context but again I think it's limited context but this is the the value that rnns brought to the technology it give the network some memory and some context based on the data fed into that Network so let's go through the quickly the different types of rnns one to1 RNN processes single input and generates a single output useful for tasks like predicting the next word in a sequence one to many RNN processes a single input and generates multiple outputs used in music generation where one Melody becomes a sequence of nodes many to one rnns processes multiple inputs I think this is the typical one we understand and generates a single output common to sentiment analysis where multiple reviews lead to a single single positive or negative sentiment many to many okay so this is just types of uh the the network that you can build challenges with okay this is this is where the money is this is what we need to I think understand before we start moving to what happened later on in the evolution of this technology while rnns are effective in capturing short-term dependencies in sequential data they often struggle to learn long-term dependencies this is due to the vanishing gradient problem where gradients diminish exponentially as they are back propagated through time leading to difficulties in training the network to remember distant information okay so um The Vanishing gradient problem the easiest way I'll describe it I don't know if it will be explained later on I forgot honestly remember you're dealing with numbers and fractions of numbers so um in the recurrent neural network back propagation I assume from reading this happens if these uh gradients or basically the values that are uh moving layer to layer in the network are reducing at some point these gradients will vanish it will go down to zero so it will lose the information that it had uh so that will limit its ability to maintain that context for a long time and I think the opposite Could Happen um so the opposite of the vanishing gradient problem so it can actually grow until a high value or maybe one Limited at one I'm not sure but the problem of how you handle those numbers within the network as you train it is is a challenge one of them here is the vanishing or the main one here is the vanishing gradient problem again as it moves through the nods if these values of the gradients are just reducing and all of this is happening through in many cases multiplications and some so if you keep multiplying by a smaller number obviously that number will keep on diminishing and it will go to a too small value maybe close to this zero where the state of the training or the model that you have vanishes it goes away I think this is um what is meant here so let's look at the back propagation through time so this is the network okay so we have here inputs these are the nods these are the outputs and uh we we'll see what L is um but again so we get uh the this is the feed forward this is the back propagation and as it goes back this is where the gradient could go down to zero okay so somehow here it operates as the the network that we know but it just has the element of um the uh recurrent fact which is as I said in the uh which I suspected in the beginning it is actually um the same or some kind of um modified back propagation algorithm that makes it recurrent as a network so yeah okay so it's good they're talking about Vanishing gradient when back propagating errors through through many time steps in in an RNN gradients tend to diminish exponentially as they propagate backwards through time this phenomena can occur due to the repeated multiplication of gradient values during back propagation leading to a very small gradient consequently early time steps receive negligible updates during training hindering long-term dependencies learning so I think it is the same idea I had because when it goes in the back propagation path or the backward path or the back backward feed path it's being multiplied by smaller and smaller gradients and that leads to the values to diminish down to zero um more or less so it will lose its state as a model okay so standard RNN gradient flow Vanishing gradients so this is just a depiction of you know what happens so Vanishing gradients uh many values are less than one so you have the activation function weight initialization Network architecture okay when the weights are very small they get smaller quickly by multiplying in the back propagation process dropping down to zero making the model unstable that's why it's called Vanishing because the gradients vanish okay exploding gradient so this is the opposite of it which I mentioned a conversely exploding gradient refers to the situation where gradients exponentially grow during back propagation the this often happens when the Network's parameters are initialized poorly or when the network architecture is not properly designed causing gradients to increase uncontrollably exploding gradients can lead to numerical instability during training causing the model to diverge okay so again this is the other scenario where many values are higher than one exploding gradients gradient clipping to scale big gradients okay again it's all fundamentally means there's a big problem with recurring neural networks of L of state so it has a memory which is good was a big development but they remained with the issue of not being able to keep that Network stable for longer times now they made variant of that which was called the long shortterm memory Network so let's read about that and see also what's the issues with this one and what was able to solve and what it couldn't solve uh when it comes to this technology to surmount the limitations of conventional rnns uh lstm networks emerged as a breakthrough Innovation armed with a sophisticated memory mechanism long short-term memory networks excel in capturing long-term dependencies with finesse so it's an RNN that could hold memory um for long term peering into the architecture of lstm networks okay in addition to the inputs recurrent and output layers lstm networks Harbor memory cells designed to store information over extended periods governed by gate mechanisms these memory cells regulate the retention for getting and updating of information at each time step circumventing The Vanishing gradient problem and adaptly modeling long range dependence so basically what's different is that they have a memory cell we'll see what that actually is that will solve the vanishing gradient issue and it holds memory for a longer time so let's look at the key Concepts okay maintain a cell state so the state of that cell is maintained for uh in some way or another use gates to control the flow of information forget Gates um gets rid of ir relevant information store a Gates relevant information from current input selectively update cell State output gate returns a filtered version of the C State back propagation through time with partially uninterrupted grad gradient flow okay so that's interesting so it has some kind of gates and I understand Gates as rules basically equations functions um some of them will get rid of ir relevant information probably information out of the bound of certain limitations it's able to store some of that in certain Gates and then update the state of that um cell and then return filtered version of the cell State okay so again you can see here um it's a complex cell so this is the architecture within the nod so you have the input you have the output and this is what's happening between all of these uh let's call them uh sub noes okay maybe this is the best way to to um describe it it's uh certain functions that either retain modify eliminate certain values from the input uh going to the output the best way I would put it it's a more complex more sophisticated activation function at least this is how I understand it so far so this is the architecture and there's a legend for it so it will help us understand so X is the input so let's see the first um uh okay the the red bubbles here those are the biases okay that's good the plus this is elementwise summation and concentration so this is a function that adds the um again this is the let's say the ideal Activation so adds all these inputs and I assume um okay so this is the recurrent element into this network so you have the output of the previous block coming here in the orange so we have inputs biases we have uh the output of the previous um uh block all of them going into the summation so okay we understand this part now all of these sums are going into these yellow things which is the sigmoid U sigmoid activation function so again this is the activation function here okay so you can see there's two types of activation functions in the this uh kind of node so you have the tan H I don't know if you call it the tan H or just the tan and you have the sigmo so all of them uh all of the summations will hit those activation functions and then okay they will go to these icons which is elementwise multiplication okay so uh this will do some multiplication and again goes to the tan H this one here it goes to a summation here goes to a mult multiplication all of that will move to um a memory from current block so it's stored at a state so this is the storage this is the output of the current block so this probably goes to the next blocks so this is the output and this one is the memory from the previous block so we have input from previous block and the inputs and the biases coming in and we have the uh memory from the previous block um and we have the output going out and the memory out of this block going out as well so this is two outputs going out from the current block and this is the memory from the current block so all of this multiplication and summation again this is the complex part you want to dig deep into this feel free but what these calculations are doing are generating values that are either eliminating something or put in a the way they put it as a block for keeping the memory or giving the the output of the current run um and so forth so it's just complex calculations that will enable the architecture to operate in the way that um it is designed which is not only taking inputs and giving outputs actually taking inputs and like recurrent networks taking the output of at the state of previous uh um layers or nods but here also it's giving an output and it's giving an output of the memory State uh of this uh layer or this node this is the storage and it's also taking as an input the memory from a previous block so it's much more complex inputs and outputs managed for uh this architecture so this is again more details around the input gate controls the flow of information into the memory cell utilizes sigmoid activation function to determine which information from the current input should be stored so this is this is this part here that it's just deciding what it needs to actually store and this is goes to the storage here so this is why it goes straight to the storage uh forget gate regulates the retention or forgetting of information from the previous time step employs again a sigo activation function to decide which information is IR relevant okay I will not go into these things fundamentally we've evolved the rnns into the lstm networks those were equipped more with a memory and the ability to um eliminate certain information retain certain information um within the architecture so it gave it longer term memory uh sort of speak to retain its state for longer time which helped again the context part of uh training those models but you know up to this point it wasn't I I guess good enough um that it went to the public it wasn't good enough that it's something that you can um use um um in models that are uh scaled in the way we see today so you can see the challenge I think even with with these limitations or even with this capabilities that uh the lstms got it wasn't good enough in terms of context maybe it was good enough to do um very short translations but nothing more than that so the conclusion let's just read that I think always these are helpful in summary recurrent neural networks and long short-term memory lstm networks represent powerful architectures for process sequential data so we keep going to sequential data this is a very uh important one as before we progress while rnns are effective in capturing short-term dependencies lstm networks excel in modeling long-term dependencies by incorporating sophisticated memory mechanisms understanding the principles underlying these architectures is essential for leveraging their capabilities in various applications from natural language processing to time series forecasting and as research in this field continues to advance rnns and lstm networks are expected to play an increasingly important role in shaping the future of AI okay so again as I mentioned in the beginning uh rnns are still an important part of the architecture so that didn't go away so what we're going to cover next is not like something that replaced them I think it's the way to describe transformal architecture is one more technology that enables artificial intelligence so these two architectures that we just covered gave us a lot so the lstm gave us that longer dependency retention within the network but still it's I think the problem that remains with them is that um they have this kind of sequential limitation of the how the data flows through them um and um that that's a problem that we will look at later on and how it's solved as of now I think we have all the fundamentals to start looking at um basic concepts that take us towards the uh Transformer architecture and what that brought to the artificial intelligence technology okay so if you're watching so far honestly you could be considered the saint to watch so far and um at this point you know I feel ashamed to ask you to subscribe uh bearing that much I hope you're getting value if you're watching so far so hey if you like it do not forget to subscribe I would definitely appreciate that so now before we go to the Transformer architecture let's talk about tokenization embedding I think these are two concepts that um at some level we must understand not only for the case here of this video but really for anything when it comes to understanding AI models and how they operate so let's progress here and start with tokenization so tokenization is the process of taking the input text and partitioning it into small secure manageable units called tokens this uh these units can be words phrases subwords punctuation marks or characters according to open AI one token is about four characters and uh three quarters words in English Okay three quarter of word in English this indicates that 100 tokens are approximately equal to 75 words tokenization is the crucial step in natural language processing during this process you are preparing your input text in a format that makes more sense to AI models without losing its context once tokenized your AI systems can analyze and interpret human language efficiently let's take a look at the key steps to perform tokenization okay so it's basically uh turning text into tokens tokens as it mentions it could be words or part of words or even punctuations and it's just a way for the model to be able to understand the context or the the text that you are entering uh basically when when it says that turning text into numbers this is usually the idea so uh the step one of tokenization is something called normalization an initial step in which you need to convert the input text to lower case using NLP tools to ensure uniformity so everything is lower case so this is new to me so when you push something into um the the normalization process you don't have Caps or anything everything is lowercase you can then strip out unnecessary punctuation marks and uh replace or remove special characters like emojis or hashtags if you're prompting with um hashtag or if you're training maybe more accurately with um emojis or these kind of special characters those have have no value or bear no value to the model then you go to the splitting you can break down your text into tokens using any of the following approaches word tokenization the word tokenization method suitable for traditional language models like engram it allows you to split the input text into individual words consider the sentence the chatbots are beneficial H in the word tokenization approach this sentence would be tokenized as the word the chatbot are so each word in the sentence becomes its own token subword tokenization so this is a different way modern language models like GPT 3.5 GPT 4 and bir and by the way if you go to open AI um uh website they have a page dedicated to understanding how they do their tokenization use subword tokenization approach this approach breaks down text into smaller units Than Words which helps handle a broader range of vocabulary and complex paragraphs consider the sentence generative AI assistants are beneficial in the subword tokenization approach the sentence can be split as okay gener ative AI assistant and are benef so it breaks the actual word into part so it's not per word the character tokenization so character tokenization is commonly used for systems like spell Checkers that require fine grained analysis it enables you to partition the whole text into an array of single characters consider the sentence like I like cats the character based tokenization would split it to basically each letter and even spaces as you can see here um its own token now this is interesting if you remember at the beginning I told you uh the strawberry test someone mentioned to me it's about the tokenization part of it and the embedding so this is we might have part of the answer here I do not think that the character tokenization is the mainstream so for example if you go to open AI they use the subboard tokenization they do not use the character tokenization so the token as far as the model is concerned is not a letter it's a part of a word so it doesn't see letters so step three so after we actually tokenized so we've normalized tokenized now we're going to mapping in this step you must assign each token a unique identifier and add it to a predefined vocabulary adding special tokens you can add following special tokens during tokenization to help the model understand the structure and context of the input data so we'll do mapping and I think mapping will come in later and then we'll do um the special tokens and these will allow again as uh the tokenization to help the model basically understand the structure better maybe it's not really explained here but let's take it as it is okay so CLS CLS is a classification token added to at the beginning of every input sequence after the text passes through the model the output Vector corresponding to this token can be used to predict the entire input okay sep a separator token that helps you distinguish different segments of the text within the same it is useful uh in task like question answering or sentence pair classification so I think these are part of the special tokens they just give more context for the model as it runs through the uh input okay now embedding so that's the really important part embedding is a process of representing the tokens as continuous vectors in a high dimensional space where similar tokens have similar Vector representations these vector ctors are also known as embeddings help Ai and machine learning models capture the semantic meaning of the tokens and the relationships in the input text to create these embeddings you can use machine learning algorithms such as word to VC or gloy the resulting embeddings are organized in Matrix where each row corresponds to the vector representation of a specific token from a predefined vocabulary for instance if a Vo consists of 10,000 tokens and each embedding has 300 Dimension the embedding Matrix will be 10,000 multiplied by 200 matrix by 300 Matrix sorry the way I understand embedding and you can see we're talking about a huge number of dimensions and this is something that your brain can't even imagine um but the way if you want to think of it in our human context so imagine you are in a VR kind of um situation so you look at things in 3D and you have all the words in the vocabulary in that space and these are grouped obviously here we're talking about the tokens so could be parts of words and these are grouped in a way where it reflects the association of these words together and this is why I think like when when you do embedding if you ever went that deep um I did it in doing uh working with flow wise uh for example opening I have has its own embedding so uh all of these tokens or words for easier terms are grouped um where semantically they are similar so let's say the words that have similar meaning or are correlated to each other are within similar chunks or groups within that huge space so if you look at space for example you can find uh let's say door and knob so the word door and the word knob are in one group because these are related uh you can look at you know another group you can probably find cat and dog something like that this is how you imagine it and um each of them is represented with a matrix of numbers it's data basically and that data the way I which is the embedding uh the way I would describe it it is the GPS it is the coordinates of that word in that 3D space so if I give the navigator that number it will identify that word but if I give it the embedding of another word it can tell is that word close to the original word that or the initial word or is it far away from it if it's close that means those are within context those are similar in meaning or related in some sense to each other if they're not well there's not really high importance of the relationship between these two words together so just imagine it this way this is your uh this is your navigation to the relationship between words or tokens among each other in that kind of multi-dimensional space which is here they're mentioning 300 I think uh the open AI embedding it goes to 512 Dimensions but just the concept you want to grasp out of this that embedding is our way of going to that you know multi-dimensional dictionary seeing where all of these tokens are placed and how they are placed and in which groups and uh um in terms of numbers and are they related or unrelated to each other so let's look at this um example here so we have the mouse run up the clock and then the mouse run down so um we have all of these words here and this is will come later in the Transformer model we have here the sequence of these words so you know some of them are repeated so only here it's done once so we have the sequence of these word from 1 to six and you can see again the sequence of these words depending on the input so here you have the mouse run up the clock it has all the words one to5 here it has 1 to 3 and six all of those go to embedding layer output Dimension is four and these are the numbers so this is the embedding um representation of and this is multi-dimensional again of these of this input here and this input here and then you can build context just comparing all of these words and their embeddings to each other we'll go into that later but just remember remember it's a way for us to see where those word words fall into that space and how related or unrelated they are to each other okay I think so if you want to spend more time reading about this and going to the detail you can look it up on this uh link here all the links will be provided in the description but for us this is what we wanted to really understand we know what tokenization we have an idea of embedding now we'll go to the really important stuff which which is the Transformer architecture and everything that we've covered so far will play a role in understanding what that did and what it changed and where we're at today because of it so moving on let's go to this amazing article by Dr Ernesto Lee so thank you Dr Ernesto Lee you know I care of putting these resources again because of the people behind them so Dr Ernesto made this blog post to explain intuitively in an easy way uh the uh the the Transformer architecture um and the paper that basically broke that technology out to the world which is attention is all you need we're going to go to the paper itself later on but I thought the first run through this is through someone trying to explain it in layman terms to someone like me someone like you and later on we go to the paper because it's very important even if you understand the idea here I recommend to everyone to go to the original paper attention is all you need and read through it it's an amazing amazing paper to read um and even if you're not technical person you're going to grasp I would say at least 60 to 80% just by going through it at it different and whole new level of understanding it elsewhere so the intuitive explanation of attention is all un need the paper that revolutionized Ai and created generative AI like chat GPT like this title says it all this is the technology that brought us generative AI that we know today at least at a scale at you know consumer scale consumer level something like chat gbt is just started with this technology which came in 2017 so this is the technology that makes chat GPT works this is the architecture now we're going to go step by step to understand this the way I'm going to um try to explain it I'll try to go through the architecture in my own understanding today and then we'll go to break it down into bits and pieces to understand it further and further and further and hopefully uh um by the end of it we'll have a much commonly me and you a much better understanding of this architecture and what it means again I'm not a technical person I'm like you trying to learn so if you see me struggling this is my way of making and putting things together this is my brain you know my human intelligence now there's a name for our intelligence called human intelligence works and I'm training my brain to really learn artificial intelligence so I'm running my own um um Transformer architecture in my brain to make sure at the end of this I'll have an updated fine-tuned model that understands AI better so here um this is the architecture in general and the the main thing about it that you have this what they call the encoder block you have here the uh decoder block and you have inputs and you have outputs going out and eventually you'll have a final output which is a probability and the way it works is you push an input so imagine the input as a text similar to the examples went through sentence I ate the apple for example or uh let's make it a part of a sentence so um the Apple dropped from so what's the word after that so imagine you put only this sentence and you end it at from so we take that input we go through the embedding which we covered so what the embedding will do it will take all of these words give them that kind of uh Matrix that position them in that you know embedding dictionary that 3D space where each word Falls and then we're going to do positional encoding if you remember uh just now we looked at it so we're going to give them the easiest term a number that gives them the sequence so what's the pos position of each word this is word one two 3 four and once we have that we push it to the encoder now the encoder what it will do it will go through what's called a multi-head attention and this is where the money is when when it comes to the the Transformer architecture this is really the the the brains of it and what it does here and what what it brought to the technology that wasn't there before it was able to give context real context and in a parallel manner so this is where things are not happening sequentially so what it does it takes all of these inputs and positional encoding sums them together and what it does is push them through some kind of algorithm that will look at the embedding of each word in relation to the other words and it will create that kind of you the easiest way I describe it a heat map so which words are more related to the other words and this is from the perspective of each token that is going in and this is why it's called multi-head attention it all happens in parallel so all of these kind of matrices or heat Maps happen together and you can the the model will see each word and how it Stacks against all the other words in terms of of attention what is the when we say attention what are the other words that are the highest importance to that um uh to those other words in the input so for example we said the um the Apple fell from right this is what we said so um is the related more to the apple or fell or from and so forth it will do it for each of these words and through this multi head attention uh calculation and after that it will do a um adding and normalizing of that data so it can process it further and it will push it through a feed forward Network this is your normal Network here again it will do an add in norm and then the output of the encoder which has by this stage all the input information with its uh embedding and positional uh encoding so it has the meaning it has the positions of the words but also because it went through the multi-head attention it has that context ual contextual knowledge of that input of which words matter more to which other words so it understands the relationship between words the easiest way to put it and all of that goes into the decoder now again the decoder is um remember this is part of the bigger way of how this operates so here you we want to um predict the from so this is it says shifted to the right so we said the Apple fell from what's after that so that output prediction or the expected output goes into the same process so let's say at at the first word nothing happens here it's blank I would assume is my understanding because we have one input still it didn't generate the next word uh but eventually let's say after from it will put the that Apple F fell from the and later on three will come for example so once the is generated when it comes to running for the word after that uh to to um put the probability of three the will be the input here uh towards the output decoder part and it will go through the same things it will embed that word it will give it a positional encoding it could be a couple of words and then or tokens it will go through the um masked multi-head attention so it will put the context of the output and here it will do it again but here it is linking it to the input okay so it it has previous words the input plus previous words it will give it like that longer context so the Apple fell from the tree now maybe it will continue after that and then it will normalize at each stage obviously push it through a feed forward then it will make it kind of a linear softmax is a certain special type of function that will give probability for all the possible outcomes so for example at at this output it will say the Apple fell from the and if so all of these are possible outputs the softmax will give probability let's say the is 60% if is 3% you know so each possible output will get its own percentage eventually um the highest probability or the highest percentage will be the output of this in our example the Apple fell from the tree it will be most probably that the entry will be the highest probable next tokens to fall after the input this is is in the shortest sense of the form how I understand today the uh Transformer architecture the thing that it did which wasn't there before is that M multi-head attention getting the context so it understands words in the very similar way that we do so when when you hear someone saying the Apple fell from the you have the context of that statement and you know because of these words probably after that will come there and then maybe most probably a three there's a context of the Apple foring from somewhere to somewhere I don't know if I complicated more for you or made it simple I hope I made it simple but this is in a nutshell the um the transform architecture if you don't really want to know more about the details of it you can uh jump off the video here but now we're going to go into really really interesting stuff believe me it's intriguing and really interesting to learn how all of these blocks work we're going to go for it I will not uh uh go for like deep deep understanding of the equation and the math behind it I just want you to have that knowledge that will make you above 99% of people around you who understand artificial intelligence which will enable you to do much more if you ask me with AI even with chat gbt once you understand how this thing works obviously this happened in 2017 there are developments happening around it so I don't think today the way they train the models it's exact exactly like this but this is definitely is the DNA of it so 90% of what they use is the Transformer architecture that you're looking at and we're going to look after that at what's happening out there that could take us to the next level of the transform Transformer architecture what what is next there that could really even make AI models much much better okay so let's let's carry on so this is I gave you my understanding maybe you liked it maybe not but let's go and um this is just an intro to the to the concept and you can see it was this paper was published in 20 2017 it was uh the introduction of the transform architecture okay and and you can see here before I move forward um uh prior to the Inception of recurrent neural networks RN ends and their variation of lstm and Gru I don't know honestly what gru is write me a comment in the comment section if you know what this is were the go-to solutions for sequential data processing while effective these models had the itation particularly in terms of training efficiency and handling long range dependencies with data so it's less about the sequence here it's more about still the dependencies okay Enter the concept of attention this is why it's a brilliant title you know it's like a YouTube video If you give it a wrong title it will not work so they gave it a really good title attention is um is all you need so this is why this paper focused on attention so rather than processing data sequentially well I'm right so it's more about doing things in parallel so rather than processing data sequentially the attention mechanism allows the model to focus on different parts of the input data providing it with the kind of shortterm memory to discern what's essential this Innovation enabled models to capture intricate patterns and relationships in data with remarkable accuracy so why is the paper of monumental importance basis of subsequent Innovations the Transformer architecture forms the foundation for models like Bert I think this is the early name of Google GPT and T5 which have dominated NLP task ranging from translation to text generation so this is the translation which was the thing they everyone played with before to text generation this is the generative AI part elevated AI capabilities with the power of attention AI models can now generate more coherent and contextually relevant content this led to enhanced chat Bots improved search engines and more relat language translation tools again just to pause here for a second I think at this level you also in your mind you can imagine why the model hallucinates like the way it works the way the Transformer works of predicting the next word based on relationship it can easily get things wrong so Hallucination is part of um uh this kind of idea of how it works so the understanding of the details of how models operate I think it will help you even navigating your your interaction interfaces with AI models anyways democratization of AI the rise of pre-train models which owe their Genesis to the Transformer means businesses and developers without vast resources can now access state-of-the-art AI capabilities now doing things in parallel means and I'm assuming here that you need less computational power to train certain models um obviously with a specific number of parameters I think the more than parameters the more computational power you're going to need but and and this is one of the biggest problems in AI today we know that's totally expensive like the one of the most expensive technology uh to to perform today is training an AI model and um that's affecting things in so many ways so it's limiting certain Societies or areas or countries to have access to the technology so by doing these Innovations which were're not there yet I think in the future people are still working towards things that will require less computational power to get higher quality models you will give access to more people to benefit from this technology and that creates value you know I know that's different topic of AI taking jobs y yada these kind of Technologies they create lots of value that maybe one of the scenarios they remove the threat of that technology to people losing their jobs because lots of money lots of value will be created in the market so you want to make it also um as one aspect of looking at this techn ology of how do you make it accessible for more people again just a you know stop in the in the middle of all of this so let's carry on but these are some of the main importance and what this technology or the Transformer architecture brought to the world of artificial intelligence the attention mechanism explained imagine you're reading a lengthy novel and every time you come across a pronoun like he or she you instantly recall who it refers to from the previous paragraph or chapters instead of reading the novel linear from uh start to finish and understand every sentences context your brain intuitively attends to the relevant part that helps make sense of the current sentence I think this is a better way of putting what I try to explain but so these kind of pronouns when we read something we just keep relating them to the right character or the right thing basically so this is what the attention mechanism uh did for artificial intelligence this ability to refer back or pay attention to specific parts of the text for comprehens for comprehension is exactly how attention mechanism in neural networks like chat gbt okay let's take an simplified example let's do that I think the is like what's if it's simplified I want to work with it so the sentence Dr Lee who loves his dog Daisy often takes her to the park and gives her several good boys okay I think this is how you pronounce it or how you say it if you want to know who her refers to your brain would likely attend more to the world Dr Lee and dog Daisy to drive the context in essence you assign different weights to different words based on their relevance so the weights are basically what we get out of these um attention heads okay so let's go to the attention mechanism so but you understand the stage we want we want to get um want to give a Mark or grade or weight for certain parts of the context to to really understand it at least for the machine to understand it and this is what takes us to this mechanism words in a sentence are assigned different weights these weights determine how much Focus or attention each word gets when trying to predict or comprehend another word connecting to CH okay CH like other Transformer based models leverages this attention mechanism when you pae a question the model doesn't just look at your current message in isolation it attends to various parts of entire conversation context assigning weights to different parts based on relevance this ability helps the model to generate relevant and contextually appropriate responses so just as your brain attends to specific words in our example sentence to understand the reference to of her chib attends to different parts of the conversation to provide a coherent reply so it's not only looking at the input today it's looking at the input and the context of the conversation or the thread that you're having with chat GPT so imagine all of that amount of data is assigned weights and the model is able to tell what is more important uh to all the other words in the context for each word so it will predict the next one I hope I made that clear so again we go to the uh same architecture how it looks I went in my words of how that should be explained I'll just go to Snippets of the things Dr Lee is highlighting here and then we'll move to different sections of really digging deep into the sub areas of that architecture so the first one is embedding and Source sentence so here for example you get the catat on the mat so you input sentence the catat on the mat the model First turns these words into numerical vectors through a process called embedding we looked at embedding we're not going to dig deeper but again we turn text into vectors of numbers which will help the model understand in the space like where that word Falls okay and then we have POS positional encoding so this is the step we talked about and this is the one that assigns a sequence number for each word so the cat sat on the mat so this is a matrix single layer Matrix of um uh which has the numbers of each word so you you know the sequence of the words so we have 1 2 3 4 one again which is that and then five since Transformers don't inherently understand the order of words this step adds information about the position of each word in the sentence so the word cat might get some positional information indicating it's the second word so simply it's a number just to tell the model this is where the word Falls within that context in terms of sequence not attention multi-head attention or multi-headed attention so so this is the left side because we said we have the encoder and the decoder side left and right so cat and sat are important and related in the sentence so this is what happens it runs through each of the words against all other words in a matrix um based on the sum of the embeddings and the position and it will you'll see why I keep using the term heat map it will look at the numbers and let's say the higher the number the higher the weight the higher the attention it should give to that word and that context so this is the magic step where the model figures out which words in the sentence are important and relevant to each other here it might recognize the cat is closely related to sat because the cat is the one doing the sitting adding and Norm so we talked about normalization in in previous um networks when we talked about the RNN it was at that time making everything lower case let's see if this is the same one here after attention is applied the model normalizes the data ensuring it is in a format that is easy for the model to work with Okay I think this is is more around numbers nothing to do with the normalization we talked about full understanding of the input sentence and what words are important and what words are related so again this is pushing the data out of the attention multi-head attention so the attention data pushing it in the network it will get the network to understand okay so I need to focus on this and this and this fur further these TOS are linked these TOS have nothing to do with each other it's not that important of a connection so that's the the the actual Network that does this understanding based on the multi-head attention so this is just a simple neural network that does further Transformations on the data output embedding and output sentence so start assembling the translation and look at preer translations so this is basically an example showing a translation of this sentence um that does with the cat sitting on the mat so the decoder starts its process by looking at any prior translations initially it might just be the starting token indicating the beginning of the translation if you remember we had those kind of special tokens in the tokenization so the first token because there's no like the first word to be translated that doesn't exist yet so you'll have to start with it so there will be a special token that will um highlight or flag to the model that this is we're going to predict now the first word it will not take any real um uh output input basically maybe that will confuse you but it will not have anything coming in here um other than later on the output from the encoder so that will be the first word after that first word is out I think uh that will be the input uh for the uh second word to be predicted the decoder starts is processed by looking at any prayer translation initially it might okay so we just mentioned that U masked multi-head attention so uh on the right side the decoder we have two steps of the multi-head attention one of them is mask and one of them is un mask and when we say mask if if you ever dealt with images or Instagram if you do these things masking an image is just highlighting an area you want to keep and the area that you don't want to keep so this is the mass multihead basically do a little bit more to uh just mask out certain parts and keep certain parts this is the way I am understanding it if you think I'm wrong write a comment in the comment section so what are the best words to include in the translation based on what's already been translated but don't look ahead so basically this is all happening without considering um what's happening later on so not looking ahead and based on the words being translated focused only on the output this is how I understand it again um this will uh decide you know what to include and what not to include later on it will get mixed with what you're getting from the input embedding and or the the other output or the previous output that will change but uh for some reason we have to do it purely without masking it purely to the uh output itself and to the full context maybe doing both will give us higher quality kind of translation or generative um um context of the words that the AI is predicting the decoder pays attention to the previously translated words this is masked to ensure the model doesn't look ahead at Future Words which it shouldn't no yet okay so as I mentioned before the output will be multiple options and predictions so you don't want it to look at those future possibilities just focusing on the uh previously translated words only and later on the the output will be all of these multiple options with probabilities that to use to select the next word I know you know this can get more complex again the more you dig deep into the science behind it or the math behind it maybe more accurately you'll understand it further I will give you the tool to do that if you want to do that I didn't do it myself but I'll give you that tool um but just here I hope you're getting the flow of what what this is doing for us we do the first MTH multi-ad attention then we do the next one again each of them comes with the ADD and Norm or normalization so the second one is using the preer translation plus the new text that was input remember it is the it is represented as a number okay identify the best words Poss probabilities okay so here we mix both we take the prior translation the new text that was the input so we give it the first input and then it will start predicting the input so the first word will go to the input here maybe this is the part where I get confused when I look at the Transformer architecture if you have the answer correct me so let me read it one more time time using the preer translation plus the new text that was input the word here is new it's new text that was the input okay so maybe this is at the first run still this is the input that we were looking at so the cat sat on the mat it's the same one remember it is represented as numbers okay identify the best words probability okay so I'll take it as I understand it I'll not complicated basically it will take the um previous word it will take the input and it will start giving the probabilities of the next word once the probability of the next word comes that word will again go here to the input shifted right and this is the loop here this will keep looping I think so it will not go to the input the input is standard the loop will happen here in the decoder part this is why sometimes they call it the encoder decoder so I'm just presuming here now the decoder also pays attention to the encoder's output trying to figure out the best words to trans late next so it might look at cat and decide the next French word should be Shia I think or Shia I don't know I not try to translate here to French add a norm important from a tech perspective but not so much for an intuition perspective so okay we need to put numbers in a certain way um for us to put the probability and the words uh going forward and then again the data is normalized feed for the neural network does its Transformations and then we go to the linear and soft Max convert numbers toward so this is the opposite of embedding so this reverses the embedding taking the numbers putting them as words and I assume this is again going to something similar to the embedding where takes the numbers goes to the word in that kind of 3D space we talked about picks the words and drops it out this is the function that does that I assume um these steps convert the decoders internal data back into actual words after going uh through this the model might output the word sha or shat um as the translation for cat this process continues for each word until the full sentence I will not read it in French is generated remember this is a very simplified example in reality the model considers many possibility translations at once and uses complex math to choose the best words so this is what I I keep telling you about the output will be so many words with probabilities and the model usually selects the highest probability as the word that should come next okay but this should give you a basic idea of how data flows through Transformer when translating a sentence okay so again here is giving one more example I'll will not go through it you can read it but this is the the highest level or broadest level of understanding how the Transformer works now let's understand one concept before actually going to the multi head attention which I said the most important part of about the Transformer and then I'm going to look with you at the paper we will be repeating some Concepts when we look at the actual paper but I think it's very important after that it will be the fun part where we look at things we can have or the future what it will bring um I keep reminding you of the U because this is a long video of what's coming so let's move to the next concept before we move to the um uh the multi-head attention concept and then the paper of attention is all you need okay so we're going to go to look at something um these are efficients or coefficients or factors that are very important to understanding how the multi-head attention works the qk and V which is the query key and value um we're going to go briefly at them I don't expect you to really understand them 100% I don't understand them 100% but at least this helped me um grasp the idea of of how important it is to the calculation that goes behind the multi-head attention and based on that it helped me understand the multi-head attention more so that's the whole um reason I'm covering this here later on we'll go to the multi-ad attention it will be uh these kind of three the query the key and the value will keep coming up at least when that happens we'll have the understanding and will not be lost in in that explanation of the multihead ention so that's the whole that's the whole um idea of me covering this quickly as as quickly as I can um okay so let me read the introduction to this um article on LinkedIn uh by this Chief digital and Technology officer at constellar okay in the realm of natural language processing large language models llms like gpt3 and birth have revolutionized how machines understand and generate a human language at the heart of these model mod lies the concept of the qkv and the multi-head attention are the key it sounds cryptic to me at the very beginning and it takes me a few weeks to figure it out so imagine someone like that is saying you know it sounds something complex for me to to grasp took them took him um weeks to understand it but again so if if if so that's very humble to mention in such an article very welcoming for someone like me to read um so if you don't fully grasp it yeah the person telling us about it didn't get it from the first time but we just want to get the idea behind it so um the following is what has been explained in the paper so this is the scale do product attention and multi-head attention so so these are two types of attention mechanisms I assume but forget about these details let's look at the multihead attention this is what we want to really uh focus on you can see the way it receives the information if you remember in the original architecture when we looked at it once the embedding and the positional sequence happens we put them together and then we uh push them to the multi-head attention the way it goes in it goes in through these variables let's call them variables for simple terms which happen on a multi-layer so in parallel we have V which is the value key K which is the key and then Q which is the query and so let's let's try to understand them and see what what is the multi-head attention is getting as an input for it to be able to do some calculations to give us the map of where attention should go before it push pushes that to the feed forward Network so the query represents the current item that the model is focusing on in this in a sequence the query is like asking a question about a particular element okay so let's say we have the the cat sat on the mat let's let's work with the previous example when we are looking at cat cat is the query this is how I understand it so cat is the element um in the sequence that we are focusing on key represents all items in the sequence that the model could focus on the keys are what the query is compared against to determine how much attention should be paid to them so the key I would say is the other words so if we are if the query is cut uh the the key would be the sat on the mat so without the cat so it's all the other values that the query is measured against to see where attention is going the value each key associate uh is associated with a value once the model determines which keys are important based on the query the corresponding values are used to construct the output so each key is associated with value so I I think this is the value it could be an input but at the same time it is the main output out of the multi-ad attention I think so let's read on but this is the value that we have now we know what's the key we know what's the query so the query is the word the key is everything else the context around that word the value is what gives that weight let's let's use the original term of each of the key parts of where attention should go more or less when it compares to the query which is cat so let's read this part maybe this explains it a little bit more a simple sentence like Tom is going to fish at the river bank is easy for us to understand to let computers understand it we need to encode every word into numbers which is called word embedding that's we understand assuming in a simple six-dimensional space the word River can be represented as a word embedding of okay he gives it just numbers just to show an example of the embedding of the word River those words with a higher similarity will uh be close to each other for example Group One River fish and uh fish Fishman group two Hospital post office and restaurant it becomes interesting when we try to figure out where to put the word bank it is a policyi that can be interpreted differently based on the context of the sentence in which it is okay so again this is just about the embedding still and we we talked about it that 3D space and imagining how these groups of embeddings are um put together so different tokens are more relatable to each other so he's just explaining it in simpler terms the same way but he he he brings um um an interesting point so Bank the word bank it could be here closer to this group or closer to this group so river bank so how does it differentiate or the bank itself so how does it differentiate where this should be associated with I think this is his question here when we read it we know a bank cannot be the place where you draw money why well the presence of the words river and fish contribute more to our understanding of the context compared to the rest therefore they should have high attention scores and be closer to bank okay so let's go back to the important part where we're going through this article which is the query key and value how does a computer determine uh that it should pay more attention to River and fish and not others this is where query and key comes in they are two linear Transformations that help answer the question within this sentence what are the similarity scores among the words first okay so here we have all these linear matrices so we have each of them as a linear Matrix and you can see these are a bunch of numbers some of them when you do a certain calculation on it be closer or farther away from each other so I think river is the query so the river is the query we're looking at the key is everything else so the multi-dimension basically when you put them together um Matrix of the other words so this is just um visual depicting the uh key and um the query here so we have the bank and we have the river similarity okay so this is just uh showing you how all of these come together you can see the value which is um kind of putting together the query and the key so we have the width of the Matrix the dimensions based on the query I assume here here you have the rest of the keys um I think this is like inverted The Matrix is inverted here so all of this is mathematical operations that happen to the embedding and positional data that comes in to make sense of it the easiest way to put it and then you get the value this is the value that tells us okay this token is more important to this token and this token is less important to these token the output goes through the steps match mule scale mask and soft Max all of these are operations mathematical operations that are needed to prep the data to go into the multi-head attention and even later on to be pushed into the feed forward Network and even to be pushed as words that we get from the model now in the previous um article if you remember Dr Lee didn't want to go through that because he wanted to keep what's important to understand the context of the Transformer here it's going a little bit through that I want us just to grasp the concept of how the query key and value work so um just just uh understand that Matt mule scale mask and soft Max so we looked at mask like the mask multi multi-ad attention and the unmasked one so just understand that these are mathematical operations that happen to to manipulate the data this is the easiest way to put it we are manipulating the data to be able to get really the output that we want and there's lots of computing behind it and lots of Genius kind of mathematics people created to really get us to that so we're not trying to fully understand that today so let's carry on we then have the final output a weighed sum of these values where the weights are determined by how well each key matches the query so we're just matching that those keys to the queries we're giving it values the V and that's it we're good to go and uh use that information so the new embedding compared to the original one captures more contextual relationships I I think this is what happens in the multi-ad this is the mult like this is a single head of the multi-head um attention so it's all functions that the query and the key goes through eventually uh the value is added which we'll have to really uh grasp later on is the value just the input embedding or it's an outcome of the whole thing so but what I understand at least from what is being said and the graphic here is that the input is one which is the sequence and the input embedding we take it through certain functions that based on the query and the key will tell us what's attention should be given to which Keys compared to the query eventually that output will be different from the input embedding goes in with the value which go into that mat mule here and really give a matrix that contains the information of attention for the key and the queries and this will happen for each query within the all the possibilities of the keys in parallel uh in the multi-layers OR multi-heads of that attention calculation or manipulation of our data hopefully that makes sense so this is just a look at how maybe the possibilities are looking when you match each uh query with the keys and it generates numbers the higher the number the more attention is given to that key um I think this is interesting so um this just uh uh this part is just saying that is one set linear projection representing so-called attention head okay where okay yeah so so here it mentions that this is also um applicable to images somehow so you can use it for images to understand the objects in this image if you remember we covered the convolutional networks and I told you Transformers didn't only re revolutionized the neural networks the general ones it also affected the convolutional neural networks which has to do with images anyways so we understand the concept of q k and V those are the embedding now the query is the part of the embedding that handles the word that we're looking at the key is the input embedding that handles everything else that is in relation to the query and the value is in theory the weight that is given that could give us the the understanding of attention for these words they go through certain functions at the end we have um a big Matrix multi-dimensional Matrix complex one that contains all of these information it has the sequence it has the uh attention uh relationships between the words it has the queries and the keys and the values all of it is put in one huge Matrix that we later on if you remember normalize and push to the feed forward Network so that's just the concept that you need to grasp now we can actually go and look just at the Transformer um U when it comes to the multi-ad attention um and go a deep dive on that and then we'll go to the paper want to give more time for the part of the Transformer architecture that really gave it its its superpowers and the importance that it got and everything it brought to us from 2017 so far so this is a paper by kitan DOI around the multi-head attention particularly so let's scroll down okay so how attention is used in the Transformer attention is used in the transformer in three places self attention in the encoder uh the input sequence pays attention to itself so this is the self attention again this is the encoder this is the decoder if you remember and you can see that now I think all of you should if you paid attention you should understand what is happening position encoding embedding Target um this is yeah input and Target Target so this is the target word so I was correct so you understand how these thing work uh these things work so you you now have you're more Savvy when you look at the architecture you get it um we want to just dig deeper into this these parts of attention the encoder decoder attention self attention self attention we want to dig deeper in that and really try to understand it because this is where the money is when it comes to these um to this architecture so the first one is self attention in the encoder um the input sequence pays attention to itself self attention the decoder the target sequence pays attention to itself and encoder decoder attention in the decoder the target sequence pays attention to the input sequence okay this might have been very broad or not articulated in the detail but this is the beginning of this article but for us we understand is here this is the self attention for the input I'm just um simplifying it here this is the self attention of the uh expected output or the projected output this is the encoder decoder attention which is the attention for the expected output plus the what we get from the input and then this goes to the output probabilities in in layman terms okay let's go to the more details within those self attention mechanism the input sequence is fed to the input embeddings and position encoding which produces and encoded representations for each word in the input sequence that captures the meaning and position of each word okay so this we get which is the embedding again and the sequence position sequence um now let's carry on this is fed to all three parameters query key and value in the self attention in the first encoder which then also produces an encoded representation of each word in the input sequence that now incorporates the attention scores for each word as well so now see it's easier to understand because we already looked at the query key and value so we take the embedding and the positional encoding we um there's some manipulation that happened and out of that manipulation we take the query which is the word we're looking at we take the keys which is other words and the values we push them in a way that will give us let's say a body of information or Matrix that contains uh the attention relationships of the input uh keys uh that we have within our tokens okay so we now we have the input or the encoder attention figured out in a data block let's let's simplify the term to this okay so U so as this passes through the encoders in the stack each self attention module is also adds its own attention scores into each words representation okay so this is uh just one graphic to look at so we have input embedding and position encoding it goes to you have a m matx here okay there's a sequence and there's an embedding obviously this looks as a single layer I don't think so this will be um multi-layered even at this stage here and then some manipulations that we've seen before happen uh that breaks it to a value key and query these are again functional manipulation I want to remind you things that we play with the Matrix to get parts of the Matrix modified mat matrices with the same size or different size that we need to have to have the value key and query which each of them as we know now gives different uh piece of information all go to the multi head self attention which is again a different level of functions that run through these M different matrices and then we get the uh um uh the encoder output we go and feed it back to the second layer here but before we do that uh remember there's normalization here because the way the output comes out of the encoder has to be normalized before it goes to the encoder decoder attention in the decoder but here it's clear because here they're showing the normalization so here the same happened output embedding and position encoding this happened for the next sequence word this is the Matrix that we have then we manipulate it again for Value key and query it goes to the self attention manipulations of its own so different type of functions it will give us all of the information we have so far in this one big Matrix that we normalize and that Matrix that we normalized goes as the query for the encoder decoder attention this is like the interesting part here what we get here is from the input is the value and key so the only thing we're getting from the input is the attention of the query and it's a relationship to all the other Keys um while the query becomes the expected next output word in the decoder okay so decoder self attention now I explained it let's just read it quick coming to the decoder stack the target sequence is fed to the output embedding and position encoding which produces an encoded representation of each word in the Target sequence that captures the meaning and position of each word this is fed to all three parameters query key and value and self attention in the first decoder which then also produces an encoded representation of each word in the Target sequence which now incorporates the attention scores for each word as well so this is the same what happens in the encoder we understand it now we'll go to the encoder decoder attention which is the interesting one along with that the output of the final encoder in the stack is passed to the value and key parameters in the encoder decoder attention the encoder decoder attention is therefore getting the representation of both the target sequence from the decoder self attention and a representation of the input sequence from the encoder stack therefore produces a representation with the attention score for each Target sequence word that captures the influence of the attention scores from the input sequence as well as this passes through the decoders in the stack each self attention and each encoder decoder attention also add their own attention scores into each words representation so that's a mouthful the way I will simplify it to you so so and and this is why it's important to understand the the query key and value so when we do the encoder we break down the um inputs into the key value and query we look at the query versus all the other Keys We assign values to them we get the output as a matrix now the decoder part the output part as we are producing the um next token or next word and the probabilities we are doing that number one in relation to the words and that were generated and are being generated also we're keeping that as a query as a whole in the second layer we take it only as a query the output of the first um um decoder uh uh self attention phase on the right we take it as a query to the second uh encoder decoder so we look at the word that is being predicted in context as well so we have the information of the attention with the input itself so it's linking everything to each other so not only we do it once looking at the words uh uh where the input is how the input keys are related to each other we actually do it to the output also in relation to itself and in relation to the input so if you want to think about it all the words in the input and the output going through the Transformer are mapped in terms of attention of these words to each other all of them input the previous words in the output and the possible words in the output coming out so this is how we decide the next word so the more we generate output in relation to um a certain query we are looking at the input the previous outputs that were generated at high probability and the next word is decided at the attention based on all of these Keys together um uh uh put together in that complete context so we're not only looking at the input we're not looking only at the output we're actually looking at the whole the model is looking at the whole thing when it's predicting the next word this is the power that made generative AI be able to give us words it still hallucinates we know that but for many ways it gives us the next word which is highly probable unexpected um term or word within the model that is generating that makes sense even to us as humans because it's looking at complete con context now we're doing it with a very small examples like the cat at on the mat imagine that at paragraphs this is what you do with chbt and C today so the whole context is part of it and I assume today from what we've read so far even the thread the whole context of the chat the Open chat is part of that context and now in open AI they actually gave it memory abilities so everything that you interact with when it comes to chat GPT potentially is also part of that attention calculation um in the Transformer so this is like the the whole idea that was generated in the Transformer architecture gave that open door for doing all of these manipulations where you can actually generate real context in the data that you provide the model and you gave the model the ability to see that context that in the past it wasn't able to see at um in parallel which helped the Computing and where in the past as well it couldn't see it at longer context or longer sequences of dependencies okay before we move on to the paper let's just look at this the multi- attention head in the Transformer the attention module repeats its computations multiple times in parallel so this is why they call it um multi-ad attention so or multiple attention heads because it does it multiple times in parallel each of these is called attention head the attention module splits its query key and value parameters n ways and passes each split independently through a separate head all of these similar attention calculations are then combined together to produce a final attention score now the way I understand even the splitting of the query keys and the the values is done in different way I'm assuming if my understanding is correct correct me if you know more in the um um in the in the uh comment section is that it will do it for each query so each word in that context and the other keys so if we're looking at cat we look at the other keys if we're looking at Matt we're looking at cat as a key not a query um and so forth so we have that multi-ad calcul happening at all then all of it is merged and we have a matrix eventually that encompasses all of that attention relationships within it okay attention hyper parameters again the embedding size so here you know if you want to carry on with this part on your own this is interesting I did a quick read for this it just again explains simple to uh similarly to what we did already it shows you the way these uh matrices are being um manipulated changed modified masked whatever you call it for us to get the end result that we get so splitting of these matrices masking these matrices reshaping them you can see all of it here again for me I want to understand that this is happening you can see Swap and three shape I want to understand that this is happening to get to the result um maybe at a later stage I will go more into the actual calculation it happens for these matrices to get what we get um if you're interested I'm going to give you this um link as well as everything else you can see masking here and yeah so just understand that this is part of the manipulation now I think even if you stop here you understand the Transformer architecture for the most part um I want to spend little while on the paper itself I will not go through everything because at this stage I think we covered it and then I want to go to the Future and the cherry on the top me giving you certain tools that took me some time to find that are amazing for understanding this at a whole new level even more than what you can get out of this video so let's go to the um actual paper so this is the paper again I'll not go over the names of the great people that gave us the Transformer architecture the name which I said it's an awesome name so whatever whoever is doing this actually could be really good in doing YouTube videos attention is all you need okay I will read the abstract quickly the dominant sequence transduction models are based on complex recurrent or convolutional networks neural networks that include an encoder and decoder the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple Network architecture the Transformer based solely on attention mechanisms dispensing with recurrence and convolutions entirely so basically they're removing the whole concept of recurrence and convolutions and they're solving so many of the problems that we covered in those kind of attention heads that we talked about in the architecture where it encompasses all of the information the model needs really to be able to predict the next word our model achieves again this is these you'll see all of these kind of blus um these are what I read about online these are kind of metrics related to translation accuracy so don't worry about it so because the example or the testing they did the training for preparing this paper they did it for a translation kind of job so our model achieves 28.4 blus on the WMT 2014 English to German translation task improving over the existing best results including uh embl by over two Blu on the WMT y yada so just saying how well the job did or was done uh when using or when deploy deploying the Transformer architecture um so again I'll just go to certain things that I think are interesting uh to read everything else you can read it for yourself it is definitely something everyone should read um but we already I think covered the important parts of the Transformer architecture so let me go to the model architecture most competitive neural sequence transduction models have encoder decoder structure here the encoder Maps an input sequence of symbol represent representing X so these are the inputs to a sequence of continuous representation Zed given Z the decoder then generates the output sequence of symbols so these are the Y's the output um one element at a time at each step the model is auto regressive consuming the previously generated symbols as additional input when generating the the next so as we already explained it takes the input and the output that it predicted as an input as it predicts the next word and the one after and the one after and this goes through the encoder decoder stack so this is the original you can see even it looks cleaner the orig original architecture that was created um by this team okay so um attention I just want to read again lots of repetition here I know but it will just help so I want to to read you attention in their words an attention function can be described as mapping a query and a set of key value pairs to an output where the query Keys values and output are all vectors the output is computed as a weighted sum of the values where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key we already discussed that keys and uh queries relation and values scale do product attention Okay so these this is what again here here um uh what we discussed before but again these are the functions just to make it Crystal Clear if I didn't yet do that these are the functions that you have here so in the multi-head attention in each head in each layer you go through each of these manipulations or modifications that we talked about you can look any of them up and see the calculation behind them so just to make this crystal clear so we covered that we have the Q K and V we get everything that we concate later on basically put together group make it linear so it goes to the feed forward everything has to go linear to go to the feed forward neural network okay you can get the soft Max and equations related to it here and they talk about the multihead attention that we already covered again the equations are all mentioned here I I think it gets more complex than this they just put it in a paper for um for peer evaluation I would assume so people who are evaluating this paper probably know these concept cepts they're just giving them the high level modification of the architecture that they've done like what's they're bringing as a change all the other you know sub calculations that you need to figure out for you to be able to just look at these kind of seemingly simple calculations you must already have knowledge of um so don't think it's as simple I don't think it's as simple um and I'll show you something that will help in that sense later on applications of attention in our model again how the attention is used and positive wise feed forward Network embedding and softmax again how you embed something and how you turn it back to a word uh positional encoding again some details around how that works it's not simply one two and three there's um some some more details around that so in all the Articles we looked through it it was only simple sequential numbering which is um as you might have expected is not the way to do it or probably not the way to do it and then why self attention this is an important part I want to read in the section We compare various aspects of self attention layers to the recurrent and convolutional layers commonly used out for mapping variable length sequence of symbol representations again the inputs to another sequence of equal length which is the the the Zeds here with the um the this is actually the function that um compounds all of the uh summations of the inputs and the weights together such as hidden layer in a typical sequence transduction encoder or decoder motivating our use of self attention we consider three deata I think this is how it's pronounced one is the total computational comp complexity per layer another is the amount of computation that can be parallelized so done in parallel as measured by the minimum number of sequential operations required the third is the path length between long range dependencies in the network learning long range dependenc is a key challenge in many sequence transction tasks one key factor affecting the ability to learn such dependencies is the length of the path forward and the backward signals have to Traverse in the network the shorter these paths uh between any combination of positions in the input and the output sequence the easier it is to learn long range range dependencies hence we also compare the maximum path length between any two inputs and output positions in network composed of the different uh layer types so basically what they're saying here as I understand it is one of the the amazing things about the Transformer architecture is doing this multi- attention heads in parallel will reduce the length between input and the output and in simple terms it will allow it to contain or maintain more context or um more dependencies Within that range compared to doing things in sequence and this is was one of the problems that we had when it came to the uh recurrent neural networks if you remember and the convolutional ones which they refer to in the beginning of this paper as noted in taper one the self attention layer connects connects all positions with a constant number of sequentially executed operations whereas a recurrent layer requires o n okay this is the the sequential operation in terms of computational complexity self attention layers are faster than recurrent layers when the sequence length n is smaller than representation Dimension okay so this is again some of the equation context of this paper which is most often the case with s sentence representations used by state-of-the-art models in machine translations such as word piece um uh and bite pair representations to improve computational performance for tasks involving very long sequences self attention could be restricted to considering only a neighborhood of size in the input sequence uh centered around the respective output position this would increase a maxim okay we plan to investigate this approach further so at that time when they did it it wasn't you know figuring out everything they need to figure out but I think here what they're talking about is how this architecture would support reducing the computational requirements I assume a single convolutional layer with a kernel so if you remember when you looked at convolutional layers there's convolutions and kernels and again this is why I I touched on it in the beginning while I didn't dig deep on it um the Transformer was a big shift and big change for even the convolution neural networks so here they're just saying a single convolutional layer with a kernel width of so and so does not connect all pairs of inputs and outputs positions doing so require a stack of convolutional layers in the case of uh contiguous kernels or in the case of delated convolutions increasing the length of the longest path between the two positions again this is the path sequential aspect of the neural network but for convolutional networks convolutional layers are generally more expensive than recurrent layers by a factor of K separable convolutions however decrease the complexity of considerably even with k equal n however the complexity of a separable convolution is equal to the combination of self attention layer and the pointwise Fe feed forward layer um the approach we take in our model OKAY long story short even for the convolutional networks this optimizes the Computing requirement for training these models here they talk about the training with some equations numbers and results and then they conclude their um uh their their uh their paper here you can read it even you have a link to the original GitHub repository for the architecture if you want to go there I will have the link for this paper as well in the description if you want to go and see the actual code for this architecture as the same team that created the paper put forward the link is here so it's an interesting to look at now I'll spend short short amount of time to give you an idea of what coming in the future when it comes comes to um future architecture to expect and uh I'll give you at the end the amazing things that will help you understand this at a deep level if you want to do that now here this is um one more uh article that I'll give you the link I'll not go through it it will focus on you know what's what did the Transformer architecture change where it was um uh solving a problem what it's the downside like what are the gaps in the Transformer architecture and maybe what's the future to solve that uh it's a very interesting one just go read it it's it's interesting I I went through it quickly now there's the main architecture people are talking about today on the internet that could be um the future and the next uh stage of evolution after Transformer architecture is something called the Mamba large language model architecture and this is what they say you know this is the new paradigm in machine learning this is the new thing it's not fully cooked I think it's um as an architecture it's there I I'll give you the link to that architecture it's not stable yet it has its problem but it's deploying a technology that we'll touch on quickly that has high potential of taking you know this to the next level when it comes to Transformer architecture today and um going to the next level of contextual understanding of the um the the inputs that we have and the outputs within the L language model training but I think the the the main main aspect of the mumum is the comp the reduction of comput computational needs for training these models which will become something very important if not the most important things because at some point um training these models might become so expensive that it will be also so expensive to fail at these trainings because you expect that companies when they train the models they're taking a chance on the design it might not work it might work if it's super expensive not to mention that it will limit the accessibility to certain demographics but also it will deter corporates and organizations from actually training more and more models and it will slow down the technology we don't want that so there will be a big focus on reducing the computational requirement for these models so let's go to the important part of the Mamba to understand what the hell is the Mamba and then we'll move forward to the interesting stuff of the tools and the visuals that we can play with okay so this is as recent as 2023 so inate 2023 researchers from Carnegie milon and Princeton University published a research paper that revealed a new architecture for large language models called mamba mamba is a new state space model architecture so this is the technology I am talking about State space model architecture you can read about it more concerned with sequence modeling it was developed to address some limitations of Transformer models especially in processing long sequences and has been showing promising performance okay what is the mamba mamba is a new large language model architecture that integrates the structured State space sequence s for model to manage lengthy data sequences and I think this is much more in terms of sequence to what we have with a Transformer even today in 2024 so combining the best features of the recurrent convolutional and continuous time model so you see those are still alive the recurrent and the uh convolution S4 can effectively and efficiently simulate long-term dependencies this allows it to handle irregularity sample data have unbounded context and maintain computational efficiency throughout the training and testing so three main things it will allow it to handle irregularity uh in the sample data it will have unbounded context that's a big claim to make and it will U maintain competitional efficiency throughout training and testing so each of them is a breakthrough by itself now the architecture is promising all three expanding above the S3 uh Paradigm Mamba brings about several noteworthy improvements especially in handling time variant operations its architecture revolves around special selection mechanisms that modifies the structured State space model uh parameters according to the input as a result Mamba May successfully filter out less important data by focusing only on crucial information within sequence according to Wikipedia the model translations from a time uh invariant to a Time varying framework which impacts both the computation and efficiency of the system so it has I think this uh State space model is a different mechanism for uh capturing the context and the attention information within the model so the features the main features Again The Selective State spaces this is something to read about particularly spend time on it if you're interested all of it is something that is happening through the research remember I think at a point of time um we viewed the transformer in the same way people just improved it with time it became mainstream thing the same goes here so what we are looking at here could be in the future the state-ofthe-art technology when it comes to artificial intelligence that everyone talks about and we people will refer to 2023 when this paper came out from Princeton and Carnegie um whatever um the amazing people behind this kind of research in AI simplified architecture so it's simple in AR architecture Hardware aware parallelism so again um you know doing things in parallel will reduce the hardware requirement will make the barrier lower for computing and you know doing more with training these models so so again the link will be there you can go through the architecture but the most thing to to uh understand it's definitely an improvement um over the Transformer uh in theory they're saying it allows for unbounded context and it will reduce the computational power and it will handle irregularities in data so that's I think a big thing I don't know what goes behind the statement I'm sure it's more complex than just the statement itself so read through it it's very uh impressive and interesting I did some reading there now one interesting part that I'll give you another link where you can actually uh read it in a similar intuitive manner um some kind of simpler conversation around what it means how it's done but um also the architecture for the Mamba is on GitHub so so okay let's go just jump to the interesting part before we close this video if you're watching so far I mean I don't know you should write your your name in the comment if you really watched this far I'm not joking if you actually watched this far you should be a Super Fan um you know super friend maybe write a comment in the comment section definitely that tells a lot about your interest in artificial intelligence to encourage me even if there's one person who writes a comment with their name that they watched this video end to end one person for me made all of the effort of creating this video worthwhile um you know I helped someone this is the whole concept behind this um Channel helping people so if I helped one person perfectly fine I spend hours working on this if you're here you're that one person don't forget to subscribe to my channel channel and write a comment engage with me so I'm going to give you three things that you can use to build things on your own or play with with with these architectures on your own and one that you can use it to visualize everything we talked about and even show it to others so the first one is this uh GitHub created by this amazing uh person I found on X this is his handle here he has his oh his Tom is Tom y so he created uh this repository where you can download the the code base here and what you interact with is actually an Excel file so you don't need to be a coder at all just go to this repository which I'll have in the description download the code so you can click at the code here and download the zip unzip the folder and you can look for example at the Transformer open it you will see stepbystep calculation from the input until the output going through the encoder um self attention decoder and then the uh the the um unmasked um uh self attention of the encoder decoder and then you know the flattening and everything we talked about until the output in Excel you can see the equations you can see how it works it's an amazing thing if you want to dig really really deep using a tool that is intuitive for most of us uh as Excel please download it and if you go to his account here you'll find his all his um social media handles follow him again I'm sure someone like him not only he helped People Like Us who know uh or want to learn more around AI but also he might put forward things in the future that you know might interest you you can see I'm already starting his repository the second one is basically the mom repository so we talked about the Mamba architecture which has its issues but still it's on the Forefront of things this is the repository if this is your thing if you can if you know how to handle these type of code maybe you know with AI I never tried to do this myself to train a model usually doing like serious training of this will need um really super uh comput computational availability or a budget that you can do to do it on the cloud so if this is your thing actually the actual architecture Is An Open Source on GitHub for everyone to play with it um by the people who created it again uh under this account here so I'll give you the link let me start this one as well just in case so go and check that out if this is your thing the third one is an amazing GitHub repository that someone created that's the person here um let me start this one as well I already followed this account after I've seen the work that they've done this is basically an application once you run it it will create an amazing visual that will showcase to you um how the whole cycle of the Transformer work and really it will help you understand everything we talked about visually I think it's very important to go through the whole context to read these articles to have a robust understanding the visual will be a cherry on the top but even if you want to skip and just look at the visuals that's your thing that's also nice um the great thing this same repository if you don't want to you know download it or Fork it and run your own code I'll put the link they are already hosting it on a website which is this one and here you can actually see step by step the Transformer working so it's called the Transformer explained um and this is based off the deployment of the GitHub code and you can see here I already played with it a lot and you can see what I've done here as the input um in the word strawberry the count of R is and I'm getting one now the interesting thing here when you play with the temperature the probability if you see here on the right you have all the output probabilities that we talked about in the Transformer architecture uh is here so the as you play with the temperature and this is why you can tell the temperature is a very important variable um or hyper parameter in AI models it will change what the next word is okay so it's now the so at at I think this is one at one it becomes the next word is be becomes the and then and then zero so you can crank this up you know 16 10 you know I think one of them should be three at you know at some point at certain um modifications you can even change the input and see when you will get the actual correct number but now that we understand how embedding works and again when you play with this you can actually click on it see some details so you can see this is the token embedding happening here it's added to the positional encoding that we talked about all of that is pushed through the let's go to the actual multihead self attention so what do you see here these are the qk andv so all of them are done in different heads in different ways and you can see here uh in real time when you run it so let me just hit generate here so you can see how it it runs and you can see in real time if you hover I think over um so you can see here even the this is the heat map what I kept referring to as the heat map um is shown here if you click on this at one point I think you can see uh how the keys and yeah you have to hover carefully at one area yeah so now I got it to uh appear so here you can see the um qkv calculations and you can see it's happening for each of the qu Series against all other keys and it's doing it and it's generating this big Matrix this is the Q um QV KV weights all in one big Matrix and then it's adding the bias and then it's getting all of these again um different outputs uh for the multi um for the multi-ad attention um that we have here in the visual based on the this is the visual depiction of what you've seen here and you can see like how it links so you get the query in you get the key in you get the value if you remember directly to the output all of that goes out of the multi-head self attention box here where we do the normalization and then you have the feed forward Network this is the feed forward Network and then we start getting the 11 more identical Transformer blocks okay these are the outputs and all the probable out um um next word so all the probabilities for the next token or output and us ually you get here what is um the most probable uh next output and you can by the way you can just highlight this write something else so my name is let's say see what this generates you hit generate it will run the whole network for you and it will give you um the next probable word again this is based on point6 temperature so it is predicting that my my name is a so which doesn't make sense let's push it to point8 it's giving something else so you can put for example I put my name my name is samur run again and see what I get as next so the whole concept of this amazing work by this group is to predict the next word and through the Transformer architecture but also view it in real time how it works look at the calculations of the qk V how it happens look at you know what goes into the normalization look how that's pushed after flattening into feed forward Network and how you have all the probabilities as outputs so I know it has been a really long one um I hope you did you got more value than you suffered of me talking and reading all the time and just explaining things over and over and over again it took me really a similar effort um to do these things to go through these things to really grasp them well I'm not an expert yet but I'm on that way where I think I have a much better understanding than the average person when it comes to not only how neural networks and AI works but also going at a deeper level of really understanding how the um Transformer architecture Works what's coming in the future how the calculation in theory happen and why it works as it works and basically answering um or understanding or learning from one of the people of this community who told me well that's why doesn't get the strawberry test now I get it so thank you for triggering this whole boring movie and I know it might not get more than 10 views I'm good with that if I get one person to comment in the comment section if you enjoyed it even if you got like from one of the chapters like through one of the minutes in this long video I assume value please consider subscribing to my channel and with this I'll keep it short thank you and goodbye subscribe to the Daddy's Channel give me the mic"}], "Multi-headed attention": [{"content": "[Music] hi uh welcome back so this is where we stopped yesterday where we were talking about the self attention block within the Transformer architecture and uh we saw that the entire self attention can be uh done in parallel for all the capital T tokens uh and uh it all happens through these Matrix multiplications right so in effect what is happening is the following right so this is what is known as a scaled dot product self attention and this is called one head and soon we'll see multiple such heads but we'll get there when we get there but for now just remember this is called sell scaled dot product based self attention so what exactly is happening here so this purple box here right this is the scale do product unit right so this is what is lying inside this purple block Here and Now what is the input to the Box let's see uh so you get the key uh query and the value matrices how are these constructed originally remember at the input all you had were these H1 to HT right so you had these word representations for the t-word and we were calling them as H1 H2 and so on right now from the H1 H2 what happens uh you pass them through the linear projection right through the WQ W K and W V matrices to get the Q K and V matrices right so this is just multiplying WQ by these vectors and you just stack up these vectors into a matrix so that you can do just do WQ multiplied by that H Matrix right let's just call it h then you get the Q Matrix similarly WK m mped by this gives you the K Matrix and WV multiplied by this gives you the V Matrix right all of this happening in parallel as uh three Matrix operations then you get the qkv and then this is what happens inside the uh self attention head and at the output what do you get you get uh uh zed1 you get Z1 Z2 all the way up to ZT right so this is what we had seen uh when we ended the last lecture now this is called one head so what what do I mean by a head here so this is one such unit which takes the inputs H1 to H2 and gives you the elements zed1 to Z2 right or the refined representation Z1 to ZT which also take care of the contextual information because they depend on the key they depend on the value and they depend on the query right so they take care of the contextual information uh now you could have one more such block right so you could have the same block repeated so let me just call this Z1 1 Z2 1 and z uh 1 T right so this is the output of the first attention head similarly you could have the same block repeated where you have another set of parameters right so let me just call these uh W1 right these are the first set of weight matrices you have similarly you could have another repeated block right and let me just show it on the next slide we just repeat these two blocks right and now you have head 1 head 2 and head 1 is giving you uh representations zed1 and this is giving you representations Z2 now there are several questions here right why would you have two heads and then if you had getting these two representations which one do you consider right so we'll answer those two questions but first let us motivate right why would you need multiple heads and we have already seen this in a different context before right so we want to motivate multi multiple heads in attention right so one head is uh one uh one one scale do product unit which gives use Z1 to ZT right the representation Z1 to ZT and I'm making a case for many such hits so why why is why am I doing this so we have already seen this in the context of convolutional neural networks right so where we had multiple filters right so these are three different filters uh operating on the same image right and each filter essentially does the same thing it has parameters say W1 W2 W3 it just goes over the image and gives you an output feature map right and the reason we wanted to capture have multiple filters is that we were hoping that each filter May capture a different characteristic from the image right some may detect edges some may detect blurs and so on right so that's why you had multiple filters right and more the filters the more abstract representations you could compute right the same argument holds here if you have one self attention then it will capture it'll learn one way of capturing the contextual information right but there might be more than one way of capturing this contextual information right so you might also want to have a situation where it focuses on animal with a very high attention and you might also want it to focus on some ver with a very high attention right and both of these might be important because one is indicating that it is a pronoun for animal and the other might be indicating that it is the object of this work right and so in both both of these you might want to capture so one head could learn to give more attention to animal the other head could learn to give more attention to this work right so just as you had multiple filters to capture multiple characteristics from the image you could have these multiple attention heads right and let's just look at it a bit more uh carefully right so here's an example so the animal didn't cross the street because it was too tired right and I have the same copy of the sentence here and now I'm trying to learn a contextual representation for it and now I want to focus more on was right because I want to know was what is the subject of it right so it is the subject here so I want to know that so I want to capture that information by paying more attention to was right but I also earlier made a case so what does that mean if I'm learning the alphas right so if I'm learning say let's this is 1 2 3 4 5 6 7 8 9 10 this is a 10th word so if I'm learning Alpha 10 then this is Alpha 10 1 Alpha 10 2 right and this is Alpha 10 11 right so this is the uh attention that should be paid on the 11th word when you're Computing a refined representation for the 10th word right and I want this to be high right but I had also earlier made the case that I want the attention on animal to be high because uh animal is the I mean it is referring animal animal is the word for which it is the pronoun in this case right so that means if I were to compute again the weights Alpha 10 and so on then I want this to be high so I also have a case for this to be high I also have a case for this to be high so so one way of dealing with this would be have two separate attention heads right and compute two Alphas one from one block of self attention another from the other block of self attention and this block could learn to give more importance to this Alpha and the other block could learn to give more importance to this Alpha right now of course uh this is slightly make believe right we understand that because we already visited this in the case of recurrent neural networks that there is no signal right we are not telling the model that hey you need to pay more attention to V or hey you need to pay more attention to animal we just hope that when we looking at the final loss function and if it indeed is beneficial to focus more on V right that means have a higher weight for Alpha which in turn will uh contribute uh accordingly when we try to compute the refined representation for uh say uh the 10th word right uh and that effectively reduces the loss so it would then learn to have a higher weight for Voss right similarly if it helps so this is is z 110 similarly if you had Zed coming out from the other attention head and this will also participate in the fuel computation and then participate in the loss so if it helps that now this set of Alphas should be such that it should focus more on the word animal and if the loss dictates that then the machine would learn that or the model would learn that right so that we're not giving it an explicit signal we're just hoping that by Trying to minimize the loss it has more freedom now in some cases it can learn to put a high alpha here in some cases it can put a learn to put a high alpha here right so you're making a better design Choice by allowing it more uh uh choices right or allowing it more parameters in terms of the W's the projection matrices which in turn result in different Alphas right so just making it a more flexible model uh which has more choices or more options to how to adjust the alpha in different cases so it might choose to have Alpha was high for one case Alpha animal high in the other case and then do something something different in the third head and so on right so that's the motivation for having multiple heads I just already explained this I'll just skip over this the same thing whatever I explained that in one case it would want to focus on V the other case it might want to focus on animal and this is actually from a actual train transformer right so we looked at the uh attention weights there were two heads and we looked at what the attention weights were and we found that in one case it is giving higher in one of the heads it is giving higher attention to animal and the other head it is giving higher attention to WS right so it does learn to do such things right so this is what the two-headed attention would look like you would have the Q uh uh query uh sorry you have the query key and value vectors right which are coming from your projections sorry this should have been this is the H Matrix right which is H1 H2 all the way up to HT and you get out here is qk V right so this is again the H Matrix and what you get out here is q k v right so you just have these two copies now this as I said would release zed1 Z2 all the way up to ZT and I'll just call it Z1 one and so on and this would give you Zed 1 Z2 all the way up to ZT and I'll call it z2s right now what do I do with this I have two representations now computed for this word H1 to T right so how now what do I do with these two Zeds simple I just concatenate them so that's all I'm going to do so you concatenate it so you get a larger representation and then you pass it through a linear transformation right so we'll uh see that uh soon and then what you get is the final output right so what let's just look at this carefully right what is happening here so uh again let me just look at some uh okay this important that I get get rid of this right so this is the H here okay and let's just focus on H1 right now H1 suppose H1 was a a 512 dimensional Vector right so now what I could do is uh I will choose W to be 512 cross 256 okay I'm just giving you some example so that means the U projection which comes out right my Q KV would be 256 dimensional right because it's 512 multiplied by a 512 cross 256 Matrix right or rather actually this would be um yeah so you get it it's I'll get I'll just project it down so this will be 256 dimensional right so at the output again I'll get 256 dimensional Zs in both the cases now when they concatenate I again get a five1 two dimensional output right and this I could again pass it through whatever uh transformation I want right for example I could choose a 512 cross 1 1224 I could choose a 512 cross 256 or I could choose a 512 cross 512 right and depending on that I will know what my output Dimension would be if I choose this then my Z final Zs coming out of here would be 5 and two dimensional right so let's just understand it correctly so this H1 gave me a 256 dimensional zed1 this through this network or through this self attention it I got another 256 dimensional H1 and then I got five and two dimensional uh output here which then again I pass it through a linear transformation right so it's because you're going to concatenate it makes sense that you the output of each of these is small right because if each of these is five and two dimensional then you concatenate you'll keep growing larger right and typically use eight heads so now if each of this is 5 and two dimensional and then you concatenate them then you'll get a 4096 dimensional output here which is too large right because it increases the size of the parameters that will go through a linear transformation and so on right so typically what you do is if you want 5 and2 dimensional size here right then you make sure that your each of your eight heads gives you a 64 dimensional output so when you concatenate them you get a five and2 dimensional output so you start with a five and2 dimensional output you adjust these Dimensions such that you get a 64 dimensional output at each of these heads you have eight such heads so when you concatenate them you'll again get a 5 and two dimensional output then you do an appropriate linear transformation you could choose this one so that your final Zs are also five and two dimensional right so that's what you could U do right so this is what a two-headed attention would look like and I've already told you how to extend it to multi heads you'll just have the same U block repeated as many times as you want and then finally you would just adjust all these Transformations right so as I said I could uh let's just look at it again if my hes are five and two dimensional these are hes then I pass them through I multiply them by a 64 cross 512 Matrix so I get 64 dimensional outputs here 64 dimensional outputs here same happens in all the eight heads so when I concatenate them I get a five and two dimensional output right so whatever I started with I get the same so I can just adjust the parameters accordingly and then I do a linear transformation to get my final uh Z1 to ZT right so remember this concatenation is happening per word right that means uh the Z1 representations coming out of each of these are getting concatenated here then the Z2 representations coming out of each of these are getting concatenated here right uh so it's per word so the input is a set of words you have capital t word embeddings and the output at this layer or at every layer right here here here at all these layers the output is again uh capital T embeddings right so that you should remember that okay so we are done so we have the uh multi-headed attention Okay so we are back to the basic block that we had so this is what we had we had these uh uh inputs coming in here right and then now we have seen this self attention in detail which could be a multi-headed self attention and I give it inputs H1 H2 HT and then through all the uh processing that happens inside I get outputs I think I was calling these as S1 S2 all the way up to St right so once I have got this now I need to understand what happens in the feed forward neural network right so let's focus on that now right and this encoder is typically a stacked encoder so you'll have six such blocks that's why I'm calling this a basic building block this is one layer right so you passed in H1 to HT you got out Z1 to ZT now this Z1 to ZT becomes input to another such layer and again you get a new set of representations out from your capital t representations out right this we have seen that the output of one layer actess the input to the next layer right so all of this looks identical in all these blocks and there could be 6 8 12 such blocks depending on the Transformer architecture that you're looking at now let's see what happens in the feed forward Network so now uh you had uh uh you so these are what the final output is of the feed forward network is Zed this intermediate output coming out of the self attention I should have called it s and this is the input H1 right so now what exactly happens in the feed forward neural network nothing it's quite simple so remember that each of these guys here is a 51 two dimensional representation or some D dimensional representation it is going to pass through a feed forward neutral Network and again give you a d dimensional representation at the output right that's all that is happening here so feed forward neutral network is only acting as a uh projection uh layer here right so um yeah so this is the input S1 okay there's some intermediate being computed let me just call it uh say m okay right and then you get Z1 at the output right so this could again be 5 and two dimensional input one2 four dimensional projection and then again five and two dimensional output and of course there would be a nonlinearity here you could use uh any nonlinearity that you want right uh and typically it is um one of the G based either G or one of those Nom uh nonlinearities okay and the same set of parameters right so this here would have some parameters right so you'll have some W's here and then some another set of W's here right so let me just call them W feet forward Network and let me call this W1 because it's layer one and W2 Layer Two right so the same set of parameters will be used everywhere right so each of these s1's or Si will pass through the same transition and give you the corresponding zi right so that's what I'm going to show with the animation that the same network is essentially being used everywhere right so you get this same output everywhere right so this uh uh yeah so you have the same network for each position and uh uh you use uh this uh as the this is the nonlinearity that you're going to uh use right uh so nothing great happening within the feed forward neural network whatever output the multi-headed attention gives it just uh projects it and then gives you back a final output right so that's all we are done uh with the uh yeah so that's all we are done with the uh uh encoder layer right so this is one layer of the encoder and now I could stack many sets layers but each layer the internal working would remain the same just the output of the previous layer will be the input to this layer right so nothing else changes right so we are done with the encoder part of the Transformer so the encoder is composed of n such identical layers and each layer is composed of these two sub layers one is the multi-headed attention and the other is the feed forward neural network uh and uh so the computer is computation is paralled in the horizontal direction right so what do I mean by that is that you of course so if you have these n layers right of course you cannot compute all the layers in parall right because Layer Two will take the output of layer one as input right so unless you have done the layer one computation you cannot do the layer two computation right so when I say it's parallelized it's only within each layer right so for a given uh set of U yeah input samples right within that layer all the self attentions all the alphas all the Zeds they'll all get computed in parall right unless I mean earlier when uh again I'll just repeat this because this is important in the case of an RNN when you were given H1 to HT and you had to compute Z1 to ZT right you first had to compute Z1 then Z2 then ZT and so on right and here we saw that using this large Matrix multiplications we get Z1 to ZT in parallel right you don't have to wait for the previous time step for the next time step to be computed right so this parallelism you see in every layer but of course across layers the computation is still sequential right because you need the previous layers output to do the next layer's computation you have the input then the layer one outputs get computed then it feeds to Layer Two and so on till the end and each of these T cross 512 outputs get computed in parallel right and now this final output of the uh encoder right which is the output from the last layer we are going to denote it as e right so we'll refer to it as E1 to E capital T because there are t such tokens in the input and for each token you get this final refined representation which is contextual as well as gone through several layers of abstraction or several uh deep players right"}], "Unreal Engine 5 BP Tutorial | 2.4 String Datatype | 2. Variables | Aditya Burgula": [{"content": " Hello friends, welcome back to another brand new video. I am Alithya and in this video we will be taking a look at a new data type and share Unreal Engine such a string. So this data type is very commonly used in various programming languages and even in Unreal Engine. So with our machine, let us get into the video. So friends first let us understand the data type of string. The string is simply a data type which can store a sequence of characters. So now what do I mean by characters? It can be pretty much anything from letters to different symbols to numbers. But one main thing that differentiates between a string and an integer data type or a flow data type is that you cannot perform calculations because it is not a number. A string can have pretty much anything and if you take A and B you can't add, you can't multiply A and B, you can't do any kind of mathematical operations on these. So that was the base concept of it."}, {"content": "So now let us see how we can create it. So here we have the plus button and here I will just create it to use a person's username inside the game and let us change it to string. So once it is changed we can drag and click get username and same way we can also set it to name. So this is about getting and setting username. So if I click on this and we go on the details tab you can see that is almost the same properties as with other data types. So I will just compile and here you can see username. Now here we can give a value. So I will give like something like this or random value which includes all the characters add the rate.com like something like mail maybe. So it's just random string value. Now we're using the same print string we can actually print it to the screen. So we can do a big and play. So it is already connected over here. So I'll just drag this and using this node you do a print string and do this. Now we can also reset the value and here we'll set it to hello. So you can also show a simple text. So this is what is actually taking as an input as you can see over here when in the print function. So it takes input as string data type. So we'll be taking a look at that more detail later. We can do print once again. So here it is printing both the values. So that was about kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We're taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now we'll just keep it to the introduction level. And we come to the operator section that time we'll take a deep look at how we can do operations or strings. So thanks for watching nice."}, {"content": "I hope you liked this video today. Please don't forget to hit that like button, share button, subscribe button. So see you in the next video. Bye."}, {"content": "I hope you liked this video today. Please don't forget to hit that like button, share button, subscribe button. So see you in the next video. Bye. "}, {"content": " Hello friends, welcome back to another brand new video. I am Alithya and in this video we will be taking a look at a new data type and share Unreal Engine such a string. So this data type is very commonly used in various programming languages and even in Unreal Engine. So with our machine, let us get into the video. So friends first let us understand the data type of string. The string is simply a data type which can store a sequence of characters. So now what do I mean by characters? It can be pretty much anything from letters to different symbols to numbers. But one main thing that differentiates between a string and an integer data type or a flow data type is that you cannot perform calculations because it is not a number. A string can have pretty much anything and if you take A and B you can't add, you can't multiply A and B. you can't do any kind of mathematical operations on these. So that was the base concept of it."}, {"content": "So now let us see how we can create it. So here we have the plus button and here I'll just create it to use a person's username inside the game and let us change it to string. So once it is changed we can drag and click get username and same way we can also set username. So this is about getting and setting username. So if I click on this and we go on the details tab you can see that's almost the same properties as with other data types. So I'll just compile and here you can see a username. Now here we can give a value. So I'll give like something like this or random value which includes all the characters at array.com like something like that. mail maybe so it says random string value. Now we're using the same print string we can actually print it to the screen so we can do a big and big so it is already connected over here so I'll just drag this and using this node we do a print string and do this. Now we can also reset the value and here we'll set it to hello so you can also show a simple text so this is what is actually taking as an input as you can see over here when in the print function so it takes input as string data type so we'll be taking a look at that more detail later we can do print once again so here it is printing both the values so  That was a basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We will be taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now, we will just keep it to the introduction level. When we come to the operator section, that time we will take a deep look at how we can do operations or strings. So thanks for watching nice."}, {"content": "I hope you liked this video today. Please don't forget to hit that like button, share button and subscribe button. See you in the next video. Bye."}, {"content": "So now let us see how we can create it. here we have the plus button and here I'll just create it to use a person's username inside the game and let us change it to stu. So once it is changed we can drag and click get username and same way we can also set username. So this is about getting and setting username. So if I click on this and we go on the details tab you can see that's almost the same properties as with other data types. So I'll just compile and here you can see username. Now here we can give a value. So I'll give like something like this or random value which includes all the characters, add the rate.com like something like mail maybe. So it's just random string value. Now we're using the same print string we can actually print it to the screen so we can do a big and big so it is already connected over here. So I'll just drag this. and using this node, we'll do a Princeton and do this. Now we can also reset the value and here we'll set it to hello. So you can also show a simple text. So this is what is actually taking as an input. As you can see over here, we have an end-to-print function. So it takes input as string data type. So we'll be taking a look at that in more detail later. We can do print once again. So here we see it is printing both the values. So that was what kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We'll be taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now, we'll just keep it to the introduction level. When we come to the operator section, that time we'll take a deep look at how we can do operations or strings. So thanks for watching nice."}, {"content": "I hope you liked this video today. Please don't forget to hit that like button, share button, subscribe button. So see you in the next video."}, {"content": "So now let us see how we can create it. here we have the plus button and here I'll just create it to use a person's username inside the game and let us change it to stu. So once it is changed we can drag and click get username and same way we can also set username. So this is about getting and setting username. So if I click on this and we go on the details tab you can see that's almost the same properties as with other data types. So I'll just compile and here you can see username. Now here we can give a value. So I'll give like something like this or random value which includes all the characters, add the rate.com like something like mail maybe. So it's just random string value. Now we're using the same print string we can actually print it to the screen so we can do a big and big so it is already connected over here. So I'll just drag this. and using this node, we'll do a print string and do this. Now we can also reset the value and here we'll set it to hello. So you can also show a simple text. So this is what is actually taking as an input. As you can see over here, even in the print function. So it takes input as string data type. So we'll be taking a look at that more detail later. We can do print once again. So here we see it is printing both the values. So that was what kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We'll be taking a look at some of the operations which we can do with strings. That is concatenation or addition operator. But for now, we'll just keep it to the introduction level."}, {"content": "When we come to the operator section, that time we'll take a deep look at how we can do operations or strings. So thanks for watching nice. I hope you liked this video today. Please don't forget to hit that like button, share button, subscribe button. So see you in the next video."}, {"content": "So now let us see how we can create it. So here we have the plus button and here I will just create it to use a person's user  the game and let us change it to string. So once it is changed we can drag and click get username and same way we can also set username. So this was about getting and setting username. So if I click on this and we go on the details tab you can see that is almost the same properties as with other data types. So I'll just compile and here you can see a username. Now here we can give a value. So I'll give like something like this or random value which includes all the characters, add the rate dot com like something like mail maybe. So it's just random string value. Now we're using the same print string we can actually print it to the screen. So we can do a big and play. So it is already connected over here. So I'll just drag this and using this node we'll do a print string and do this. Now we can also reset the value and here we'll set it to hello. So you can also store a simple text. So this is what is actually taking as an input as you can see over here even in the print function. So it takes input as string data type. So we'll be taking a look at that more detail later. We can do print once again. So here it is printing both the values. So that was about kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We're taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now we'll just keep it to the introduction level. And we come to the operator section that time we'll take a deep look at how we can do operations on strings. So thanks for watching nice."}, {"content": "I hope you liked this video today. Please don't forget to hit that like button share button subscribe button. So see you in the next video."}, {"content": "So now let us see how we can create it. So here we have the plus button and here I will just create it to use a person's username inside the game and let us change it to string. So once it is changed we can drag and click get username and same way we can also set it to name. So this is about getting and setting username. So if I click on this and we go on the details tab you can see that is almost the same properties as with other data types. So I will just compile and here you can see username. Now here we can give a value. So I will give like something like this or random value which includes all the characters and create.com like something like that. mail maybe so it says random string value. Now we're using the same print string we can actually print it to the screen so we can do a big and big so it is already connected over here so I'll just drag this and using this node we do a print string and do this. Now we can also reset the value and here we'll set it to hello so you can also show a simple text so this is what is actually taking as an input as you can see over here even in the print function so it takes input as string data type so we'll be taking a look at that more detail later we can do print once again so here it is printing both the values so that was about kind of basic overview of the string data type I hope you have got an idea of how string data type works in Unreal Engine we're taking a look at some of the operations which we can do with strings that is concatenation or addition operator but for now we'll just keep it to the introduction level and we come to the operator section that time we'll take a deep look at how we can do operations on strings so thanks for watching nice I hope you like this video today please don't forget to hit that like button share button subscribe button so see you in the next video"}, {"content": "So now let us see how we can create it. So here we have the plus button and here I will just create it to use a person's user name inside the game. let us change it to string. So once it is changed we can drag and click get username and same way we can also set username. So this was about getting and setting username. So if I click on this and we go on the details tab you can see that has almost the same properties as with other data types. So I'll just compile and here you can see username. Now here we can give a value. So I'll give like something like this or random value which includes all the characters, add the dot com like something like mail maybe. So it says random string value. Now we're using the same print string we can actually print it to the screen. So we can do a big and play. So it is already connected over here. So I'll just drag this and using this node we do a print string and do this. Now we can also reset the value and here we'll set it to hello. So you can also just simple text. So this is what is actually taking as an input as you can see over here. and the print function. So it takes input as string data type. So we'll be taking a look at that more detail later. We can do print once again. So here it is printing both the values. So that was what kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We're taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now we'll just keep it to the introduction level. When we come to the operator section that time we'll take a deep look at how we can do operations on strings. So thanks for watching nice."}, {"content": "I hope you like this video today. Please don't forget to hit that like button, share button, subscribe button. So see you in the next video."}, {"content": "So now let us see how we can create it. So here we have the plus button and here I will just create it to use a person's user name inside the game and let us change it to string. So once it has changed, we can drag and click get username and same way we can also set it's a name. So this is about getting and setting username. So if I click on this and we go on the details tab, you can see that's almost the same properties as with other data types. So I'll just compile and here you can see username. Now here we can give a value. So I'll give like something like this or random value, which includes all the characters at the rate.com like something like mail maybe. So it says random string value. Now we're using the same print string. We can actually print it to the screen."}, {"content": "So we can do a big and play. So it is already connected over here. So I'll just drag this and using this node, we'll do a print string and do this. Now we can also reset the value and here we'll set it to hello. So you can also show a simple text. So this is what is actually taking as an input. As you can see over here. and the print function. So it takes input as string data type. So we'll be taking a look at that more detail later. We can do print once again. So here it is printing both the values. So that was what kind of basic overview of the string data type. I hope you have got an idea of how string data type works in Unreal Engine. We're taking a look at some of the operations which we can do with strings that is concatenation or addition operator. But for now we'll just keep it to the introduction level. When we come to the operator section that time we'll take a deep look at how we can do operations on strings. So thanks for watching nice."}], "#1 Installing and Setting up .NET Core and VS Code | C# Programming Tutorial | Aditya Burgula": [{"content": "[Music] hey guys welcome to another brand new video of the dot net and c sharp tutorial series i am aditya and in this video we'll be installing and setting up our computer for dotnet and c-sharp development so guys first let's open a browser and for running any dotnet application or c-sharp application you need the.net framework installed in mostly windows applications you get it already but we are going to use a.net core as it is cross platform and even if you're a mac user the process will be almost the same so i can just write dot net core download and after that you get an official link from microsoft that is net core linux mac os and windows so the process will be almost the same so guys after you open the website as you can see dotnet.microsoft.com i'll be keeping the links in the description below also so after you open this you are going to get like this like all the versions of netco and you must just choose the recommended version right now it is 3.1.net core 3.1 so click on that and it will start downloading and after that the setup is also very simple uh you must just go to the default options just like okay okay install and it will do the thing it may take a little while and after that we'll be installing vs code which is an ide and guys i hope that right now we have installed.net core and guys now we need an ide so an ide is something like a development environment where you actually write your code in and makes it much easier for us to write code because it has some features like intellisense or code completion and then it provides you with some tools extra tools for faster coding so like that visual studio is also one of them for actually.net there's even an id called professional id called as visual studio it is has it has versions depending on the er like visual studio 2017 2019 the latest one is 2019 we'll be not installing that because that's a heavy software kind of that and it takes a lot of time to download all this stuff and it's actually a very huge process and this virtual studio code it's a lightweight editor supports multiple languages so that's where we'll be using this so here i'll give the link in the description for this site also and here you can see for windows depending on your operating system you can download it the installation process is very easy and after you install i'll see you in the visual studio code page so guys right now after you open the visuals through your code after installing it successfully you might get a screen like this i actually have a different theme for you it might be a little much more darker and the first thing which we are going to do is use the terminal and check out.net installation i could actually have using the normal terminal or command prompt in windows but we'll be learning about how we can use the terminal inside vs code so here you go to the terminals tab and open terminal where and you can even see a shortcut over here you can use that so i'll be going like this and here you can see here it has opened a panel for this this is nothing but a command prompt and even you can choose what kind of comment prompt you want to use like there's windows command prompt powershell and git bash for this video we'll be just using the cmd normal command prompt and guys to check the dotnet installation you can just write dotnet dash dash version and press enter and if it was successful it will show you all these things so here we can see dotnet version 3.1.2 so now this installation was successful and now let's configure vs code for dot net on c shop development for that we must go over here which is called as extensions tab and click on that and here you must search for c sharp so i actually have already installed so if it won't be installed for you so you must search here for vs code c sharp and after you get that and you'll get a window like this somewhat like this and here you will get an install button here as you can see i have options like disable uninstall and set color theme whereas you will get an option like install and you must install that after installing you're done with setting up an extension or all the required plugins for c sharp and now we must just open a folder to work with c sharp so guys first uh here you can go over here you can see file and here we have options such as new file new window new open file open folder so here we'll choose of open folder and here i'll go to c-sharp and here again i have a folder and i'll be opening this and here you can see we have a c-sharp folder opened over here you can see the name of the folder and i'll just close this welcome screen and here we must open terminal and click new terminal and write dotnet new console so press enter and it will create the kind of a template basic template for writing c sharp code so guys right now you can see it has successfully completed as you can see here we got the message as restore success succeeded and here in the folder you can see there are more items like we have an object folder and here we have the main that is program.cs file and here after we open that you can see here we have some c sharp code so after you create this i recommend that you kind of restart your visual studio code ide close it and restart it and you must always check whenever you create like this file using the terminal check that this starts running so here we can see start omnishop server starting omnishop server so this will initiate all the required tools for actually developing in c-sharp and it will give us intellisense everything if this doesn't start or you have any problem with that you just close the visual studio code restart it again and still you're facing some problems and this is not running you can keep the problems in the comments down below i'll be always ready to answer your queries or you can even search it on google or stack overflow for solutions so guys now you can see it has started all the tools starting up all the tools so see you after all this thing is finished so guys now as you can see it has completed and now we even have another new folder called as bin now we must create all the required files to actually run and debug the program so guys now here we should open the command prompt of this vs code or you can just call it the command line or anything so to open it you must just use a shortcut key called as control shift p and for mac i guess it is command shift p and here you can see we got from dot net like generate assets for build and debug so just press enter on this and it will create all the required files for us to run the program and debug it and here if you see we got two files called as launch.json and tasks.json so if you go through over here you can see all the configurations for this so first we are going to launch.json and here we should just change we'll just change it to external terminal so external terminal and make sure that you follow the casing e should be small and t should be capital and other keyword all the letters are small and the task.json is you can just leave it same way and guys now we can just go to program.cs and just add a small line like console dot read line and here you can see we even got some intellisense and after that we can just go over here run and select run without debugging so here we got a new kind of bar over here and first it will start building the c-sharp file so here you can see executing task so guys right now here you can see after the build process you can see that we have got a terminal open for us and it says dot exe and it has run the program for us and it's just a simple hello world program and here if we can see i'll just minimize it you can see all the process is done it has opened the debug console and here you can see that terminal will be used reused by task and you must get something like this and if you have any error you can please comment on below and i'll be always ready to answer your doubts or queries so i'll just friends enter over here and it will close it and it will even close this bar over here and stop the terminal so i can just press enter over here as well and it will remove everything so guys that was about how we can use vscode.net core to actually run c-sharp programs in our system and guys i hope you like this video please be sure to subscribe to my channel and hit the bell icon so that you never miss any updates so see you in the next video you"}], "Unreal Engine 5 BP Tutorial | 2.3 Boolean Datatype | 2. Variables | Aditya Burgula": [{"content": "hello friends welcome back to another brand new video i'm aditya and in this video we'll be learning a new data type inside unreal engine that is boolean or shortly known as bool so without wasting the time guys let us get into the video so friends boolean data type is very simple and most widely and commonly used in unreal engine and in any other programming language so what is a boolean data type so a boolean data type is used to store two values either two values that is true or false and in programming sense you can also represent them numerically using zero and one where zero refers to false and one rep refers to true so now let us see a practical implementation of this data type inside the blueprint so here on the variables tab i'm going to press the plus button now once you press the plus button it's going to ask for a name now i would like to point you a comment kind of a practice that is done by most of the unreal engineers is to add prefix of small b before the variable name so what i mean by that let me just show you so whenever i name a boolean variable what you what i do is that i keep a small b before it and then i give the name so this is kind of helpful when you want to uniquely identify the boolean variable and it is done by most of the professionals or the unreal engine defs i prefer that you also do the same now once we add the b then we are going to give the name so practical example where we can use a boolean data type is where we want to check or acknowledge whether the player is alive or dead or we can use it when we want to see if the player is in match or is in the lobby or if the player is male or if the player is female so based on that you can give so for now i'll just give like is a life so if the player is alive or dead so we can just use this uh variable same like other variables we have used so far so i'll just drag this on to the screen and here we get the same options like get and set so here we'll just do a set so here we have the node now so what you can see is that we have this kind of box over here black box same like other variable sets so now actually we we can't give any value like type true or false like in programming languages we type true or type false rather here in unreal engine blueprints what we do is we just have a check box actually and we can press on it to give it the value so if the check mark is there that means the value is true if there is no check marks inside the box that means the value is false now to illustrate a better example what we can do is we'll just use the begin play i'll just copy the note the thing is begin play cannot be called more than once so here so here i just pasted the begin play and now we'll be using a new note that is called as branch and it is mostly dependent on this bool data type so here you can see we have a branch and it is asking for a condition so don't worry about what this condition is it's quite simple so and when we come to the conditional statements that time will look at all these branches and other condition based uh nodes in unreal engine very detail so don't worry about that so even begin play and then we have set so i'm going to set is alive to false and then i'm going to attach this pin to the branch node condition pin then we have these two execution pins so if this value is alive is true then whatever is connected to this true node it will be run so here i am going to do a print string and here i'm going to say player is alive now if the player is if the value is false that the player is dead then i'm going to print clear is dead so i guess now you're able to see the code much more clearly so first we are setting the value to true and then we are running a branch i'll just hit compile and now if the if i play the game so you can see it showing player is alive so this is running successfully because the value is this alive is set to true now if i uh remove the check mark and i compile it again and play you can see it is showing player is dead because the value is showing as false so the player is not alive and one more thing i think you might have noticed by now is that when the variable is be created with the prefix of b and when we do a set statement or when we do a get statement also what you can see is that it is not showing the bx you can see right it's not showing b is alive rather it is just showing b is alive so unreal engine is kind of intelligent enough to remove that so that was about it guys a boolean data type i hope it is very easy and simple for you guys to understand it so please don't forget to hit that like button subscribe button and bell icon so that you never miss any updates on my channel thanks for watching guys see you in the next video you"}], "Unreal Engine 5 BP Tutorial | 2.1 What are Variables ? & Integers | 2. Variables | Aditya Burgula": [{"content": "hello friends welcome back to another brand new video i am aditya and in this video we'll be talking about and understanding the concept of variables and data types then we'll be taking a look at very commonly used data type that is interior so without wasting any time guys let us get into the video so friends first let us understand the concept of variable and data type so a variable can be assumed to be like a box which is like acting as a container in which you can store anything like a data or for example now we'll take and keep some cubes so a box with cubes so that box is nothing but a variable and the cubes are nothing but the data now let us take another box and put inside that some spheres so now that is also a variable but it has a different shape or different type of data nothing but spheres so box one has cubes and box two has peers so that defines the type of data and the type of data is nothing but data type so what type of data is inside that variable so commonly in most programming languages you have data types like strings integers booleans so in unreal engine we have vectors rotators so we'll take a look at a very commonly used data type that is integer across all programming languages if it is unity or c plus this java it is very common in them so in unreal engine specifically we have three kinds of data types to store numbers so the first one being byte second one being integer and third one being integer 64 according to their sizes so now what do i mean by sizes so size refers to the amount of data that that particular data type can hold so suppose first let us take the example of byte so byte can only store values from 0 to 255 so it cannot store neither negative values nor numbers above 255 if you want to throw 256 you cannot inside buy it so this data type can be useful if you want to store rgb values like 0 to 255 right so that can be helpful in that byte data type and when it comes to integer and integer 64. so integer is kind of it has a range which is negative 2 billion to positive 2 billion i i guess so i'll just put up on the screen the limit for integer and integer 64."}, {"content": "integer 64 is pretty huge i don't even know what to call that number it is very huge so depending on the usage you can choose between all these three uh if you want very huge number two if you want to store very huge number and do calculations on those then you can use integer 64. for normal use cases you can just use integer data type so now let us take a look and see how we can create these variables assign them a particular data type and bring them to the screen and see how it works so previously we have worked on blueprint classes right we have created them and seen how we can use them but for now from now on we'll be actually using level blueprint which is actually quite easy to use so it doesn't have much complicated things and it's very simple to open a level blueprint we will go to the blueprints tab on the top as you can see over here so i'll just click on this and here we can see level blueprint so we can click open level blueprint and it will open it so once it opens you're going to be presented with the screen like this here it shows my blueprint even graph and details so what i'll do is i'll drag this details tab and i'll put it down over here so to create a variable first as you can see over here there's a variables and here we have a plus icon right so you can just click on that and you can also do add and here from here also you can add a variable so adds a new variable so you can use any of these so i'll just click on this and here you can see new it is asking for name of the variable so here we'll just uh set ammo so a bullets guns ammo so how many bullets does it have left so here by default it is on integer so we can also if we just click on this so if we just click on this current type integer if i just click here you can see all the different data types that unreal engine provides so almost the boolean byte integer float double and string these are kind of common in all the other programming languages and game engines so even vector rotator transform these are specific to 3d game engines so for now let us just keep it to integer and now if i just select this variable as you can see in the details panel it pops up everything so i'll just drag this over here and let us see this in more detail so here we can see the detail step so when i select a variable it is showing all the details of the variable so here it has the variable name and here it is showing us the integer so it is showing the data type so if i again click over here we can change it from here itself and here on the right side you can see the showing array set map so these are collections we'll see that later now we have instance editable we'll go on through all these properties through the course of time so for now uh we'll just focus on this the default value so the default value is given 30 usually it is 0 i don't know why it is showing 30 maybe because i have used the variable somewhere i don't know so let us just uh set the variable to sum like 90. so now the variable holds the data of integer of value 90. so i'll just drag it back over here again and now to use the variable what we can do is we can just select it and drag it into the graph like this so once you release your mouse you're going to get this options like get and set ammo so get is nothing but getting the value and set is nothing but setting the value so we can if we if i use the get node what i can do is i can use the value inside that variable so a basic example would be we'll just do an event begin play and then even begin play we'll just do print string as we have done previously and once here as you can see the print string it takes a string data type whereas we have an integer data type right so usually for all the common data types unreal engine automatically converts it so if i drag this over here integer and put it on string here you can see it is showing us convert integer to string so i'll press this and you can see a conversion node so this is how we use the get node and printed to the screen so let us see this in action so i'll just compile this and let us hit play so here you can see it is printing 90. now let us see how we can use the set node so to use a set node we can just drag it like this over here and select set and here we'll just drag the execution flow over here so now after printing the string we are going to set the value and we'll set the value to 30 so the previous thing i think and once we set it to 30 we'll again call print string and here you can see here it already gives us another node so here we can use this so after setting the value it is giving us the same variable value so retrieve the value of the variable can use instead of a separate get node so i can just use like this instead of dragging this and copying this and pasting it over here like this we can just use the same thing over here which already prove is provided by the set node so let us run this and now if you run this you might not notice the set node in action because it will just print both of these values that is 90 and 30 at the same time so i'll just hit play and here you can see 30 and 90 are displayed at the same time to actually see it working what we can do is we can add a delay node i think we have used this node before so i'll just call delay so delay node will actually wait for some time so whatever duration video usually it is in seconds so it will wait for that time after it waits then it will run whatever it's there on its right side so whatever it's there after this completed node execution flow it will run it so after it completes two seconds then it will go and set the value to 30 the ammo variables value then it will do the printing so let us see this in action i will compile this once again and let us hit play so here 90 and then you can see 30. so i hope you have understood the basic concept of variable data type and how we can create a variable how we can set get a variable and how we can print it to the screen obviously we have seen that before so i hope you have understood this concept and if you like this video hit that like button and also don't forget to hit that subscribe button and hit the bell icon and select all notifications to never miss any updates on my channel thank you for watching guys see you in the next video you"}], "Unreal Engine 5 BP Tutorial | 2.5 Name Datatype | 2. Variables | Aditya Burgula": [{"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this is kind of nothing what called as level and there is also an alternative name to this which is called as a map also. So if I go to the content drawer and here you can see by default I am in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we will go to the starter map through coding and blueprint. So we will start with this map but through blueprint logic we will transition to another map. So how do we do that is using this open level. So if you clearly notice this, the open level has means accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same, so same data type. So what you can do is instead of pressing this press button, compiling it and giving it a new name. Instead of you can use you can just drag off this node like this and then choose promote to variable on the top. So once you click like that it is going to ask for a name. So I just name it as transition level name. So I will just compile it once more and once I press this and I go to details tab here you can see it is by default it is showing starter map. So we will just change to something else I guess I think I have selected it previously maybe I don't know."}, {"content": "So we will go to the minimal default. And one more thing about this data type is that they are case sensitive. So starter map so whatever you see the value right now, s is capital and m is capital. If the m is small, it represents a completely different asset if you provide it like this. And there is nothing called a starter map with small m. So the in real in general fail to find such an asset and you may be thrown up with an error."}, {"content": "So it is very important for you to check if they are spelling and the case is actually perfect or is correct. This is called as case sensitive even a string data type is also case sensitive. So here we will go to minimal underscore default. So let me just check the spelling once. So here minimal default. So I guess the spelling is perfectly fine."}, {"content": "Now we must write the logic for it. So I will take the begin play node over here and I will disconnect it by holding alt. We will just drag this down a little bit over here. Now after begin play what I am going to do is I am going to call a print string. So here I just said loading a level. So once it says that I will give a delay of two seconds. So once the delay is completed, we will do another print string and say loaded level loaded and immediately after that we will just call this open load function. And we can just keep the variable over here. So we will compile this program and it play. So here it is showing loading a level and level is loaded. So this is the minimal default level. So as you can see this is a quick transition. So this is a kind of a simple usage of this name baritite. So that was it for this video guys and I hope you liked this video. So thanks for watching guys. Please don't forget to hit that like button share button subscribe button and press it bell icon and select all notifications to never miss any updates on my channel. Thanks for watching guys. See you in the next video."}, {"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this kind of nothing. called as level and there's also an alternative name to this which is called as a map also. So if I go to the content drawer and here you can see by default I'm in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we'll go to the starter map and through coding and blueprint. So we'll start with this map but through blueprint logic we'll transition to another map. So how we do that is using this open level. So if you clearly notice this the open level has uh means accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same."}, {"content": "So same data type. So what you can do is instead of compressing this press button, compiling it and giving it a new name. Instead of what you can use you can just drag off this node pin like this and then choose promote to variable on the top. So once you click like that it is going to ask for a name. So I just named it as transition level name. So I'll just compile it once more and once I press this and I go to details tab here you can see it is uh by default it is showing starter map. So we'll just change to something else I guess I think I have selected it previously maybe I don't know. So we'll go to the minimal default and one more thing about this data type is that they are case sensitive. So starter maps so whatever you see the value right now s is capital and m is capital. If the m is small it represents a completely different asset if you provide it like this and there's nothing called a starter map with small m. So the in real engine will fail to find such an asset and you may be thrown up with an error. So it is very important for you to check if they are spelling and the case is actually perfect or is correct. This is called as case sensitive even a string data type is also case sensitive. So here we'll go to minimal underscore default. So let me just check the spelling once so here minimal default."}, {"content": "So I guess the spelling is perfectly fine now we must write the logic for it. So I'll take the begin play node over here and I'll disconnect it by holding Alt. We'll just drag this down a little bit over here. Now after begin play what I'm going to do is I'm going to call a print string. So here I just said loading a level. So once it says that I'll give a delay of two seconds. So once the delay is completed, we'll do another print string and say load it, level load it. And immediately after that we'll just call this open load function. We can just keep the variable over here. So we'll compile this program and it play. So here it is showing loading a level and level is loaded. So there's the minimal default level. So as you can see, there's a quick transition. So this is a kind of a simple usage of this name baritite. So that was it for this video guys and I hope you liked this video."}, {"content": "So thanks for watching guys. Please don't forget to hit that like button, share button, subscribe button and press it bell icon and select all notifications to never miss any updates on my channel. Thanks for watching guys. See you in the next video."}, {"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this is kind of nothing what called as level and there is also an alternative name to this which is called as a map also. So if I go to the content drawer and here you can see by default I am in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we will go to the starter map through coding and blueprint. So we will start with this map but through blueprint logic we will transition to another map. So how do we do that is using this open level. So if you clearly notice this, the open level has, means accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same, so same data type. So what you can do is instead of cup pressing this press button, compiling it and giving it a new name, instead of what you can use, you can just drag off this node, pin like this and then choose promote to variable on the top. So once you click like that, it is going to ask for a name. So I just named it as transition level name. So I'll just compile it once more and once I press this and I go to detail staff here, you can see it is by default at the showing starter map. So we'll just change to something else."}, {"content": "I guess I think I have selected it previously. Maybe I don't know. So we'll go to the minimal default and one more thing about this data type is that they are case sensitive. So starter map. So whatever you see the value right now, s is capital and m is capital. If the m is small, it represents a completely different asset if you provide it like this. And there's nothing called a starter map with small m. So the in real engine will fail to find such an asset and you may be thrown up with some error. So it is very important for you to check if the spelling and the case is actually perfect or is correct. This is called as case sensitive. Even a string data type is also case sensitive. So here we'll go to minimal underscore default. So let me just check the spelling ones. So here minimal default. So I guess the spelling is perfectly fine."}, {"content": "Now we might write the logic for it. So I'll take the begin play node over here and I'll disconnect it by holding Alt. We'll just drag this down a little bit over here. Now after begin play what I'm going to do is I'm going to call a print string. So here I just said loading a level. So once it says that I'll give a delay of two seconds. So once the delay is completed, we'll do another print string and say load it, level load it. And immediately after that we'll just call this open load function. We can just keep the variable over here. So we'll compile this program and it play. So here it is showing loading a level and level is loaded. So there's the minimal default level. So as you can see, there's a quick transition. So this is a kind of a simple usage of this name baritite. So that was it for this video guys and I hope you liked this video."}, {"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this is kind of nothing what called as level and there is also an alternative name for this which is called as a map. also. So if I go to the content drawer and here you can see by default I am in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we will go to the starter map in through coding and blueprint. So we will start with this map but through blueprint logic we will transition to another map. So how we do that is using this open level. So if you clearly notice this the open level has means accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same. So same data type."}, {"content": "So what you can do is instead of pressing this press button come by and giving it a new name. Instead of what you can do is you can just drag off this node pin like this and then choose promote to variable on the top. So once you click like that it is going to ask for a name. So I just named it as transition level name. So I will just compile it once more and once I press this and I go to details tab here you can see it is by default at the  showing starter map. So we'll just change to something else I guess."}, {"content": "I think I have selected it previously maybe I don't know. So we'll go to the minimal default and one more thing about this data type is that they're case sensitive. So starter map. So whatever you see the value right now, s is capital and m is capital. If the m is small, it represents a completely different asset if you provide it like this. And there's nothing called a starter map with small m. So the in real engine will fail to find such an asset and you may be thrown up in a certain error. So it is very important for you to check if they are spelling and the case is actually perfect or is correct. This is called as case sensitive even a string data type is also case sensitive. So here we'll go to minimal underscore default. So let me just check the spelling once. So here minimal default. So I guess this spelling is perfectly fine. Now we must write the logic for it. So I'll take the begin play note over here and I'll disconnect it by holding alt. We'll just drag this down a little bit over here. Now after begin play what I'm going to do is I'm going to call a print string. here I just said loading a level. So once it says that I'll give a deal of two seconds. So once the delay is completed, we'll do another print string and say loaded level loaded. And immediately after that, we'll just call this open load function. And we can just keep the variable over here. So we'll compile this program and it play. So it is showing loading a level and level is loaded. So there's a minimal default level. So as you can see, there's a quick transition. So this is a kind of a simple usage of this name baritite. So that was it for this video guys."}, {"content": "And I hope you liked this video. So thanks for watching guys. Please don't forget to hit that like button share button subscribe button and press it bell icon and select all notifications to never miss any updates on my channel. Thanks for watching guys. See you in the next video."}, {"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this is kind of nothing what called as level and there is also an alternative name to this which is called as a map also. So if I go to the content drawer and here you can see by default I am in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we will go to the starter map through coding and blueprint. So we will start with this map but through blueprint logic we will transition to another map. So how do we do that is using this open level. So if you clearly notice this, the open level has means accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same, so same data type. So what you can do is instead of pressing this press button, compiling it and giving it a new name. Instead of using it you can just drag off this node like this and then choose promote to variable on the top. So once you click like that it is going to ask for a name."}, {"content": "So I just named it as transition level name. So I will just compile it once more and once I press this and I go to detail tab here you can see it is by default it is showing starter map. So we will just change to some..."}, {"content": "thing I'll try is I think I have selected it previously maybe I don't know so we'll go to the minimal default and one more thing about this data type is that they are case sensitive. So starter map so whatever you see the value right now s is capital and m is capital if the m is small it represents a completely different asset if you provide it like this and there's nothing called a starter map with small m so the in real engine will fail to find such an asset and you may be thrown up with an error so it is very important for you to check if the spelling and the case is actually perfect or is correct this is called as case sensitive even a string data type is also case sensitive so here we'll go to minimal underscore default so let me just check the spelling once so here minimal default so I guess the spelling is perfectly fine now we must write the logic for it so I'll take the begin play node over here and I'll disconnect it by holding alt we'll just drag this down a little bit over here now after begin play what I'm going to do is I'm going to call a print string so here I just said loading a level so once it says that I'll give a delay of two seconds so once the delay is completed we'll do another print string and say load it level loaded and immediately after that will just call us open load function and we can just keep the variable over here so we'll compile this program and it play so here it is showing loading a level and level is loaded so there's the minimal default level so as you can see there's a quick transition so this is a kind of a simple usage of this name baritite so that was it for this video guys and I hope you liked this video so thanks for watching guys please don't forget to hit that like button share button subscribe button and press it bell icon and select all notifications to never miss any updates on my channel thanks for watching guys see you in the next video"}, {"content": " Hello friends, welcome back to the brand new video. I'm Adithya and in this video guys will be learning about a new data type inside Unreal Engine that is, name. So without wasting time guys, let's get into the video. So friends, the name data type is very similar to the string data type which we have learned previously. So as a string data type can hold characters, numbers and special symbols, a text data type can also hold the same kind of data. Now we might wonder what makes the difference between these two data types then. The main difference is that the name data type is used to store or uniquely identify object names or asset names inside Unreal Engine like materials, levels and lot more. So let us see a kind of a simple example as to how we use this data type. So by default, Unreal actually provides a very commonly used node that is kind of called as open level. So by chance if you don't know what a level is, so a level is nothing much like this world which you can see over here which comprises of all the 3D objects, camera and the player itself. So this is kind of nothing what called as level and there is also an alternative name to this which is called as a map also. So if I go to the content drawer and here you can see by default I am in the starter contents and maps here you can see that kind of defaultly provided levels which is starter map and minimal default advanced lighting. So we will go to the starter map through coding and blueprint. So we will start with this map but through blueprint logic we will transition to  another map. So how we do that is using this open level. So if you clearly notice this, the open level has, I mean, accepts a data type of name. So this specifically used for these purposes and assets and all those things also use that same."}, {"content": "So same data type. So what you can do is instead of cup pressing this press button, compiling it and giving it a new name, instead of what you can do is you can just drag off this node pin like this and then choose promote to variable on the top. So once you click like that, it is going to ask for a name. So I just named it as transition level name. So I'll just compile it once more and once I press this and I go to detail staff here, you can see it is by default it is showing starter map. So we'll just change to something else."}, {"content": "I guess I think I have selected it previously, maybe I don't know. So we'll go to the minimal default and one more thing about this data type is that they're case sensitive. So starter map. So whatever you see the value right now, s is capital and m is capital. If the m is small, it represents a completely different asset if you provide it like this. And there's nothing called a starter map with small m. So the in real engine will fail to find such an asset and you may be thrown up with an error. So it is very important for you to check if they are spelling and the case is actually perfect or is correct. This is called as case sensitive even a string data type is also case sensitive. So here we'll go to minimal underscore default. So let me just check the spelling once. So here minimal default. So I guess this spelling is perfectly fine. Now we must write the logic for it. So I'll take the begin play node over here and I'll disconnect it by holding Alt. We'll just drag this down a little bit over here. Now after begin play, what I'm going to do is I'm going to call a print string. So here I just said loading a level. So once it says that I'll give a delay of two seconds. So once the delay is completed, we'll do another print string and say load it, level load it and immediately after that we'll just call this open load function. And we can just keep the variable over here. So we'll compile this program and it play. So here it is showing loading a level and level is loaded. So this is the minimal default level. So as you can see, this is a quick transition. So this is a kind of a simple usage of this name baritite. So that was it for this video guys and I hope you liked this video."}], "Unreal Engine 5 BP Tutorial | 2.2 Float Datatype | 2. Variables | Aditya Burgula": [{"content": " Hello friends welcome back to another brand new video. I'm Aditya and in this video we will be learning about float data type inside of V5. Without wasting any time guys, let us get into the video. So friends, we have covered about indeed data type. And that data type is very simple. We have seen how we can use it to store numbers. Now when it comes to float data type, it is pretty much the same. We can use it to store numbers. So then what makes the difference between integer and flow? It's very simple. Intitator type is used to store integers. And whereas float data type is used to store rational numbers. So let us have a quick overview of what integers and rational numbers are. In mathematics, we have very popular sets like national numbers, which are a set of numbers starting from 1 to infinity. Then we have whole number set, which starts from 0 to infinity. Then we have integer set and that set has all numbers, all the positive numbers, and all the negative numbers till infinity and then 0. Then we have on top of that, we have rational numbers. So in rational numbers, we have all these fractions. So that fractions are nothing but decimals. And those fractions we can store in the form of float data. On top of rational numbers, we have rational numbers. So that is all the topic right now here. Let us just discuss about rational numbers. So let us see the practical implementation of it by creating a new variable."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say it's like 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen and just get it. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the product. So we'll compile it now. And you can see we can set the value. And you can see 0.00. So notice the difference. We got the point over here. 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, it's the float. So let us just create a simple program."}, {"content": "I'll just call the event tick. And we'll do a princess. So now what is event tick? So event tick has nothing but just like event begin play. As event begin play is called when the game starts, exactly when the game starts and only once it is just called once, the event tick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time it will call whatever is connected to this node. 60 times is second. So if I attach it to the string, and it has already created the conversion node, just like for the integer, now let us compile this and let us run. So it's going to show like whole this top number. So it's going to continuously printed. So if I hit play, you can see 0.00. Just continuous printing. I don't know what my frame rate is, maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. It can also select it over here than node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much to complex in it. There's nothing but a derivative which can just store rational values or decimal values. So I hope you like this video."}, {"content": "If you did it, please hit that like button. And also hit the subscribe button and press the bell icon. So that you never miss any updates on my channel. Thanks for watching. I see you in the next video."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say that 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable help. Now I'm just going to drag it into the screen and just get. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the program. So we'll compile it now. And you can see we can set the value. And you can see 0.00. So notice the difference. We got the point over here, 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, it's the float. Let us just create a simple program."}, {"content": "I'll just call the event tick. And we'll do a printed. So now what is event tick? So event tick has nothing but just like event begin play. As event begin play is called when the game starts exactly when the game starts and only once it is just called once, the event tick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time, it will call whatever is connected to this node 60 times is second. So if I attach it to the string, and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff numbers. It's going to continuously printed. So if I hit play, you can see 0.00. It just continues printing. I don't know what my frame rate is, maybe 60, I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much to complex in it. There's nothing but a derivative. This can just store rational values or decimal values. So I hope you liked this video."}, {"content": "If you did, please hit that like button. And also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching. I'll see you in the next video."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say that 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen and just get. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the tokens. So we'll compile it now and you can see we can set the value and you can see 0.00. So notice the difference. We got the point over here 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, you need to float. Let us just create a simple program."}, {"content": "I'll just call it a event tick and we'll do a printed. So now what is event tick? So event tick has nothing but just like event begin play. As event begin play is called when the game starts exactly when the game starts and only once it is just called once. The event tick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff numbers. It's going to continuously printed. So if I hit play, you can see 0.00 is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much to complex in it. There's nothing but a derivative which can just store rational value. So I hope you liked this video."}, {"content": "If you did, please hit that like button and also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching. I see you in the next video."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say that 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen. and just fit get. So this is how you get the value and now in the details tab I think you can see once you've selected it is showing please compile the program so we'll compile it now and you can see we can set the value and you can see 0.00. So notice the difference we got the point over here 0.00. Whereas if I click on the integer variable and if we scroll to the value we can see we do not have any point we can just we can just store a number that's it we can't store the fractions or decimals. So when you want to store any fraction or decimals you need to float. So let us just create a simple program."}, {"content": "I'll just call the event tick and we'll do a princess. So now what is event tick? So event tick is nothing but just like event begin play as event begin play is called when the game starts exactly when the game starts and only once it is just called once the event tick is called every frame it is called every frame. So if your game is running at 60 FPS that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer now let us compile this and let us run. So it's going to show like whole this stuff numbers it's going to continuously printed. So if I hit play you can see 0 it is just continuous printing I don't know what my frame rate is maybe 60 I guess so 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here can also select it over here the node also and let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate I hope you have understood it. It's quite simple there's nothing much too complex in it. There's nothing but a derivative which can just store rational values or decimal values. So I hope you like this video you protect please hit that like button and also hit the subscribe button and press the bell icon so that you never miss any updates on my channel."}, {"content": "Thanks for watching and see you in the next video."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say it's like 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen and just get it. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the product. So we'll compile it now. And you can see we can set the value. And you can see 0.00. So notice the difference. We got the point over here. 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, you need to float. Let us just create a simple program. I'll just call it EventTick and we'll do a printed. So now what is EventTick? So EventTick is nothing but just like EventPigantPlay. As EventPigantPlay is called when the game starts, exactly when the game starts and only once it is just called once, the EventTick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node, just like for the integer. Now let us compile this and let us run."}, {"content": "So it's going to show like whole this stuff numbers. It's going to continuously printed. So if I hit play, you can see zero. It is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much to complex in it. There's nothing but a derivative which can just store rational values or decimal values. So I hope you liked this video."}, {"content": "If you did it, please hit that like button and also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching. I see you in the next video. Bye."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say it's like 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen and just get it. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the product. So we'll compile it now. And you can see we can set the value. And you can see 0.00. So notice the difference we got. the point over here 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, you need to float. Let us just create a simple program."}, {"content": "I'll just call it a event tick and we'll do a princess. So now what is event tick? So event tick is nothing but just like event begin play. As event begin play is called when the game starts, exactly when the game starts and only once it is just called once, the event tick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff numbers. It's going to continuously printed. So if I hit play, you can see 0. It is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also and let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much too complex in it. There's nothing but a derivative which can just store rational values or decimal values. So I hope you like this video."}, {"content": "If you did it, please hit that like button and also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching and see you in the next video."}, {"content": " Hello friends welcome back to another brand new video. I'm Aditya and in this video we will be learning about float data type inside of V5. Without wasting any time guys, let us get into the video. So friends, we have covered about indeed data type. And that data type is very simple. We have seen how we can use it to store numbers. Now when it comes to float data type, it is pretty much the same. We can use it to store numbers. So then what makes the difference between integer and flow? It's very simple. Intitator type is used to store integers. And whereas float data type is used to store rational numbers. So let us have a quick overview of what integers and rational numbers are. In mathematics, we have very popular sets like national numbers, which are a set of numbers starting from 1 to infinity. Then we have whole number set, which starts from 0 to infinity. Then we have integer set and that set has all numbers, all the positive numbers, and all the negative numbers till infinity and then 0. Then we have on top of that we have rational numbers. So in rational numbers,  numbers we have all these fractions right so that fractions are nothing but decimals and those fractions we can store in the form of flow data. So and on top of fraction numbers we have a ration numbers so that is of the copy right now here let us just discuss about fraction numbers. So let us see the practical implementation of it by creating a new variable."}, {"content": "So I'll just click on this and as you can see it is asking for a variable name and I'll just give it a help. So usually health is stored as a flow data data because it can have fractions in all those things and we give percent to say like 100 percent has 37 percent health and that time you need fractions. So health and I created the variable health now I'm just going to drag it into the screen and just get. So this is how you get the value and now in the details tab I think you can see once you've selected it is showing please compile the product so we'll compile it now and you can see we can set the value and you can see 0.00. So notice the difference we got the point over here 0.00. zero. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimal. So when you want to store any fraction or decimal, you need to float. Let us just create a simple program. I'll just call it a event tick and we'll do a princess. So now what is event tick? So event tick is nothing but just like event tick and begin play. As event begin play is called when the game starts exactly when the game starts and only once it is just called once, the event tick is called every frame. It is called every frame. So if your game is running at 60 FPS, that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff numbers. It's  You can see 0 it is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much to complex in it. There's nothing but a dilatant which can just store rational values or decimal values. So I hope you liked this video."}, {"content": "If you did, please hit that like button. And also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching. I see you in the next video."}, {"content": "So I'll just click on this. And as you can see, it is asking for a variable name. And I'll just give it a help. So usually health is stored as a float data data because it can have fractions in all the states. And we give percent to say it's like 100 percent has 37 percent health. And that time you need fractions. So health. And I created the variable health. Now I'm just going to drag it into the screen and just get it. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the product. So we'll compile it now. And you can see we can set the value. And you can see 0.00. So notice the difference. We got the point over here. 0.00. Where are I see? click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimals, you need to float. Let us just create a simple program."}, {"content": "I'll just call it a event tick and we'll do a princess. So now what is event tick? So event tick is nothing but just like event begin play. As event begin play is called when the game starts, exactly when the game starts and only once it is just called once, the event tick is called every frame. It is called every frame. So if your game is running at 60 of years, that time it will call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff. Numbers it's going to continuously print it. So if I hit play, you can see zero. It is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also and let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it."}, {"content": "It's quite simple. There's nothing much too complex in it. There's nothing but a derivative. This can just store rational values or decimal values. So I hope you liked this video."}, {"content": "So I'll just click on this."}, {"content": "And as you can see, it is asking for a variable name and I'll just give it a hell. So usually health is stored as a flow data because it can have fractions in all those things. And we give percent is like 100 percent has 37 percent health and that time you need fractions. So health and I created the variable health. Now I'm just going to drag it into the screen and just get. So this is how you get the value. And now in the details tab, I think you can see once you've selected, it is showing please compile the tokens. So we'll compile it now and you can see we can set the value and you can see 0.00. So notice the difference. We got the point over here 0.00. Whereas if I click on the integer variable and if we scroll to the value, we can see we do not have any point. We can just we can just store a number. That's it. We can't store the fractions or decimals. So when you want to store any fraction or decimal units to float, let us just create a simple program. I'll just call the event tick."}, {"content": "And we'll do a printed. So now what is event tick? So event tick has nothing but just like event begin play. As event begin play is called when the game starts exactly when the game starts and only once it is just called once. The event tick is called every frame. It is called every frame. So if your game is running at 60 FPS that time,  call whatever is connected to this node 60 times a second. So if I attach it to the string and it has already created the conversion node just like for the integer. Now let us compile this and let us run. So it's going to show like whole this stuff numbers. It's going to continuously printed. So if I hit play you can see zero it is just continuous printing. I don't know what my frame rate is maybe 60 I guess. So 60 or 70 maybe. So now let us just give a value to it. So I'll just select the health over here. You can also select it over here the node also. And let us scroll and give it some value and compile and play. Here you can see it is just printing it. So that was about float rate. I hope you have understood it. It's quite simple. There's nothing much too complex in it. There's nothing but a derivative which can just store rational values or decimal values. So I hope you like this video."}, {"content": "If you did it please hit that like button and also hit the subscribe button and press the bell icon so that you never miss any updates on my channel. Thanks for watching and see you in the next video."}], "Dart Programming Tutorial | #3 Type Conversions In Dart | Aditya Burgula": [{"content": "hey guys welcome back to my channel I'm out there and in this video we'll be covering about or understanding type conditions in dark so without wasting time let's get into the video I just want to talk about the previous video where we have covered about the data types and variables so when we were talking about that ifs we actually covered about one more data type which is called as dynamic so this data type allows us to program dynamically that means that we need not mention the type of data we where we are handling with but that actually isn't so great dynamic programming languages are little bit more prone to errors so they are more prone to errors in large applications especially when you write a large code it can become little bit difficult to find out where the error comes through whenever you write an application you are going to get an error it is sure you are going to get errors in the beginning so you must again debug it if you don't know what debugging is developing is like just checking what all are the errors while running your program step by step so now let's go ahead and learn about type conversion this this type conversion is for statically typed programming languages if you're from Java C++ there are statically typed programming languages so in dart it is pretty much very easy how we can know types so let's go let's see how we do that so here actually have written kind of a pre code or have written some code before itself so that it becomes easy for me to explain and reference so here I have a value called as in in - value equals 10 the variable name itself has value and here I have assigned an integer variable so first we are going to work with into data type so we have got a converted to string and the next line let us go to the next line and see what's gonna happen so I'm gonna check the type of data so to check the type of data actually we have a kind of it is called as if you can call it a video it is actually from object programming or classes you can think but we are not yet on that Advanced section right now so you can just think it as a property of a variable non time type so this run time type will just give us the value or the datatype or off it of the variable so if this if I pass value so here value dot run time type so it will give the time so get run time type so it will give that data type of that variable so it will give the data type of value I hope you're getting it so that time when we actually assign assign and declare it at the same time it will be of in that a type and now I have created another data type I didn't mention any data of what particular data type it's gonna be so that in whenever I try to go to the next data type conversions I don't need to change it again to double or string that's why I have written wire so that it stays like that itself know here what happened here I have kids kept the same value value and here I will given a function so we have not yet covered about functions right now so don't worry a function is simple like it does a task it does it performs a task so what task is gonna do it is gonna convert the data type to a string data type so here values of into data type and it's gonna convert it to string that type so let's run this program and check it out and by the way in the next line on the fourth line we can see that after conversion it's gonna print that add a type of this one so after converting converted Val and its runtime type over here so let's run this program to run we'll just use the terminal new terminal and here I think you can know the command main dot dot da RT and press Enter and as you can see over here I'll just scroll down here so as you can see your before conversion that at a type is int so value dot runtime type is int over here and after conversion or after using this method the value the converted value type is strength so this will be convert into string now let us check out what is the next thing which is in trouble so here what is a method to double so I think you can guess right now so we'll just write value dot to double as you can see in the intelligence we call to double and now let's Akane open the terminal and let's run this program and I think you can see over here before the value was of int and now the data type of converted value is double so that was pretty easy conversion of int to string and in true double now let us explore about double so double and here I will just keep a point so that it becomes easy for us to understand the data type so double value is equal to ten point zero so it's a decimal value or let us keep it ten point one now what is the first thing we must wanna see we are going to see is double to string so same we can just write to just replace it with two string and let's see what's gonna happen so open the terminal I just clear these all things before each other and I'll scroll down here okay and main dot dart and as you can see the conversion where the conversion has been successful and it has converted to strain now let's check what is the output actually of it so I'll just remove this runtime type so that we can even see what is the string value so string value is also same and the double value is also same so I will just remove this and and now you can see that the values are pretty much same you cannot just tell by looking at this that's why I use the runtime dot type so that you can check it so I'll just so now we have covered about converting double to string so guys the next one is a double to end and the function is to end so it is very simple just converting this to end so replace this with two and int and if we run the program again and we are going to get the output as the and so the value is double and after conversion it is int okay that's great so right now we have covered about all the int conversions and double conversions and now let's move ahead and learn about converting string to different data types so first of all I must change this data type to a string string and here I'll just keep these two in okay I'll just add another value so I'll write my name so always I mind him as an example okay now to convert it we must use a different way even he were here I have written that parse method right path function so how are we going to do that so first we must use the data type which we want to convert it to so first we are going to convert it to end so we are gonna write int and then we are gonna write dot and parse as you can see over here we got a function called a sparse and press Enter and here you want to get our parameter which you wanna give that is called a source or which what you want to convert it to end so I want to convert this value the string value into int so I will just write value now if I convert new terminal or yeah I have already open a terminal and now let's clear this I'll just clear this once so and now let's run this program and we catch some error so the error is that we have actually used characters which cannot be converted into integer datatype so all they cannot be converted into integer datatype right so rather we'll be using some integer inside that inside a string also that time when we run the program we should not get any error it is impossible to convert characters to end so when I press like that it is successful because the value inside the string is also a number actually it identified it and it has successfully converted to int now what's the another one it is called as double from I mean from string to double or another another conversion so here just replace this with double do you ple so double then that's it and let's run this again so main dart and as you can see it has successfully converted from string to double as well okay now we have covered about conversions of int double string and this last conversion called a spool which we have learnt it only has two values which we can store in them that is true or false so for that I'll just change it to bull so this is a boolean value let us change the value also to true so nice here for bool we only are converting it from mu to string because usually we don't convert it to bull to interval to double so we don't even use that conversions but you can use external kind of libraries to convert it from in to to bool or pool to end but this dot code library doesn't have it so you can use some kind of libraries in the dart library or pop the tab from there I have seen some libraries where you can convert from pool to end and into bool vice versa but in this video we are going to not cover that so to convert it from bool to strength we are gonna use the same thing which we did so for int and double how we used to string and that one so we are going to write value dot to string and now let's just press keep a semicolon and let's clear this first and now let's run this again and as you can see it as successfully converted from bool to end so that is that was about to type conversion guys I hope you liked this video please be sure to subscribe to my channel I'd hit that Bell icon so that you never miss any updates thanks for watching"}], "Dart Programming Tutorial | #4 Basic Operators In Dart | Aditya Burgula": [{"content": "hey guys welcome back to my channel i'm aditya and in this video we'll be learning or understanding about operators in dart which is very easy and one thing is that if you are coming from c plus list or python or java like that then these operators are very similar to them you can just check out the increment and decrement operator if you're coming from a python because there we don't have plus plus and minus minus you can check that how it works and after that you can go to the next video if you're completely beginner to any programming language or you're completely starting programming right now you can just continue watching this video so without wasting your time let's get into the video so guys here as you can see i have just a main method and i have written some operators in dart so here we have plus minus star and this hash forward slash and then this percentage symbol and then we have plus plus minus minus and we even have operators like plus equals and then these are similar to python you can i think you can guess it in python also we use like these operators for incrementing and decrementing okay first let us start with the first operator plus minus so we'll not uh do this plus minus and star star is for multiplication this is a very similar in mathematics like i think you can even understand them so very simple operators we will start from division because there are actually two types of division operators the forward slash and forward slash and percentage so let us check out the difference between these two division operators so i'll just write a simple print statement to check them so let us divide 10 by two simple division so 10 divided by 2 so we are using the first division operator so this one then let's run this open a new terminal so terminal and then click new terminal and to run a program always uh write this command dot and then you write the file name main dot dot so press enter and we got 5.0 so always remember that whenever you do double uh means whenever you do a division you are going to get the result in double value so uh if you are seeing if you are not seeing the directive double is a data type which can be used to represent decimal values or fraction values so when we divide sometimes we we are going to get a decimal value or sometimes you might get a whole number but still it shows us in a decimal value so that it can be more precise okay the second operator is percentage symbol so what does this thing do so let me just replace that with percentage so here percentage and now let's run this again and we see some difference the percentage symbol gives us the answer as zero so why is that so when we were using only the forward slash that time it was giving us the output of uh means the output as quotient so it is giving 2 divided by 10 that is 5 5 is the quotient and when we were doing this percentage symbol that time we are getting the remainder of the uh calculation so when you divide what you get the remainder that time that is going to be printed so suppose i keep it to three then we are going to get some big we are going to get some remainder so see as you can see over here we got remainder if we search for the question it will go on give me till infinity maps no it doesn't so as you can see it is continuously dividing three three three three three so that was about these two division operators so i think you know about these operators uh the basic operators like plus minus star and then forward slash and percentage symbol now let's move ahead and learn about these operators so these are increment and decrement operators so first these two not these two so this is increment operator and decrement operator so for understanding this i'll just create a variable so i'll write int a or i'll write in num1 equals 1 simple and then here next we'll write num1 and what was the first operator plus plus so we'll write plus plus and close the line and again we'll print over here print num1 and before that also before doing the operator increment operator we'll also print again the num so print num1 and close it so i'll just clear this and let's go here so oh so we did we made uh okay so now press go to the terminal and as you can see when we are printing for the first time without performing the increment operation we are gon we are getting the same value as num1 what we assign to it after incrementing we are gonna we are getting two so that is incrementing so it always increments by one so this is useful when we are doing when we are doing uh loops which will be covering in the future also so don't worry about that we'll be doing that in loops especially in for loops and while loops there we use these increment operators and same you can even do minus minus so let's run this again and we are going to get 1 and 0 as you can see minus minus okay that's great now let us check this operator or plus equals minus equals and we even have star equals also like uh star equals and then division equals you can do pretty much everything like all the four base five basic operators which are over here plus equals minus equal star equals forward slash equals and percentage equals so these all are represented like if i write num plus equals now 10 so i'll write 10 and close it this simply means that num1 equals num 1 plus 10 so i think you're getting it uh so here this simply represents as num1 is equal to num1 plus 10. in python we actually don't have uh these plus plus and minus minus operators if you are coming from python so that time we use these so we use these operators for incrementing we will just write so when we replace it with one it will be like num1 is equal to num1 plus one so a lot of ones so that was about these operators you can even replace it with one uh minus so i'll just replace plus with minus and here we it will just be replaced with minus so here num one num num one equals num one minus one that's actually a lot of nums okay so we have covered up some of the basic operators i think you are very similar with them right now so that's not much complicated it's very easy nothing much there in this just like simple operators mathematical operators but even in conditional statements we'll be exploring some more operators so please be sure to subscribe to my channel and hit that bell icon so that you never miss any updates thanks for watching and see you in the next video"}], "Dart Programming Tutorial | #7 Functions in Dart | Aditya Burgula": [{"content": "[Music] hey guys welcome to another brand new video so in this video guys we'll be learning about functions in dart programming language so function is common in any programming language sometimes they're even used interchangeably that is they can be called as methods and sometimes they call as functions so it's always the same thing so the definition is always the same so before going ahead to coding let us understand what exactly is a function so a function is nothing but a group of tasks or a group of behavior so it is kind of a behavior which we can execute or which we can run so two main things and functions are that first you must declare a function and then you must call a function so i'll just make a function like talk so which will just print hi hello that is nothing but function declaration then i'll call it suppose uh somewhere in the program i just want to call like i want to say hi to this person so i'll write talk and i'll use a parenthesis we'll learn about the syntax don't worry about that so this very simple concept the syntax everything is very similar to c plus plus c sharp everything is same so let's get on with coding so i'll just zoom in a bit so that it becomes much clearer so here we have a main file main program main method so this is also actually a function so main function and here let us try have some variable at first so i'll just write where greeting or where text let us have a simple name where text is equal to hello how are you so guys this is a simple variable we have created now let us have a function so i'll write void greet now don't worry about all those void things we'll cover it don't worry it is very easy no need to worry about those things so here we have a function declaration kind of a template now in this great function we are going to say that it should print this text so we must take in the text it is not actually available in great so it might be a little bit confusing to understand right now so don't worry so it is like string txt so here we are getting some error so s t r i n g string so what is the error okay so we should make it capital so here we have a simple variable called a string text and here we are just gonna write print statement so print txt so guys whatever variable we are giving over here is nothing but a a parameter or an argument so there is actually a slight difference between parameter and what an argument is so for now with the time of definition we'll think it as a parameter so this txt is nothing but a parameter which is called which the function is gonna take while calling this function so let's check out as an example how this works so i'll just write great so now we are calling the function so we it it just means that we are using the function at this time right now so i'm using this function and i'm passing a text variable which is a string and always end it with a semicolon now let's run this program go to terminal main dot okay so actually i have made a small error typo so let me just so here dot main dot dot and now let's run this program and as you can see it has printed hello how are you so this is actually a function void greet and what this function is doing is that here we are declaring as it will take a value which is called which is of type string and it will print that string whatever string is taken it will print that string and here we are calling it so this is definition so i'll write in comments so comments are just like a reference text so here definition and calling the calling so here we are defining here we are calling you can even call this as declaration also definition now we can even have a somewhat more simple kind of a function so i'll just replace with add and here instead of string let us take integers so let us take two integers a comma and b and now the text variable as you can see here uh we got an error so here it is saying that undefined name text so here whatever is there in the parameters we can use without defining it or declaring it variable so here we actually do not have any variable inside the function declared you can declare a function over here also inside the functionals you can declare variables like in name or anything like that and you can even use variables which you can get while from calling so here there's no variable actually declared so if i declare like something like string txt and enclose it you can see we got the error so we didn't get any error right now so that's how you can declare it inside and you can even take it from while calling now let's remove this variable txt and what we are going to do is that we are just going to print the sum of a plus b now we have a simple function called as add which will add two numbers so here in calling i'll just write add 20 plus 10 so here it has done this but actually we got one error because it needs two arguments so here it is actually not 20 plus 10 we must separate it with a comma and here inside this it will automatically do no need to worry about this whatever task is given inside this block function block will be executed you don't need to do anything over here so here 20 and 10 are given and the answer we should get is 30 so i'll run the program and as you can see over here we got 30. so it was successful like that you can add conditional statements it is just like a uh it is just like an own your blog a function blog where you can define your own behavior you here we are defining how an ad behavior will work in programming you can make custom like greeting all other things like checking logging in a username you will take two variables like username and password and check the both values using conditional statements so that's actually a lot of fun functions are also a lot fun in the intermediate or advanced section we'll be covering about all the different kinds of topics and functions so it's a very interesting topic i hope you like this video please be sure to subscribe to my channel and hit that bell icon so that you never miss any updates thanks for watching see you in the next video"}], "Dart Programming Tutorial | #2 DataTypes and Variables in Dart | Aditya Burgula": [{"content": "hey guys welcome to my channel I'm Athiya and in this video we'll be starting to program in dart language and we'll be covering the basic stuff like data types and variables in this video so without wasting time let's get into the video guys right now I have opened Visual Studio code and I'm on a file called main door tart so as you can see over here I have the dot folder mailed or tart and then here we have a main function so main function is where your code starts to execute from so if you write it outside won't execute if you write it inside only inside of these curly braces you can see from there only it will start executing so first we'll cover about data types and variables so first we'll learn about data types so in dart language we have int so I just write heading so that it would be easy for you data types so here I have written data types and no the first data type is in credit I and the second we are going to learn is double and the third string and fourth one is pool or pool and the last one is dynamic so here int so n does nothing but it is used to store an integer value so that integer value can either be positive value or a negative value so now we have here end and here we have double the next one so the double is nothing but it is used to store decimal values for fractional values so you can store point zero zero one or point one zero one zero zero zero one so like that you can store all the fractional decimal or floating point values so if you are from C place place we use the float so the float keyword and here the third one is nothing but string so string is just like using kind of it is used to store character sequence of characters just like your name and name address so one thing is that they cannot perform calculations whereas int and double can perform calculations and string cannot perform calculations so that is one of the thing to remember and the next one is boon so bool is actually used to store two kind of values only two kind of values true or false so it can store true or false either of them it depends on the condition so this bull data type is going to be extensively used when we come to conditional statements so the last one is dynamic so what is this dynamic type I think you can kind of kind of predict what it's going to be so dynamic is a data type which can store any kind of value it can store into that a type double data type string data type bool data type and anything so lists and maps which are kind of data structures but will not be covering them right now so a dynamic can't store any kind of data so what is the use of having all these data types when you just have a data type or a dynamic data type which can store any kind of value so actually there's a concept of static and dynamic typing so when we use only these kind of when we want to use an integer we only use the entry word to represent an integer that time it is called a static and whereas dynamic the value is being able to change so that time it is called as dynamic programming so where the Tara type can be changed so I see it we have int double string pool and dynamic we have covered them it might be a little bit confusing for you to understand right now so let's try to code it and understand and now let us even while coding we live and understand about the variables as well so to make an integer datatype I just use a keyword int and here I'll just write my age equals some so I'll just declare a variable called my age so what is a variable over here so variable is my age so what exactly is the definition of a variable so variable is nothing but a box or a storage container in memory and that storage container box is used or referenced using this name so here we have given the name as my age so it will be referenced as my age so the container which name is my age okay next the my age container doesn't have any content or value inside it here we can give a value so one thing to remember is that we cannot give the floating-point numbers like this because we have already declared that it's going to only have an integer datatype so if we keep like this we are gonna have more errors so that's great we have learned about declaration declaration and here we have learned what initialization initialization okay now let's move ahead and we'll just print this out so I'll just write print and I can just pass the variable inside it so and end it with a semicolon I think you are noticing that I'm always ending every statement with a semicolon okay now let's run this program to run this program I'll be using the terminal don't use his Run button so here I'll just open a new terminal and here you will even get a shortcut key which which will be helpful for you so to open it quickly so here to run any dart program first you write the dart name so dot and then I write the file name so it does mein daughter so main dot dot press enter and now as you can see we have got six seven eight four three so case that was about integer datatype no I can even change it to double also qub le so double data type and still as you can see we don't get any error now let's run this program again and check out so here you can see double well if we add a double over here you can see that it has already added a decimal value like point zero so you can even give integer values but it will add more precision so in future rows if you want to add more precision you didn't just use double so let me just clear this and I'll just even close this thing okay so this was about integers and doubles now let us go ahead and learn about strings the strings are teletypes which you can store to use a sequence of characters so what do I mean by streak of sequence of characters so I can illustrate string my name equals so I'll directly declare and initialize at the same time like this so other the\u00e1-- so I'll write like this and we are here declaring it and here we are also initializing it in the same line now as you can see I have enclosed some using double quotes you can even use single quotes also so there's no Rodman you can use double or single quotes in dark okay that was about how we declare and initialize now let us come to the point of what makes it different from integers and double so you cannot actually perform calculations like subtraction division but you can even perform addition which is called as concatenation but will be not covering it right now so I just can't multiply with other like that other Thea into or some other name are the chain to bucola like that it doesn't work if I do all the place bucola it will do some kind of output which I'll be showing you in the next video so see the next video also so what happens concatenation or if you are very equally you can check out on the internet what concatenation is so here we can write any kind of numbers are also possible and you can even store special symbols also so that's nothing much in this string datatype it is used to store just text so that was about strength now let's come to bool defe type so bool and I'll just write is how a school is a student so I'll just write a student the still de NT I'm a student so I just write true so a bool data type is going to have only two back we search say the true or either false so I think you must have understood and I can even just replace it with false so the standard I was actually used in conditional statements extensively usual condition statements okay guys so here we have covered about these first four I think so int double and string that's great now let's learn about the last one which is dynamic and here also we can see dynamic no if I write dynamic equals so I should write dynamic my name equals are the Theia and end it and let's again print this friend and alike my name and now let's open the terminal now I'll just open a new terminal so I'll just write over here dart main dot dart so and now I press run and it's going to give me the output as odds tail now let us change the data type so I think I have told you that dynamic data type can show any kind of hair type so I will just type number and now we don't get any errors so if I was to keep it as a string we will get some error so if I keep like this as you can see over here it has given length the value of end cannot be assigned to a variable of type string so what does this dynamic to actually so dynamic so dynamic is nothing but it is used to store any kind of data and you can ever change it in the future without any warning suppose there's even another keyword called as bad you can use where so this is actually somewhat a type safe know what doesn't actually specify what kind of data type you're using but once you declare it as a number over here we are using a number so it is an integer data type so I can illustrate if I in the next time if I write my name equals the string data type so I always write are the Thira and end it with a semicolon still you are getting that string can be assigned to a variable of type int so once you initialize it for the first time that time it's gonna set its data type of my name as integer and if you again reinitialize with another data type rather than the data type which you are used for by initializing for the first time then you are going than ever thrown up like the string can't be used as an integer is already declared in that at the time of initialization so that was about this guy's dynamic and Static typing or data types and variables so I think it might be little bit hard to understand for you guys right now so just watch this video again if you are not getting it and in the next video we'll be looking some more over the stereotypes and variables so thanks for watching please be sure to subscribe to my channel and I said well I can so that you never miss any updates"}], "Dart Programming Tutorial | #8 Lists in Dart | Aditya Burgula": [{"content": "hey guys so welcome back to another brand new video so guys in the previous video we have covered about functions or methods the basic stuff so we have declared some basic functions like add and create and then we have called those functions given some parameters and called using arguments so you might be a little bit confused right now so let us go from those functions part and let's come to kind of data structures so these are kind of some interesting stuff we'll not be going too deep into these data structures and not so just like light touch we'll be just having uh so we'll be just learning about what these data structures are so this video will be covering about list so let's list is nothing but an array you can think of it as so an array another programming language is an arrays group of elements so list is all the same and mostly in the next video also we'll be covering about another data structure called as map in dot so as of this video only list so without wasting time let's get into the video so guys as you can see in the we have just a small main function over here and here first let's declare a list so it is same way how you declare variables using var the same way you can declare lists so i'll just use the keyword list and then i'll declare a variable so i'll just write my list or let's give some meaningful name like friends names so friends names and here i'll keep square brackets so this tells the dot compiler that it's a list so here we don't get any error and here we just get a blue marks so don't worry about that and here we can give multiple items inside that so here i can just give like i'll just give my name and then here i'll give kumar and then i'll give one more name maybe so i just kept it outside so lake lux and here are the so here i have simple list where i have my friends names only five six members i think one two three four five so before going ahead and printing all these things i just want to tell you one thing is that now we are seeing this list the first element of the list is actually represented by zeros so every element in the list is represented using numbers which makes it much more easier like getting a item from a list and adding an item from a list and doing some operations on a particular item item on a list so numbers are actually used to reference these so numbers always uh the counting always starts from zero in programming if you take even in any programming language it will always start from zero so here are there this is represented with zero then one then two then three then 4 so this is how in referencing works so now a small example so without first let's print the whole list and check out does it work or not so write a print function and i forgot to tell you print is also a function in the last video print is also a function which is pre-declared so it is already declared and we can directly use it so there is no declaration but it is there somewhere it is there in the kind of core library the print function definition but we are not able to see it right now but we are able to use it right now we are able to call print function okay now let's not talk about print function so let's see about this list so here i'll just write friends so here i got intellisense and now go to terminal and run the script and as you can see it has printed the whole list same as it is it has just removed those two codes and now we can even see all the variables now let's try another way of printing a particular item from list so here i can just write 0 as i told you we can use the numbers to reference a variable an element inside the list so here i can just run the program again and as you can see it has printed aditya so as i have told you the first element is represented as number zero and then so on one two three four so like that if i keep some four and then again let's come back to the terminal and let's run it so as you can see it has printed the last element it has not printed luxury as you can see one if you count from normal way of counting we'll see one two three four but you must count from 0 so 0 1 2 3 4 so that's how it works and you can even perform kind of methods or you have some functions which you can perform on them so here we have add function and then we have remove and dot length now let's see first what this dot length will give us so actually this function will give us the number of items inside the list so now by press as you can see we got five so one two three four five so this actually doesn't start from zero so only the reference thing or the thing uh all that stuff starts from zero whereas the normal counting like dot length gives us from normal one to five one to six how many other elements are there okay now let's check some more functions like we have what else functions we have on this so we have add so first function is add now here we can add a variable so i'll just write like this keep up i'll just add a string variable so i'll just write some other name like uh david so david so here we are getting some error so what is that error let us see okay so we are printing it actually so we must remove it out from the print statement so then we won't get the error so i'll just write ctrl x and here we'll just paste this and as you can see we got the error off just end up with the semicolon and now let's print it so friends names go to terminal run the program and as you can see we got aditya kumar ram laksh aathi and at the last it has actually added david so that's how it works we can for checking it whether it added or is it previously there so we'll see this we'll add it before adding it now here i have added it it's a lot of add so the function name is add and i'm using the word add so here you can see i think the difference i'll just expand this a little bit so here first before adding it before adding the element it was still ati and after using the add function and adding a variable we are getting another item which is david great now let's check some more functions so here first function we checked was add and now let's see what else functions we have so keep the dot operator and you'll get all the functions so add all so add all will give us uh means you can add multiple items in that you can use a for loop and assign it to a variable or you can create another list and add it so i'll just write add all and you can add list of all the items so i can write rde and enclose with brackets uh quotes and here i'll write adi so capital rd let's write jack okay so now let's run this program and let's check if this works so as you can see it has worked pretty much the same so in the add only add function we were only able to add a single element whereas when in add all you can add multiple elements so let me just close this let's check some more functions like what else are there so it's hard to memorize that's why we have this intellisense which always helps us so here we have dot any dot s dot map dot clear so clear will clear all the elements so let's check this thing so run this program and as you can see before clearing the where all the elements were there instead and after running the clear function all elements push so all elements are gone so those were some of the functions you can try them out if you find any difficulty in understanding what a particular function is you can just see over here when you go to a function here you'll get a documentation if you are still not able to find it you can just comment down below i'll be always ready to answer your questions so that's it for this video guys i hope you like this video and in the next video as i have told you we are not ending with this beginners right now so we even have a maps and maybe more also so please be sure to subscribe to my channel so that you never miss these interesting programming tutorials on dart so see you in the next video"}], "Dart Programming Tutorial | #13 Introduction to Object-Oriented Programming (OOPS) | Aditya Burgula": [{"content": "[Music] hey guys welcome to another brand new video i am aditya and in this video guys we'll be starting something interesting i hope you might have guessed or if you have not guessed guys we are starting object-oriented programming dot language so guys hats off to you first of all that you have completed almost the basics of this programming language that is start and actually it is a very tough job in the as as a beginner for you guys to actually learn all these basic stuff and i have taken almost 11 to 12 videos i think for completing all these things and now we'll be stepping ahead and learning about advanced topics like this oops concept and guys one more thing is that we have not actually totally covered about basic concepts in our dart language there's still more left but we just can't keep staying here and learning all the basics and almost 20 videos like that it may even take more mass to them so we'll be just stepping ahead and learning this and go on like that it is up to you that how you master all these concepts so guys without wasting your time right now let us go and learn what oops concept is so let's dive in so guys here uh you can see that we have a file open so guys what exactly is oops concept or object oriented programming so most of the people who are new to programming world hearing this word called as oops they get like tensed and worried about what exactly is this oops oh my god object 200 having what is it about so no need to worry guys it's a very simple concept so let me just take as an example in game development uh you first create a model or whenever we want to create a key and design actually uh i'm talking about game game development because i'm actually a little bit more interested in that so what happens in game design is actually that mostly we create first what exactly is the game is gonna be like a blueprint of how it's gonna work and then according to the blueprint we'll be implementing things in the game and we'll be creating like that depending on the blueprint so same goes uh with the object-oriented programming as well so object oriented programming so we see everything as objects now as i have told you the example of a game development scenario where we create a blueprint and then implement it in the game development process same way in our lives also uh there's actually like we can take our life as an example as a blueprint so suppose i just say i am a human right so i am a male human so you can take you can write it down on paper just like uh i have brown eyes i have two ears so whatever things description you right about that that's nothing but a blueprint about how i am looking and you can even write functions about what i can do i can talk i can sleep i can walk so whatever you write all the functions and all the properties of myself just like the game development group blueprint same like that if you write in a paper that's nothing but a class so that's how object-oriented programming works and taking that class or the blueprint you create an object so you can say that i am an object of that paper of the information in the paper so i hope you're getting it might be a little bit difficult to get this concept in the beginning don't worry guys you watch this video again you'll surely get it and now guys in the programming style it's just called as class and it has all the natural things which we have just like inheritance so inheritance is just like there are kids right then there are brothers they inherit all the features of their parents same like that one object can inherited inherit features from another object so that's similar and there are multiple forms so just like multiple forms is that when we are with friends we behave little bit differently when we are with parents we behave differently so there's multiple kinds of mindset with us right when we think when we talk with people same like that objects also behave differently depending on the situation that is nothing but polymorphism and then we have abstraction so abstraction is simple it is just like uh like private things and public things some things you tell it to the public some things you keep it to yourself and then we even have protected some things you don't want to tell it to the whole public but you want to tell it to some people just that is just like abstraction and then we have encapsulation so encapsulation is nothing but just like using these variables and functions so variable you can just take it as i have two eyes so you can just keep ice number equal to 2 that's nothing but a variable simply in programming terms so now i can use these eyes to see so c is a function and these two are nothing but variables so function and variables are being used together by each other so that's nothing but encapsulating so i hope you got these four main concepts and oops that people most get confused a lot of times so now let us see a basic implementation of a class or a blueprint how we use it in that programming language so guys first we'll write class and here i think you might even see in the intellisense so here we got a snippet so here i'll just write like this and let us declare something like a human so just the same example which have taken human and guys here we can actually create variables so and ice number of eyes i can just keep number and now i'll just end it with a semicolon so this variable inside that and here we can even create multiple variables like hands so i won't write my hands and i want like number of hands like that so i'll just write hands and then legs so usually humans have like that you can even create your own blueprint like an area like five hands five legs so that would be little bit more funny but as it is programming it is not real world you can create whatever you want know or not even care about it but it depends on the condition when you are developing a software you will use this object to end programming a lot so here we just created i think three basic variables and now let us create a constructor so now what exactly is a constructor so now as we have created this variables when we actually create an object or initialize an object so that time so this actually class so you might be able to what is uh creating an object initializing an object so as this class right now this is not actually the object itself this we are just defining that an object will be like this so using this blueprint we must create an object so here for simple uh demonstration i'll just create human so i'll just create myself so ah so human aditya and here that's done we have created a human object and here we can just write you uh i think it yeah sorry i can just write aditya dot number of ice and i'll get the number of ice i have so that's how it works so now and i think i must even change it to a over here so that small a always uh the class name should actually start with the capital letter rather than the uh variable itself now guys we must actually define a constructor so we were not even getting some intel essence right that will all be fixed after we define a constructor so now what exactly is a constructor right now these all variables are having the value of null so we must actually give them some value right so that's why we use a constructor and constructor doesn't have any return type so you won't write in or void so you will just write human and here define it just like this and you won't write any statement like that as i mentioned and here i can just write or if i can even give named like that so i can just write in and mostly i'll even enclose this into like this so these are all optional parameters now i can just create something like if so now it will check actually what are these values if they are equal to null then set it to this and here it will even go to the if statement else and all those things and let us check actually how does this work really or not i think i have made some mistake i don't know it happens in programming you can make mistakes now let us check it so i'll just write aditya dot and here we must actually okay here we have declared the variable now let us actually give some values so for giving i can just write lex is equal to two and suppose let us give just like three and i'll change it to something like alien human is an alien and here we'll even give a comma and we'll say hands is equal to 4 so let us leave the eyes itself same like that and now guys let us print them so print and here we'll write alien dot and here we'll just write number of uh hands and ends to the semicolon so here we get some error so guys we get an error the reason is that we must actually enclose it within the main function and after that all the errors will go off so now i'll just keep over here like this and all the errors are gone as you can see so we were actually just keeping it in the class itself so now i think the class is also inside a whole separate block and the main function is also in a whole separate block so now guys let us run this program so to run this program we'll just open the terminal and here i'll just write dart main dot dot and now guys as you can see the output we got four so the number of ants which we gave is four and the legs is equal to three so now even let us print the number of legs as well so ctrl c and let's give control v three times to check all the values so now let us run this program again by using the command dot main dot dot and here we actually get null and none so there's actually some problem as i mentioned i have made some small mistake so it has made a mistake and right now the video is also going very long so we'll just fix this error in the next video and in that video we'll even be covering about more options how we can use functions with this so don't worry we'll fix it we'll learn more interesting topics guys and it's common you will also make a lot of mistakes guys so i hope you might have got the idea about how oriented oops concept works so guys that's it for this video i hope you like this video please be sure to subscribe to my channel and hit that bell icon so that you never miss any updates thanks for watching see you in the next video you"}], "Dart Programming Tutorial | #11 More on Functions | Aditya Burgula": [{"content": " Hey guys, welcome to the brand new video I am Antia and in this video guys we will be learning about some things on functions, especially we will be learning about how we can use optional parameters and named parameters in dot language which is also a feature provided by dot language for us. So without wasting time let's get into the video. So guys right now here on the screen I have a dot main dot dot file and here I think we can remove this dot collection. So we don't read it right now because in the previous video we have covered about the three basic kind of previous videos we have got about three basic collections or data factors that is list maps and sets. So if you have not seen them you can go and check them out and they can come over here. So guys first let us declare a function. So to declare a function first we write the written data type. So I think I even have not covered about the written data type which I will be covering in this video as well. So here first I will write I will give a name kind of my function my function and here let us declare it. So we take parameters right normally we take parameters such as int a comma int b. So like that we take the parameters right but sometimes we don't need a parameter which is sometimes optional. You sometimes you may need it in your function and if it is even not there your function is fine. So that time you can use optional functions or named parameters, optional parameters I am sorry optional parameters or named parameters. So for doing that for the next two types as I have to do optional parameters. First we will see for optional parameters you will use syntax in my list. So you will keep the two square brackets and inside this you will mention your parameters. So now these are your optional parameters you can name any of them nothing no problem and here you can name your required parameters int b. So like that I can do like that and now inside this function I will just write if a. So I will just remove this int b function int b parameter because we don't need that right now as we are only covering about this thing and here I will write is not equal to null. So null is something when there is no value inside it. Suppose we didn't give anything to a. So suppose they did not give any value or they did not use the optional parameter a. That time the variable a will be assigned to null. So if the user gives any value to the optional parameter then it will have that same value if user doesn't give then it will have null and here I will just write like this if and then print and here we will give a string and here I will say dollar symbol. So dollar symbol and a. So you use dollar symbol just called as formatting also. So in python you use dot format method if you are similar to python if you are coming from python you use dot format method and if I am in java i javascript I think we use the same dollar symbol to actually represent the variables inside a string. So here we are using a string double quotes right. So like that I can use that and here a is given as optional parameter and now here else I will say print. So I will write print and I will say optional parameter is not given and now we will end to the semicolon and here let us run this function. So to run it I can just call it so my function and here I will end to the semicolon notice you see that when we have added the square brackets it is not showing any error in the function call. Now as soon as I remove those brackets we get an error saying that one positional argument is expected. So now as soon as I add this it is changing it to an optional parameter. So it does not actually require it. No let us run without giving any any value inside it. So for that I can just go to the terminal and write dot. So see a less first and dot main dot dot and press enter and it should say that optional parameter is not given. So we have not given it. So here the value of int a is not known. So I will just when print the value value equals value of a equals dollar a. And now let us go back over here again on this program and as you can see optional parameter is not given value of a is equal to null. So I have told you if there is no value inside it then it is assigned as null. So now let us give some value to the optional parameter. Let me just close this thing and here let us let me just give an interior value. So here we have int a and no. Let us run this program again and we should get h7 is given as optional parameter. So that's about optional parameters. So guys now let's cover named parameters in Dart. So for that I can just delete the previous one and I'll use curly braces inside and here I'll write int h. So here I n and t okay now int h like that I have given a name the parameter here. I can just write h. So as you can see it has given me age colon. So you use colon and here I can say 15. So that's my age so I'll just give like that. So now it is accepting it. Now if I remove this still no problem. It will still work. Now let us just run this program again and here we are getting okay. We have we should change this to actually age. So here age and here also age and here age. So now let me just run this program again and optional parameter of age. So here also if we can change it to age then still we can have a better understanding or better printing. So here we can say optional parameter is not given to value of age is equal to null. So this is named parameter we can write named and so that's what it works and now let me just give a value inside that. So I'll write age, age, colon and here I'll give some value. And now let's run this thing and here you can see 787 is given as optional parameter. Okay."}, {"content": "I did not change it. Let's leave it like no. So that's it about named parameters and that's why flutter is so famous because it is very similar in flutter. If you're not familiar with flutter in flutter uses named parameters a lot is a lot. So that's why it became famous because this named parameter syntax is very similar to CSS or cast-cading style sheets in web development style. So there you use call-ins and you separated to commas or semi-call-ins. So that's why it was very similar very easy and people liked it a lot and that is the reason it is becoming very famous. The flutter and dark to grammar language, especially the flutter framework. Okay guys now I have covered about the optional parameters and named parameters. So I think I did not see about the written data type in the functions explanation video. So let's cover that over here. So what exactly is a written data type? So written data type or written value is something that is written up with the function is executed. So right now when the function this function is completely finished, it is not doing anything. Sometimes we want the function to do something after it completes a task. So that's what suppose like checking it or doing something or returning a value mostly you return a value. So that's what we'll do over here."}, {"content": "So I'll just try to return."}, {"content": "So here we have changed it to white. So white tells us that why does when we are creating a function saying that it has a written type of void, we are telling it that it is not going to return any value. So for right now I'll just change it to int. So right now I'm saying that return integer value and here I'll write a return H and H plus one. So this is a simple written statement. So after running this program, I'm written in T H plus one. So let's run this program and see does it work actually or how do we use it first? So first let me run this program and here nothing much difference. So let me just clear this here less and here I just give my age as 15. And after running this function, I'll just I'll create a variable called as where my name or my age equals my function. And here I'll just add like this my function. So here I think I have made a small mistake by not writing function. So I'll just write function and here function and here let me just give the value of age equals 15. And in the last we'll just print my age and get rid of the semicolon. Now let's see what will actually happen."}, {"content": "I think now we are getting the concept. So let's run this program and here we can see 15 is given as optional parameter. So it is actually running twice. I'll just remove this one."}, {"content": "So it becomes much clear. So here as you can see it has executed this program successfully when we wrote this print my age function. So after execution it is returning a value and that value is nothing but 15 plus one. And here it is given the six which we have written over here. So let me just comment this out first and let us see what happens without this. Let me just run this program and here it has run this program actually. So actually I'm sorry guys I just told you I think I told you that it is running in the print statement. No it is actually whenever we assign it it is running that thing function. So guys I hope right now you got the concept clearly."}, {"content": "If you have not understood anything so you can comment down in the comment section below. So guys that's it for this video. I hope you like this video. Please be sure to subscribe to my channel and hit that bell icon to never miss any update. And if you like this video if you like it so see you in the next video."}], "Dart Programming Tutorial | #9 Maps in Dart | Aditya Burgula": [{"content": "hey guys welcome to another brand new video guys so in this video we'll be learning about maps which is another data structure or you can call it as a con collection so in the previous video we have covered about list right so in this video as i told you we'll be covering maps so without question time let's get into the video so guys actually before coding let us understand the concept of what a map is exactly so usually when we were creating a list and doing some operations on that you might have noticed that we are using numbers to actually identify each element whereas if we take a map we have an option for actually referencing it with the name rather than a number so you can give a name and the actual definition of what exactly a map is is nothing but it is a collection of key value pages so this is most uh if you take python by them they are called as dictionary c sharp also they call as dictionary and even in swift programming language dictionary c plus plus they call it as maps in the stl so they're actually used a lot of places in a lot of programming languages maps so now let's go ahead to coding so guys as you can see i just have a void main program and i have imported a collection uh i have imported a dart collection package inside these you have all the kind of collections which you can see as you can see here maps dot dot q dot dot set the list which we have covered and now we are covering maps dot dart so let's see how we use maps so to declare a map you can just write map and here you can just give it a name just like list where we just used to write list and give it a name you can just use map and give it a name i'll write my details equals and for list we were using square brackets right for this we'll be using curly braces so curly braces is used to identify a map so here just to make much to make code much more readable i'll just press enter and it will give me a new line and here i can give anything like my name so this is a string my name equals so i'll and here i can separate each and every element using a comma so now this whole thing is an element so as you can see here we are referencing it with the name my name and here we have given the value as aditya or just the actual element you can think of as and this is a referencing variable so referencing name or you can think so guys it is not only compulsory that we can only use the string we can actually use any kind of data type you can just write true you can even use booleans and equals and you can create a string and like that it doesn't matter what variable you use but mostly people use strings to reference them because name right you can use numbers also to reference them so now i'll just write roll number equals and write one two three maybe email email equals to aditya i'll just write some random mail yes so now i'll write a true or false value like is male equals true so now this is a boolean value and here i think we are getting some error what is that error so did we just yeah we must i just forgot the comma and now we just have a simple map so first let us print it before going on performing multiple operations on these so just press my details and end it with a semicolon go to the terminal and here right mid dot main dot dot press enter and it should print all the details which we have given over here so here we have given it vertically and this printed over here horizontally so saving space so here my name aditya burgular roll number okay now it has printed all the details correctly now let us access single element so i'll write another print statement and here i'll say my details and just like indexing operator we used in the list where we use these same square brackets to access an element here also we'll be using the same square brackets to access an element so in list what we were doing is we were using numbers just like zero one or two or three like that we were using numbers whereas in this maps we use actually the names which we have given over here the key so this is called as the key part and this is called as a value what value part as i've told so this key part is nothing but the identifier so i can just try it and you must enclose it with the same kind of if you're using string over here you must use string over here and the name whatever the content is it should also match you should not keep like one capital one small it should exactly match whatever is there over here so i just want to print what is there inside my name so i'll write my name so n-a-m-e so that's actually wrong when i wrote all capitals now this is correct now let's run this program again and as you can see it has printed the value first burkla so first the print state first print statement is printing halite which is inside my name and so on we can actually print even the roll number also let us check that also roll number and now let's run this program again and should point one to three so as you can see it has printing all correctly so this is how we can use the indexing operator to again get each and every elements even in python also you just use the same indexing operator okay now let's see what all operations we can do on these so i'll write my details dot and as you can see we've got some kind of operations which you can do first we'll check the length and let me just even close it and print it actually and here print and end it with a semicolon go here run this program again and as you can see it has shown as four so we have four elements one element second element third element fourth element so that's all right now let us see what else do we have so i'll just mostly i'll comment it down control and it's commented out and let's see what else we have so my details dot and here clear or we can use contains key and clear cached add entries add all so first let us see what else are there are there any variables keys okay let us see what this keys is dot keys so this is also actually a variable a data type i think it is a list of list data type so i'll just write print and here and do the semicolon so now let's see what we get the output as so here as you can see first principle i'll just comment this out and we'll run it again so that just more clear and as you can see when we wrote mydetails.keys it has printed all the keys or all the referencing variables or all the referencing values which we have given here that is my name roll number email and ismail so these all are keys so it is printing that okay now we have checked keys let us check even other things such as my details let us go again over here dot operator and here we have add all function so here at all is similar to that list at all where we give a list of all items here also we can give a map of all items so here i can just write somewhat like um friends f r i e n d s friends equals so always give it a data type so string data type and here i can just write i can give a list also inside this i can give a list i can say uh so these are some of the names which i have my friends names and now i'm adding all these things i can i'll even create one more variable name like uh close friends so now we have added two values as you can see first is friends and second one is close friends element now let us see so i'll just comment this back out so now let's run this program and first let us see what we have got over here okay first it is printing all the keys what we have so it has successfully added friends and close friends as you can see over here the first line is showing friends and close friends also and here in the values also we got friends which is list and close friends now you might be wondering how do we access elements in these things also so for that if i want to access those i can just write my details and what is the value for this so what is the identifier for that or referencing name for that we have friends so that's the reference name so first we write friends and here again you keep a square brackets and here you can use numbers so zero i have kept and i'll go here and run and now it should print ram as you can see first one element is ram and then we have one now i'll run it again so as you can see lux so like that it works so you can use that also if you want to get each element and you can even have nested elements so we'll be talking about nested elements soon don't worry about that or the multi-dimensional arrays you can think of multi-dimensional lists so here we are talking about maps right now we have covered about my details dot add all add all function now let us check sum of function so my details dot and here we have clear so let us check clear function and let me just write this so here it is clear everything there is nothing no value inside that and here you can see the print statement actually doesn't work this friends because it has actually cleared everything so i'll just delete this so that it doesn't occur any error maybe so let me run this and as you can see we have nothing empty array it has completely cleared it now if you try to print it over here it will print it because we are doing it before the clear process so i can illustrate my details end it with a semicolon and i can go here run this program so uh here it is completely filled with all details and after i run this dot clear function it has emptied everything no elements so that's great now let us check another kind of function or method which we can use on these dot the dot and here let us check contains key and here my name i think you might have understood what this contains key will do so it will check what are the referencing variables available so does this whole map have a reference variable called as my name so mostly it will return a true or false value i think i don't know what does it retain it retains a map value so let's check it out once so here it doesn't do anything so i'll just remove these two print statements and i'll add a print statement in this print and here also end it over here and now let's run this program again and it should print actually true as you can see we got for it saying now it is saying that the key my name is present so here my name is present here my name is now i'll just change it lightly i'll just make all capitals and let's check does it work so it should give us false because it is case sensitive it is totally different so if you write just like name is totally different from name so capitalist name so i think you understood that now similarly we even have other function i think dot contains value so here i can write one two three so it is not a string so one two three and now let's run this program again and it's true because there is a value like one two three so inside a referencing variable there's a value called as one two three now if i keep a string value same one two three we should get false because it even checks the data type now let us see what else we have so my details so my details dot and here for each so these will be covering mostly so here we have removed so we'll cover this remove one also so here remove is nothing but it will just remove some value or some element so suppose i want to remove the key my name so i'll just write my case control set and here i need to write my name so my name and enter the semicolon and before that also i'll print [Music] my details and end it with a semicolon go back on this program and as you can see difference so first print statement is printing everything successfully all elements are printed my name roll number email is mail so it is printed and after we run this uh myredance.remove function it has removed the key value page that is my name from the whole map so this is how we can use some of the functions remove function i think there is even a remove all function so you can check you can check them out and if you have any doubts you can comment down below i'll be always ready to answer your queries or doubts and questions and mostly even in the future videos we'll be covering more and more functions about that once we even learn about arrow functions and stuff so guys that's it for this video i hope you like this video please be sure to subscribe to my channel and hit that bell icon so that you never miss any updates thanks for watching see you in the next video"}, {"content": " Hey guys welcome to another brand new video guys. So in this video we'll be learning about maps which is another data structure or you can call it as a collection. So in the previous video we have covered about list right. So in this video as I told you we'll be covering maps. So without much time let's get into the video. So guys actually before coding let us understand the concept of what the map is exactly. So usually when we were creating a list and doing some operations on that you might have noticed that we are using numbers to actually identify each element. Whereas if we take a map we have an option for actually referencing it with the name rather than a number. So you can give a name and the actual definition of what exactly a map is is nothing but it is a collection of key value pace. So this is most if you take Python Python they are called as dictionary C shop also they call as dictionary and even in Swift programming language dictionary C++ they call it as maps in the STL. So there actually used a lot of places in lot of programming languages maps. So now let's go ahead to coding. So guys as you can see I just have a void mine program and I have imported a collection. I have imported a dot collection package inside these you have all the kind of collections which you can see as you can see here maps dot dot q dot dot set list which we have covered and now we are covering maps dot dot. So let's see how we use maps. So to declare a map you can just write map and here you can just give it an name just like list where we just used to write list and give it a name you can just use map and give it a name. I'll write my details equals and for list we were using square brackets right for this we'll be using curly braces. So curly braces is used to identify a map. So here just to make much to make code much more readable I'll just press enter and it will give me a new line and here I can give anything like my name. So this is a string my name equals so I can separate each and every element using a comma. So now this whole thing is an element. So as you can see here we are referencing it with the name my name and here we have given the value as odd it here or just the actual element you can think of as and this is a referencing variable. So referencing name or you can think so guys it is not only compulsory that we can only use the string we can actually use any kind of tie tie you can just try true you can even use Boolean and equals and you can create a string and like that it doesn't matter what variable you use but mostly people use strings to reference them because name right you can use numbers also to reference them. So now I'll just write to roll number equals and write on to three maybe email email equals to other here I'll just write some random mail. So now I'll write a two or false value like is male equals true so now this is a Boolean value and here I think we are getting some error what is that error. So did we just here if you missed I just followed a comma and now we just have a simple map so first let us print it before going on performing multiple operations on these so just press my details and end it with the semicolon go to the terminal and here I may dot main dot dot press enter and it should print all the details which we have given over here. So here we have given it vertically and this printed here we hear horizontally so saving space. So here my name I'll be Google or all number. Okay now I just printed all the details correctly now let us access single element. So I'll write another print statement and here I'll say my details and just like indexing operator we used in the list where we use this famed square brackets to access an element here also will be using the same square brackets to access an element. So in list where what we were doing is we were using numbers just like zero one or two or three like that we were using numbers whereas in this maps we use actually the names which we have given over here the key. So this is called as the key part and this is called as a value part value part as I've told so this key part is nothing but the identifier so I can just try it and you must enclose it with the same kind of if you're using string over here you must use string over here and the name whatever the content is it should also match you should not keep like one capital one small it should exactly match whatever is there over here so I just want to print what is there and so my name so I'll write my name so NME so that's actually wrong when I wrote all capital now this is correct now let's run this program again and as you can see it has printed the value first all the people so first the print state first print statement is printing all the people which is inside my name and so on we can actually print even the roll number also let us check that also roll number and now let's run this program again and it went one to three so as you can see it has printing all correctly so this is how we can use the indexing operator to again get each and every elements even in Python also you just use the same indexing operator okay now let's see what all operations we can do on these so I'll write my details dot and as you can see we got some kind of operations which you can do first we'll check the length and let me just when close it and print it actually here print and end it with a semi-colon go here run this program again and as you can see it has shown us so we have four elements one element second element third element fourth element so that's all right now let us see what else do we have so I'll just mostly have commented down control and it's commented out and let's see what else we have so my details dot and here clear or we can use contains key and your cash to add entry is add all so first let us see what else are there are there any variable keys okay let us see what this keys is dot keys so this is also actually a variable data type I think it's a list list data type so I'll just print  And here end it with the semicolon. So now let's see what we get the output as. So here as you can see, first principle, I'll just comment this out and we'll run it again. So that it is more clear. And as you can see, when we wrote my details dot keys, it has printed all the keys or all the referencing variables or all the referencing values, which we have given here. That is my name, role number, email and is mail. So these are all our keys. So it has printing that. Okay, now we have checked keys. Let us check even other things such as my details. Let's go again over here, dot operator. And here we have add all function. So here add all is similar to that list at all where we give a list of all items. Here also we can give a map of all items. So here I can just write somewhat like friends, fri and us friends. Cool. So always give it a data type. So string data type. And here I can just write, I can give a list all change. I just I can give a list."}, {"content": "I can say. So these are some of the names which I have my friends names. And now I'm adding all these things. I can I even create one more variable name like close friends. Oh, now we have added two values. As you can see, first is friends and second one is close friends element. Now let us see."}, {"content": "So I'll just come in this back out. So now let's run this program. And first let us see what we have got over here. Okay, first it is printing all the keys what we have. So it has successfully added friends and close friends. As you can see over here, the first line to showing friends and close friends also. And here in the value also we got friends, which is list and close friends. Now you might be wondering how do we access elements in these things also. So for that, if I want to access those, I can just write my details and what is the value for this? So what is the identifier for that or referencing name for that? We have friends. So that's the reference name. So first we'll write friends. And here again, you keep a square brackets."}, {"content": "And here you can use numbers. So zero, I have kept and I'll go here and run and now we should print Ram. As you can see, first one element is Ram and then we have one now I'll run it again. So as you can see, like so like that, it works. You can use that also if you want to get each element and you can even have nested elements. So we'll be talking about nested elements soon. Don't worry about that or the multi dimensional arrays. You can think of multi dimensional list. So here we are talking about maps, right? Now we have covered about my details. Add all add all function. Now let us check some of function. So my details dot and here we have clear. So let us check clear function. And let me just write this."}, {"content": "So here it is clear. Everything there is nothing. No value inside that. And here you can see the print statement actually doesn't work this because it actually cleared everything. So I'll delete this so that it doesn't occur any error. Maybe so let me run this and as you can see, we have nothing empty array. It has completely cleared it. Now if you try to print it over here, it will print it because we are doing it before the clear process. So I can illustrate my details and it with the semicolon. And again, go here around the program."}, {"content": "So here it is completely filled with all details. And after I run this dot clear function, it has emptied everything. No elements."}, {"content": "So let's create now. Let us check another kind of function or method, which we can use on these dot keep the dot and here let us check contains key. And here my name, I think you might have understood what this contains key will do. So it will check what are the referencing variables available. So does this whole map have a reference variable called as my name? So mostly it will return a true or false value. I think I don't know what does it retain it retains a map value. Let's check it out once. So here it doesn't do anything. So I'll just remove these two print statements and I'll add a print statement in this print. And here also ended over here. And now let's run this program again. And it should print actually true as you can see over here. We got through it saying now it is saying that the key my name is present. So here my name is present here, my name is now I'll just change it. A lightly I'll just make all capitals and let's check does it work. So it should give us false because it is case sensitive. It is totally different. So if you write just like name is totally different from name. So capital is name. So I think you understood that. Now similarly we even have other function. I think dot contains value. So here I can write to 123. So it is not a string. So 123 and now let's run this program again. And it's true because there's a value like 123. So inside a referencing variable there's a value called as 123. Now if I keep a string value, same 123, we should get false because it even checks the data type. Now let us see what else we have."}, {"content": "So my details. So my details dot and here for each. So these will be covering mostly. So here we have removed. So we'll cover this remove one also. So here remove is nothing but it will just remove some value or some element. So suppose I want to remove the key my name. So I'll just write my case control set and here I need to write my name. So my name and enter to the semicolon and before that also I'll print. My details and ended with the semicolon, go back on this program. And as you can see difference. So first print statement is printing everything successfully. All elements are printed. My name or all number email is male. So it is printed. And after we run this my details or to remove function, it has removed the key value and paid that is my name from the whole map. So this is how we can use some of the functions remove function. I think there is even a remove all function. So you can check it."}, {"content": "You can check them out. And if you have any doubts, you can comment down below. I'll be always ready to answer your queries or doubts and questions. And mostly even in the future videos will be covering more and more functions about that. Once we learn about arrow functions and stuff. So guys, that's it for this video. I hope you like this video. Please be sure to subscribe to my channel and hit that bell like me so that you never miss any updates. Thanks for watching. See you in the next video."}], "Top 8 Web Development Trends for 2025 You Can\u2019t Ignore | Web Development Trends | Intellipaat": [{"content": "hello everyone and welcome back to our Channel today we are going to dive into web development trends for 2025 we'll be talking about everything from the job market and emerging Technologies to what's really driving the future of this ever evolving field first off Kos to all of you for making it through another year if you are still in Tech it means you have survived the ups and downs of 2024 navigating through layoffs staying ahead of Automation and just adapting to an industry that's constantly changing it's no small feat you know as each year is the web just becomes more and more a part of our daily lives think about it whether it's banking catching up latest news or even getting groceries delivered within minutes it's hard to imagine a life without the web now the pandemic really pushed this trend into overdrive making the demand for web development Talent so so whether you're just starting out or you have been a part of this game for a while keeping up with the latest trend is absolute key to staying competitive in this field and here's the thing the difference between a good developer and not so good developer it's the ability to embrace new challenges I mean adapting to new tools framework and practices isn't just something you could do it's something you have to do to stay relevant now let's talk about the job market for a bit employment for web developers and digital designers is projected to grow by 8% from 2023 to 2033 that's going to create about 16,000 new job openings and web development pretty exciting right and cherry on top according to glass door the average salary of of a web developer is around 450,000 so now that we have covered all of that I can't help but just wonder will the web continue to become even more embedded in our daily lives and what are the top trends and technologies that are going to shape the future all right in a with the chitchat let's dive into the very first Trend so starting up with the very first Trend in our list which is artificial intelligence okay so you all must agree that 2024 was the year that belonged to chat gbt and back in the day when developers used to rely on pair programming they used to sit together to write better codes and stack Overflow was the go-to option for finding solution but well things have changed now now with Gen AI in the mix developers get realtime help from their coding environment and tools like open AIS co- pilot on GitHub is transforming the code process by providing realtime code suggestion making development faster and more efficient and companies are also adopting AI models to enhance developers productivity overall reshaping the traditional workflow now talking about some of the starts so according to google 64% of developers feels a sense of urgency to start using gen Ai and Deo forecast that 25% of Enterprises using gen AI will deploy AI agent in 2025 with the number growing to even 50% Higher by 2027 meanwhile according to one report 92% of Chief product officers are exploring ai's potential and planning investment M so we can say that AI isn't about replacing the humans it's about boosting your creativity and productivity for example smart tooling can automatically help developers turn messy data from spreadsheet or website into structured database and with natural language processing or NLP so now analyst can now speak databases question in plain English instead of writing complex queries and U you must have heard about time series analysis which uses machine learning to predict risk in financial data helping overall business stay ahead so I wouldn't be wrong in saying that this era belongs to Ai and it's completely shaping the future of web development moving ahead with our next Trend which is internet of things or iot so think about when you ask your Amazon Alexa to play your favorite song it's a perfect example of how iot device and web application work together to deliver a seamless experience and this kind of integration is becoming more common and with current Trends and iot is all set to redefine how we interact with technology so we can say that this internet of think trend is revolutionizing the technology by enabling real-time data collection Predictive Analytics and personalized user experience it supports smart analytic remote monitoring and automation which is driving Innovation across Industries like healthcare manufacturing and Smart Homes uh while web developer might not be designing the devices themselves that's usually left to product designer and you UI export but they play a crucial role in creating the application that process analyze and display data from these devices and companies like bug laabs are already developing apis that allow developer to easily communicate with iot devices which is making it simpler to bring iot data into web application okay so as we look towards 2025 developers are increasingly focusing on integrating iot data iot data on web platform which allows for real-time data visualization Predictive Analytics and highly personalized user experience the rice for iot has been fueled by the need for smart analytics and remote monitoring especially it arised during the covid-19 pandemic and according to finance online there will be 25 billion iot device which are expected to be used by 2032 and according to blockchain the global blockchain iot market is projected to grow by2 b49 Million by 2026 so we can say that iot is truly shaping the future of technology in web development moving ahead with our next Trend which is low code no development tools so low code no development tools are becoming a major Trend in web development while AI tools are helping with coding right but building website and apps can still be challenging and costly with many project facing delays or difficulty IES and these tools are changing the game by allowing business to build web application without needing deep coding knowledge these tools are changing the game for by allowing business to build web application without needing deep coding knowledge meaning you don't need coding knowledge and depth so no code platform let you design apps by simply dragging and dropping pre-built components which makes web development faster and more accessible it's like a template so low code platform offer more advanced option but still simplify the process for developers helping them work more efficiently now talking about the demand so the demand for low Cod development is skyrocketing and according to Search growth searches for low code have risen by 286 and no code by 950 in The Last 5 Years so a report by KPMG highlights that companies using low code platform have experienced a positive return on investment and talking about the platforms like bubble and adlo are leading the charge enabling business to quickly create tools like uh budgeting apps and the project management system without extensive coding now talking about market growth the glow code Market is projected to reach by $148 billion by 20130 underscoring its lasting impact on web development Okay so let's see our next Trend which is internet of behavior also known as iob so the internet of behavior or iob is an exciting New Trend in web development that merges technology with behavioral psychology to create more personalized online experience so unlike internet of things which focuses on collecting data from devices I zeros in on understanding user Behavior by analyzing their online activities such as browser housing habits and purchasing pattern and this approach is set to revolutionize how web developers and marketers create userfriendly and engaging website so for example developers can track customers purchasing habit and understand how user interact with various devices this data can help optimize marketing campaigns in real time and create more responsive and accurate online content although iob isn't specifically tied to platform like WordPress but it can be integrated through apis and plugins to enhance user experience now talking about the job market so the iob market is expected to reach $1.6 trillion by 2027 growing at a cagr of 33.4% talking about the business adoption 75% of business believe that iob will be critical for their digital transformation efforts so ultimately iob is about using smart data to develop website an online experience that truly understand and caters to user need making it a significant step forward in web development okay so talking about the fifth Trend which is single page application so remember when web app used to feel like static pages that required a full reload every time you clicked something well that's a thing of past today we are in the world of single page applications or Spas where smth smooth app experiences are the nor and single page application have transformed how we build website offering a faster and more Dynamic interactive user experience and these apps are not just stopping here they are taking on some powerful trends that are pushing the boundaries of modern web development so by adopting trends like Progressive web apps server side rendering and web assembly they enhance performance scalability and offline functionality so talking about Progressive web applications uh they combine the best of both web and Native apps talking about serers side rendering which is making the single page application even smarter by rendering the content on the server before it hits the browser and talking about the web assembly which is an other game changer for SPS which enables high performance code to run on the browser unlocking the potential for complex task okay now talking about the market growth the global SP Market is projected to reach by 13.3 billion by 2025 growing at the CGR of 14.1% and talking about the adoption of Trends features like graphql PW and server workers are driving a surge in sbaa adoption making web app perform even more better and remain functional offline so the future of web app is dynamic and Spas are leading the way forward now talking about our six Trend which is serverless architecture so the serverless architecture eliminates the need to manage server allowing developers to focus solely on building application it ensures cost effective scalable and simplified solution perfect for realtime processing even driven task and microservices so you imagine a world where developers no longer worry about managing servers or scaling infrastructure and that's the magic of the serverless architecture with serverless architecture platform like AWS Lambda or Google Cloud functions take care of everything behind the scene you only pay for what you use so we can say pay as you go or pay as you use meaning no more shelling out for ideal server time plus it automatically scales with the demand and if you have got a sudden traffic Spike no problem serverless Ed just instantly keeping your app running smoothly so let's talk about numbers so according to a report by AWS 75% of the companies have adopted serverless architecture in some form I'm talking about market growth the seress Computing Market is expected to reach by$ 21.1 billion by 2025 growing at a rate of 38.4% from 2020 to 2025 so in short we can say that seress architecture is transforming how we build and run application making development faster cost effective and less complex and it's a future of cloud computing moving ahead with our next Trend which is container technology so container technology streamlines web development by enabling consistent portable application across environment it improves deployment speed reduces infrastructure cost and enhances the scalability all while simplifying management through tools like Dockers and cubes and it is revolutionizing web development by making it easier to build deploy and manage application so for web developers container bring faster deployment reduce infrastructure cost and improve scalability and tools like Dockers and kubernetes automate scaling and workload management allowing team to focus on coding rather than infrastructure and to keep Pace with these changes developers should familiarize themselves with containerization tools and practices and understanding how to create manage and orchestrate containers will be crucial and keeping up with trends like microservice which pair well with container technology can also help developer stay competitive now talking about deployment speed so containerization enables fast faster deployment and more efficient management especially in environment with complex infrastructure cost efficiency containers help reduce the infrastructure cost of course by eliminating the need for heavy virtual machine and promoting resource optimization now talking about our last Trend which is Edge Computing so Edge Computing is changing the way we build app by processing data closure to where the user is this makes a faster and more responsive which is especially important for things like online gaming video streaming and iot devices where every milliseconds count so by cutting down the travel time for data Everything feels quicker and smoother so one example of hedge Computing in action is CDN you must have heard about or content delivery Network they store the content closer to the user so web page load faster and server don't get overloaded so this help apps handle more user without slowing down even during busy times and plus Edge Computing help helps save bandwidth since less data needs to go back and forth to the central server now talking about some numbers The Edge Computing Market is expected to reach $66.72 billion by 2025 growing at the rate of 31% so talking about Enterprise adoption 85% of Enterprise plan to adopt Edge Computing okay so as this technology grows developers are learning to build apps that are most of edge Computing ensure they stay fast efficient and ready for the future future so that's all for the trends if you have liked this video kindly hit the like And subscribe button for intell parts YouTube channel thank you and see you in the next video [Music]"}], "Power BI Interview Questions and Answers | Power BI Certification | Intellipaat": [{"content": "[Music] hey guys welcome to this session by intellipaat business intelligence has played a very important role in driving today's businesses to success right there are many tools one can use and power bi from microsoft is one of the most popular ones and in this top power bi interview questions set we'll be checking out and taking a look at these questions that have the highest probability of occurrence in a power bi interview so use this guide to understand how you can approach these questions and how you can answer them effectively because we'll be discussing a lot about the components of power bi tables visualizations and a lot lot more so make sure to stick around make sure to check out this video and we're sure that it'll help you out to ace your interviews well guys before we begin with the session though make sure to hit that subscribe button and that bell icon so that you'll never miss any updates from us also guys uh if you're looking to become experts in business intelligence make sure to check out intellipaat's latest offerings on the same by heading to our website now well without further ado let's begin the class now coming on to the first question on this list of top power bi interview questions let's begin by checking out this what exactly is self-service bi well self-service bi or self-service business intelligence right it is called as ssbi so you can be asked a question saying what is ssbi as well and the answer is the same so it's an approach that we take to perform data analytics to report the result of these analytics and to of course complement it with good data visualization that at the end of the day will make sure that a user a client or anyone has the ability to actually you know perform all of these above mentioned tasks and uh you know understand it drive business insights out of it create dashboards without having to put much effort one of the most important things that are supposed to mention whenever you asked about the ssbi is that you know the person who's working with this right the person who's trying to provide the bi solution he or she really does not need any sort of technical proficiency or let's say a large amount of expertise when it comes to working with reporting because at the end of the day you will be making use of lot of filters that are already available uh data manipulation options that are actually put into place to help the user and uh you know you can work with it you can customize it and you can have a solution to your business intelligence problem without having to put much effort that at the end of the day is what makes ssbi special now with this we can move to question number two that states what is the difference between managed enterprise bi and self service bi whenever we talk about the managed enterprise bi right the data flow there is a little unordered so there is no structured way that the data flows uh but in the case of cell service bi the data is via an ordered flow and with this unordered data and order data flow of the difference that we have in between managed uh enterprise and cell service bi it affects a lot in the second point as well that is efficiency so when we're talking about efficiency right since we're ingesting another data since you're bringing another data into the pipeline it means that it's going to take a lot of time it's going to take a lot of effort and it's going to take a lot of pre-processing as well in the case of managed enterprise but in the case of self-service bi uh since the data itself is coming in in a structured way in an ordered way uh you know the process can go on in such a way that the process methodologies and the models can actually you know start working on the data rather than having to uh spending a lot of time worrying on you know how would how the data is coming what order is it coming in and you know working with it as well so coming on to programming skills when you talk about managed enterprise right here is where you will require training proficiency and uh you know working knowledge of how uh the bi tool works because at the end of the day here you'll be making use of complex programming skills uh and this comes by practice you can be thought a lot of things but at the end of the day practical experience and hands-on experience in the production environment matters a lot for managing enterprise bis but when you contrast it to the self-service bi here is where the code is really simple uh by the end of the day you will not require much of code at all a beginner a person who is uninitiated in the field of bi can actually get started and work on providing a decent enough solution uh you know business intelligence solution of course uh without having to put much effort into it so it's small code that has a large effect on the program or the outcome or the goal in discussion as well so this is the simple differences that lie in between a managed enterprise business intelligence tool and a self-service business intelligence tool so question number three it says why are you interested in power bi what makes you think that power bi would be a great career option there is a very good chance your interview can start with this question itself so to go on to approach this question understand it first uh you know the first part of it says why are you interested in power bi uh here you can approach it in two ways one you can tell your inclinations of why you like power bi or how you've solved a problem before and how you're planning to solve a problem using that as well but uh there's another approach that you can take is tell why power bi is something that you have to prefer because if the interviewer is expecting an answer saying why power bi why not other bi tools then you can go on to talk about the advantages of bi for example talk about the advantages of power bi for example power bi uh you know makes use of a cloud-based data reporting tool it's an amazingly easy to use visualization tool backed by microsoft it provides everything that's needed to perform uh you know data handling and data reporting as well and it's just not that after the report generation has been done dashboards have been created you know you can share it across the globe uh easily uh be it in your workspace be it to someone else uh be it online whatever it is so this one particular aspect of easily being able to finish your task and share it of course is what makes power bi very very efficient and very popular among all the business users out there because let's face it at the end of the day less work better efficiency is something that uh you know all the businesses would definitely like to enforce and this does just that so with this it's a win-win win right so that's one of the reasons of why you should be thinking towards answering it this way second part of the question says what makes you think that power bi would be a great career option well business intelligence is in demand everywhere right now right there are hundreds of thousands of businesses across the globe where everyone wants an analyst everyone wants a solution to the problem that they have a bi analyst will fit in 99.9 percent of the business companies out there now in that case considering that it is a trending career it is among the top 10 roles to have in the next decade and of course this decade as well it makes it a no-brainer that you guys would love to jump on the trend uh make full use of it and provide effective solutions to the problems we have in the world of business today so moving on to the fourth question uh states mention the important features of power bi well uh as you might already know it's now that you're on the video here uh power bi has many many features right so you can probably talk 20 minutes just about the features of power bi but then do read the room uh understand what the interviewer is expecting uh is he or she expecting a concise and a small answer or is he or she expecting you uh to elaborate on it whatever the case at least make sure you mention four you know of the important features see for example power bi is used to transform data into beautiful looking visualization and aesthetic presentation and helps you share it very easily first point the second point is that you can have the ability to bring in data from multiple sources uh you know in fact perform a quick analysis even before you work on it have an all-in-one view of it as well coming on to point number three point number three talks about how you can scale your data how you can scale your solutions right from the startups all the way till the established organizations out there be it governance beat security be it military whatever it is the span of power bi is as good as the implementation itself so at the end of the day uh when you're thinking about it you can span it anywhere you want and then fourth point uh you know once an output is basically generated what happens in the case of power bi is that users have the capability to actually scale these and show it in multiple devices at the same time if it supports the power bi application as well if you want to show the output on your phone yes possible laptop yes i have it on the cloud yes put it on a website yes so there's many many things that you can do with power bi if there is support to do it as well and the support for power bi from microsoft is of course really good so these are some of the very important points right but then when you have to try to showcase it you know a little bit more about power bi you can talk about the nlp capabilities of power bi you can talk about power bi q a which of course we'll discuss in the next couple of questions on how uh you know of course user can make use of queries on reporting by not really having to use uh complex programming languages or let's say complex querying techniques to work with the database as well so there's many many things but as i mentioned make sure to talk about the top four or five features of power bi now with this uh moving on to the fifth question which states is power bi free to use the answer is yes our users can actually make use of power bi for free in case if you have had practical experience using it well but the best of the best features of power bi can be availed with the pro subscription that costs a little bit of money of course this can be purchased from microsoft store or through the application itself uh you know it's subscription based application where you can actually go on to uh you know use enhanced features of already existing feature set that's provided with the free account so if the basic features and functionalities work good enough for you to solve your problem then perfect if not if there is a requirement for uh you know let's say more ease of use or let's say there's a requirement for more powerful filters or whatever it is then considering a pro account or a pro subscription definitely adds a lot of value because what most business users do is they have a subscribed account where uh you know it it provides a lot more features see for example instant data refreshes data refreshes by the day and uh many other powerful solutions which are missed in the free version it is totally understandable but do do know that power bi pro subscription makes power bi pack a huge punch it's a very powerful tool uh you know even in its free form but once you go paid once you go all in uh it's a beautiful app to work with as well so make sure to answer it this way coming on to question number six it states what are the versions of power bi see power bi versions uh you know you really don't have to talk about the version number and all of that when you're asked this question but talk about half power bi is basically divided right it's a tool that brings a lot of flexibility when you talk about data reporting handling and working with it and what not so to complement all of these uh you know to leverage the power of power bi and make the full use out of it there are it is divided into multiple versions we have the power bi desktop which is basically used as an on-premise software for windows 10 or any other windows for that matter and then we have the power bi services which are used for publishing and pushing the reports to the cloud directly and then when the mobile power bi where uh you know we try to cater uh solutions we um we are the mobile operating systems as well be it iosp android and more so these are the three important versions that you should mention when you ask this question it's power bi desktop power bi service and of course power bi mobile or mobile power bi now with this we can move on to the seventh question that states what is power bi desktop this can be a follow-up to the last question as well so power bi desktop is actually a windows based application that is uh used for personal computers it is useful laptops it is used for micro computers up that is basically used uh you know to design dashboards to generate reports and to publish them to the cloud at the end of the day and finding uh the most native way to do it in the windows operating system is what power bi helps us to do so by making use of power bi on pcs right a full-fledged environment is actually not even required to work with if you have a basic machine that has the capability to run power bi then it's more than sufficient that you start working with it for example for some features right you will not even require being connected to the internet as well so if you have to work offline then of course you can do it but then if you have to publish it to the cloud or share it with colleagues then of course there is the requirement for internet but i'm sure you already know that now when you're thinking about how easy it is to get started with power bi desktop well you can jump right in check out some samples out there as well and begin working uh to learn or let's say explore around power bi without having any prior experience uh using the tool as well so that's one of the big advantages of making use of power bi desktop you can get right in get your hands messy and see what you can come up with as well now moving on to the eighth question it states what are the major components of power bi power bi has five very important components that you have to talk about one is the power pivot where we talk about how we can fetch the data clean the data and load it on to what we call as power query power query is used between the user interaction between the data that is present and how we interact with it based on a lot of features or programming sets and for example operations such as loading the data pulling the data cleaning the data and a lot more and of course power qa is basically to interact with the reports that we generate from the data and this is done by using simple language as well and then we have the power view the power view is basically used to create a lot of charts graphs maps and many other types of visuals that we can use to aesthetically present the data in the form of a good dashboard or create report as well and lastly we have power map power map will make sure that you know we can have the processing power of the global icons on the screen where we're trying to uh you know depict or show the processing of accurate geographical locations that are present in the data sets uh for example sales figures in a particular country uh support operations uh you know in another part of the world a lot more that you can do with power map so when you ask this question make sure to mention the five different components of power bi that's power map power q a power query power view and of course power pivot so with this we can check out the ninth question that states what is the purpose of the get data icon in power bi this is a very commonly asked question but then it has a very simple solution the get data icon as the name suggests is actually uh used to ingest all the data into your power bi dashboard or your working environment uh you know you can have excel files you can have xml files be json pdf sharepoint csv files uh you know a lot of other files that you can directly uh ingest into your program or in fact into your working environment with just one or two clicks that is how simple it is and we use that icon for this purposes right so uh in case if you have databases such as sql server analytics services your oracle ibm mysql access microsoft access of course and a lot lot more that you can you know start pulling in and check out what data sets are supported by power bi how you can pull it and then how you can make full use out of it with one or two clicks as well that's the specialty of this particular button now if you're wondering about uh pulling data from the cloud directly the answer is yes you can directly uh you know hook up your power bi into uh microsoft azure network and uh you know multiple other cloud platforms as well and through this button uh through this icon you will find all the options and all the things that are required to pull all the data and bring it into your working environment easily that's the advantage of power bi now moving on to the tenth question it says how can we filter the data we ingested uh in power bi well data filtering is something that power bi is definitely known for its popular form what happens here is the data can be filtered by making use of a lot of filters that are already present in fact implicitly so you really don't have to explicitly start working on it if you program your power bi to do it automatically for you it has the capability to do it but then when we talk about the filters right there are three important types of filters in power bi that we have one we call as the drill through filters second we have the page level filters and third we have the report level filters when we talk about the drill through filters what here what we're trying to do here is that especially in power bi desktop uh you know users can go on to create a page where their report is present but then there are many things on a report that can get confusing right but with the drill through filter as the name suggests we're trying to drill through to the specific requirements that we have let's say you're talking about a manufacturing pipeline here you can you know go on to concentrate with suppliers concentrate about customers manufacturers the graphs that are involved behind it how these values are changing with respect to time and a lot more so to hone in on that particular requirement that we have to focus on that is where drill through filters come into the picture and then we have the page level filters page level filters where we try to filter out via each individual page as the name suggests pretty simple and then we have the report level filters again here we try to uh you know simultaneously filter all of the charts that are present in all of the pages of a single report so one report can have multiple pages if a page has to be filtered we can use page level filters if the reports have to be filtered we have report level filters but at the end of the day if you have to get into a particular report a particular page of that report then uh you know perform some focus analysis on it then you have the drill through filters so make sure to answer this in a clear and a concise way enough coming on to question number 11. question number 11 states where is data stored in power bi when when data gets ingested into the power bi environment it gets stored into two types of tables one is we have the fact table and the other we have the dimension tables so what happens in the case of fact tables is that we have a central table in the schema what we call as the star schema uh you know you might know it if you worked on data warehousing so a fact table will consider and store all of the quantitative information that is required for us to perform analytics but but but one very important thing that you have to mention about fact tables is that the data is not normalized uh in the case of fact table so make sure to mention that now when we talk about the dimension tables right dimension tables on the other hand also make use of a star schema but here what we're trying to do is we're trying to store the metadata of the data as well so think about storing attributes think about storing dimensions think about storing things that actually describe the data or describe the objects that are present in our previous fact table where the data is actually stored fact table is where we have the data dimension table is where we express and uh you know talk more about the data think of it that way so when you ask this question about where the data is stored make sure you talk about fact tables and make sure you talk about dimensional tables and of course state how each of these work as well so moving on to the 12th question uh it says what are content packs well content packs are again one of the most important things in the world of power bi because this is where we can package reports dashboards data set share it with other people very easily because one thing about a content pack is that whenever you head to our website right your portal the power bi portal uh here you can uh you know merge your workspace list here you can check out all your reports check out all your dashboards without having to put much effort into it but then when you have to stress a little bit about content packs right so what it mainly does is it provides uh you know limited access or complete access to certain charts so to list certain pages and certain reports based on you know your organization structure so if you want to deny access or authorize access to some people you can do that the second thing is that you can customize it with the title give it a little description and make sure that the user or the person who's seeing the reports of the dashboard has the ability to pick what's required and leave what's not required so with content packs that can be done and of course we have the next point which talks about how users can have access and create new dashboard from the content pack itself rather than having to open up the tool again and working with it hands-on so these three features form to be one of the uh you know most notable features of content packs and at the end of the day whenever you talk about content packs know one thing uh this is a topic where you can definitely stress on and talk about for five to ten minutes straight but then there's a very good chance that the interviewer is expecting a clear and a concise answer so make sure you keep it that way now with this we can check out the 13th question that states what are the different views uh in power bi desktop well there are three different views in power bi that you're supposed to know about and each of these view has its own unique purpose for example we have the report view in the report view what we're trying to do is we're trying to add visualizations add multiple report pages add multiple reports and see how we can publish it right then and there from the power bi desktop so this is the report view where we can actually have a full scale view of what's going on then we have the data view data view is where we perform operations such as data pre-processing data shaping and all of these by making use of the query editor tools so this is data view and then we have the uh third different view and power bi this is basically the relationship view the relationship view is used to actually show and manage the relationships that are present in the data sets uh uh you know that we are working with currently as well so each of these view as you can see a relationship view gives the uh ability for us to manage relationships data view gives us a header through which we can work with the query editor tools and we have the report view where we can add visualizations and work with the report directly as well so you can see these are three different views that do three different things as well so it is very important that you answer it this way now with this we can check out the 14th question uh that sits what is row level security this is a very important question so make sure to concentrate so a low level security basically what it does it means that it is trying to restrict the data that a user can actually access uh the user can view and work with it by making use of certain set of filters that come built in power bi so uh you know to configure this and to begin working with row level security all you have to do is you have to uh you know configure and put into place certain rows and of course certain roles and of course certain rules as well because in power bi this is very easy to do and these rules and roles that you provide is applicable as soon as you push it to the power bi service as well let's say if you have to put it in the cloud pull over website power bi mobile whatever it is so there is a function here what we call as the username function this is used along with the table relationships to see which user has access to what kind of data in the data set as well so for the current user working on it you know this function can actually show what restrictions are applicable to the current data now when you're thinking about enabling row level security right uh one important thing that you have to know about us to do this you require the power bi pro subscription so this cannot be done in the free subscription uh but then you know excel sheets can of course be used wherever uh it's being converted to the pbix file format which is the power bi index format as well so make sure you talk about how the power bi pro subscription is needed how it works how you can configure and how you can use the username function to actually showcase the relationship between the data access and the current user who's using it as well now moving on to the 15th question it says what is dax and what are the benefits of making use of variables index dax is an acronym for data analysis expressions and this is one of the most important things in power bi because this is what we use uh to query and return all of the data by making use of table expressions in power bi so it's actually a language think of it like sql uh where we're actually you know trying to use formulas where we're trying to perform basic calculations where we're trying to you know perform data analysis on all the data that's present in powerpivot uh but then we're not trying to talk about the data directly but then we're trying to see how we can work with the data hence this is where it differs from your query language uh you know it's used to compute your columns calculated fields uh your measures and you can do a lot with dax at the end of the day what you cannot do is you cannot insert or modify the data but then apart from that since your data is already ingested from multiple sources working with it performing analytics on it and then driving results out of it is very easy and that's the primary purpose of dax so what are some of the benefits of making use of dags right see when you're using variables and tags what happens is that every variable that we use in dax can be reusable so here you will really not have to write multiple queries to get a lot done small amount of queries can do a lot to your source database and at the end of the day variables are these entities that makes dax expression understandable by us so keeping it high level keeping it simple ensuring reusability is set makes our job very easy when we work with tax or data analysis expressions as well so when you ask this question make sure to provide the answer in a clean and a crisp way now coming on to the 16th question the 16th question states what is bi-directional cross-filtering in power bi well bcf as we call it in short or bi-directional cross-filtering is actually used to allow the data modelers to make sure that they can use filters the way they want to control the relationship between tables as the name says it's bi-directional right so we'll have a secondary table here you know on the other side of the actual primary table relationship where we uh propagate the data on the filter context from the first one to the second one this is done to make sure that you really don't have to use any sort of complex dax formulas or any uh you know complex data modeling entities that will help you solve uh the relationship what happens when whenever we discuss about many too many problems and to avoid uh you know the ability of actually going to mix up the data or messing up the relationship in the table as well so with this cross filtering we are trying to use multiple tables again to make sure we determine the relationship and then work with it so what this does is it simplifies the job for data modelers wherever they have to you know work with many too many problem and simplify on how data can be used pre-processed analyzed and of course at the end of the day documented and published uh you know in in the required formats as per the client's requirement or as per the goal as well so this is what bi-direction cross filtering is coming on to the 17th question is how are relationships defined in power bi desktop well one thing you should know about power bi desktop is that there is no null values there is no duplicate rows and there are relationships in between tables that can be defined in more than one way so what are these ways we have two things one is we can do it manually the second thing is we can make use of the auto detect feature for this if we have to do it manually if you have to define the relationships manually then the user can go on to use the key structure where you can use primary keys where you can use foreign keys map the data and then work with it but then if you have to make use of the auto detect feature right with the auto detect feature if we have this enabled power bi has the capability to actually understand the relationships in between tables and then map the relationships for us automatically sometimes it does it right but then sometimes you know it's a minor case where one or two relationships can get a little too itchy and messed up in that case of course you can manually go in and fix it but do understand that it's very powerful it knows what it does and 95 of the time it has the capability to detect it automatically and split out and create the relationships that we want in the case of power bi as well so with that power bi is something that offers a lot uh for a lot less effort that we put into it actually now moving on to the 18th question it says what is advanced editor in power bi well advanced editor as the name suggests is where we try to see the query that power bi uses whenever we go on to you know import the data the query is actually written in a language what we call as the power query formula language or in short it's called as the m code so to view our particular query code right especially when we're making use of power bi desktop just go to edit queries from the home tab this is where you can directly access it and you know you can work with the advanced editor to see what query is being used uh it will display the query for you what is the current query that's running what query have you selected and what transformations you can apply on it as well so of course to apply the actual transformation we're gonna have to go into the query editor and select each individual step type in the query and then work with it but then one thing you really have to know is that all the things that we can do right to the particular code that we're talking about it's easily done in the case of advanced editor because it provides all the basic functionality and a lot more as well so in that particular case you have the basic functioning bare bones of it and you have a lot more features that you may or may not use as well so to check out all of these make sure are you talking about the m code make sure you're talking about how you can go on to actually use the advanced editor and what the advanced editor does as well now moving on to the 19th question it says why should general formatting be applied to data in the case of power bi well this is a very generic question uh you know even though it might seem a little specific because at the end of the day uh you know general formatting needs to be applied to a lot of data that we have right see in power bi what we're trying to do is we're trying to categorize the data we're trying to identify data and we're trying to see what's the easiest way we can work with it so to do this you will require relationships to do this you'll require custom formulas and a lot more right now to do all of this uh you know let's say you have to create a new column to work with it you can go on to add a custom column type in the name of the column type the relationship type in the formulas and it is just four or five clicks away so general formatting is basically done to make sure that you know your task can be uh can be can be reduced to a minimum at the end of the day as and that's exactly what power bi aims to do you having to do a lot less for uh the code and the code in fact is going to do a lot more for you so to uh you know have a new column name have a new box for it uh have a formula for it to map relationship to see what it does is probably five or six clicks away and it's as simple as that so to uh kind of leverage this we make use of general format type now coming on to the 20th question it states what are the most common data shaping techniques that are present in power bi uh it's not just power bi there are many other business intelligence tools that use this as well some of the most common things are you know manually removing columns and rows based on the requirement we can add indexes to keep the data a little more structured we can apply a sorting order to see how we can work with the data and how we can actually work with it in a structured way and of course there are many other things too but what are some of the most widely used data shaping techniques if it is asked the answer is this either manual removal of column and rows or addition of indices or of course you know sorting the data based on an order as well so moving on to the 21st question it says which in-memory analytics engine is used in the case of power pivot so one important talk about power pivot is of course it's in memory analytics engine right and there is no other engine unlike x velocity in my opinion because this engine has the capability to handle large amount of data store it and columnar databases and present it to you like it's absolutely nothing like it it doesn't break a sweat to do it that is how efficient it is and that is how intelligent can get as well so this is done very quickly in a very rapid manner because the data actually gets stored onto the ram and uh since it is on the ram and not on the hard disk your processing speed your data read speed your data write speed has gone exponentially up and what this does it makes sure that your processing speed is very high and that you can uh you know use it rapidly as well and this is exactly what makes the x velocity engine uh you know very powerful as well so with this we can come to the 22nd question that states what is power bi designer power bi designer is actually a new tool that's been put into power bi where it provides the ability for the users to create a lot of good-looking reports intuitive dashboards without you having to put much effort again so you can change the visual aspects of the data the aesthetics of the data with probably four or uh you know five clicks as well because this is done to make sure that you can spend a good amount of time with the actual process of the goal with the actual process of making a decision based on the analytics that you just drove uh rather than having to spend time uh you know creating all of these reports so this easy manipulation of for the aesthetic part of it is exactly what power bi designer does uh one thing one thing about the power bi designer that microsoft is doing right is that they have put in a lot of drag and drop capabilities what it means is that you know you can start dragging in and dropping in our data let's say where you want to create a canvas report or let's say if you have a layout where you want to start uh dragging in widgets dragging in charts dragging in maps whatever it is right it's only a mouse click or a pull away that's how simplified things have become uh in today's world of power bi with the help of power bi designer so when you ask this question make sure to answer it in a clear and a concise way coming on to question number 23 it states is it possible to refresh power bi reports after they get published to the cloud the answer is a very simple and a straightforward yes because gateways are put into place for us to do just this after the rippers have been pushed to the cloud we can uh you know edit it and we can refresh it based on the changes in the data that has happened so how can we do this there are two ways so if you have shared it using sharepoint then we have the dmg gateway or the data management gateway that we make use of it and then if you have shared it on powerbi.com then we have the power bi personal gateway that we can make use for this purpose so depending on how you know the data gets published to the cloud either by using sharepoint or either by using the powerbi.com website we have two gateways that help us refresh the data now after understanding this uh there could be a follow-up question which states what are the gateways that are available in power bi and why do we use them right see a gateway uh is as you just saw in the previous question a bridge that is used to gap the differences and a gateway as you previously saw is basically a bridge that is used to gap the on-premise data with the azure cloud services that are present so to do this right to make use of gateways are there are two types of gateways that are present in power bi one we have the personal gateway and we have the on-premise gateway a personal gateway is one there where we can you know have the data imported and make sure we use it but only by making use of power bi service and on their website so by making use of personal gateway check out the name it says personal uh this means that it can only be used accessed and worked on by one authority or one person now we have the on-premise gateway on-premise gateway is let's say an expanded version or an addition to personal gateway with a lot more features here you can work with direct query you can have multiple users work with this you can have many users refresh the data to see uh you know the progress as well to see when the refresh has happened at what time you can basically get analytics about refreshing as well so that is how powerful gateways are so you ask this question make sure to talk about the personal gateway make sure to mention the on-premise gateway about how uh you know personal gateway only works for power bi servers while the on-premise gateway allows multiple users to work with as well moving on to the 25th question it states how can geographical data be mapped into power bi reports geographical data can be mapped into the power bi reports in a very simple fashion it's by making use of map charts and filled map charts right so i'm sure if you're at this uh point of time in your interview preparation you will already know this or you would have used this as well so by making use of map charts and field map charts uh we find the easiest way where we can visualize uh geographical data be it your local locality be it your local regional data or uh you know global data as well power bi has a one important integration is with bing maps where we can pick up the default coordinates for locations you can use geocoding and you can make use of mapping places without having to know its latitude and longitude coordinates directly as well so with this integration with bing maps it makes it very intuitive for the user and the developer to actually go on to work with the tool uh you know rather than working on understanding knowing noting and using longitudes and latitudes as well but then of course if you have to use it you can but there do you have to compulsorily use it the answer is no now moving on to the 26th question it says what happens when you click on a single data point in one of the multiple visuals in a report see it's very simple what happens when you click on a single data point is that the data gets selected and copied into your clipboard that's about it but then it's a very tricky question and sometimes people fumble upon answering this but they do not do that now you know the answer to this right so when you go on to do that all your data gets selected the single data point of course it gets copied to the clipboard and of course whenever you have copied data you can paste it anywhere that you need it and use it and this is at the end of the day put into place to make sure that you can work with power bi effectively and uh you really don't have to spend much time to go on to spend much time with all of these native things as well so when you click on a data point it gets copied use this in the clipboard to paste it wherever you require it and use it as simple as that moving on to the 27th question it states what is z order in power bi z order is actually a design strategy that is put into place where we try to arrange the visuals by making use of certain shapes so what the z order does is that you know we can define it we can have an implementation where we're trying to show the report which consists of multiple elements where we're trying to refresh the display you know of how the order of elements gets changed in case the reports itself are changed see you'll have data in the report and this will be constant and working fine but then if your data changes in the report there's a very good chance your report can change too right so let's say your order of item changes uh your item entity itself changes or there's a change in the relationship in this particular case we make use of this strategical approach where we're trying to arrange our visuals by considering the shapes so with this what happens is that you know when anything changes right it really doesn't mess up mess up the dashboard or mess up the workstation but it works really well and it holds the aesthetic ability for uh you know the display as well so in that particular reference z order is a very important thing in power bi so coming on to the 28th question it states how do you stay up to date with the new and upcoming concepts in power bi or in bi itself see whenever you're talking about this question right in a business intelligence interview this is a very commonly asked question the interviewer here he or she in fact is trying to assess uh your interests of power bi to see if you have the ability the interest to actually you know go on to explore new things that come into this world of bi and how best you can leverage it because today's software is tomorrow's obsolete when you think about it right even in the case of power bi yes uh just check out the initial releases of power bi and power bi now uh there's a day and night difference with the number of added features and how simplified power bi is to use these days so in that particular case the interviewer tries to understand on how you approach something new how do you understand it how you learn it and how you implement it as well because it's not just learning new things right in the world of business intelligence there is a problem that requires a solution it is as simple as that if you know the latest bells and whistles the latest tools and techniques that you can go on to use it then of course it makes your job very easy it saves you time and money it saves the employer time and money now at the end of it you know the interviewer wants to understand how well you are prepared to scale yourself up in a production environment as well so if you're a fresher or a person out of college you might not have a lot of production experience right in a production environment now uh with this the interviewer can get a solid idea of you having the capability to learn new things and effectively build on it too so answer it in a clear way state your interest so state how you you know find your own ways to learn new things there is no one answer here that will fit for all of you guys so i suggest you sit uh have a notepad uh you know make note of how you learn new things and state it that way now moving on to the 29th question it says do you have any experience working in the same industry as ours before this is definitely a commonly asked question in a power bi interview because as i told you business intelligence is required for every industry every field that you can think of in one way or another so let's say if you are applying uh you know for the military let's say if you're applying to perform business intelligence for a social media for a hospital or let's say a medical firm in that particular case they will want to know in case if you have worked in the same domain uh it's just data familiarity it's just knowing that you will be a better fit to that team there so if you're a fresher of course you can just say no that you really don't have any experiences or you can take a little intelligent approach where you try to probably talk about the projects that you have done either in the certification program or you know in your college or something that that that has helped you understand this industry that you have tried to apply that you're trying to apply to so uh make sure to talk about the experience in case if you have the experience but then if you do not uh if you're a fresher right out of college then do talk about the tools the techniques that you have used and how you think that'll be a best fit for the industry that you're applying for enough moving on to the 30th question it states have you earned any sort of certification to improve your learning and your implementation process this is a very very very important question because interviewers especially in the world of business intelligence they understand the weightage they understand the leverage that are certified bi uh you know comes into the picture because at the end of the day uh you know they know that if you have put in time and effort into into a certification program that you've learned something new uh it provides them a confidence that you have the ability to work on your career and this is what you want to do in the long term that you have the strong aspiration to actually learn on new things pick up new things and eventually solve problems by making complete use of your knowledge as well and of course you being an effective learner is something that they'll definitely look forward to if they're hiring you right these are some of the aspects that they expect for and with the certification program you already know that uh you will have comprehensive knowledge taught by experts a good certification program will give you industry level projects uh you know it will give you access to a lot of things where you can learn power bi or any other id concept for that matter learn it understand what the problem is that you're trying to solve find out ways on how you can uh you know approach experts uh you know learn from them and at the end of the day work with the community or you can solo it out through to solve a problem because any id concept right think about big data think about ai think about business intelligence whatever it is everything here is put into place because we want to make use of the computational power around us to simplify a problem or simplify a task to the fullest that is your advantage and as first i mentioned the first three points we discussed was the advantage that the interviewer wants to see in you so uh when you're talking about a certification program right it's it always adds value that you talk about what you learned and how you implemented your learning there uh you you might have three or four certifications or let's see even more as well in that particular case quickly list all the certifications you have and if you feel like the interviewer wants more details about what you learned which he or she definitely will do in that particular case start elaborating about what you learned and what you got out of it because the outcome of a certification program is very important not uh you know what the certification program is so talk about what you learned talk about how you plan to implement it talk about how you are you have already implemented it in the past and you should be good well guys i take this opportunity to wish you guys all the very best uh for all of your interviews with this set you have a clear idea of how you can approach these top uh power bi interview questions in a clear and a concise way and how you can answer them to the best of your abilities as well so with this i'm sure uh you know you guys can be one step closer to acing all of your power bi interviews so all the best for that and i'll see you on the next one we hope this session was very informative for you all if you have any comments or any questions you want to add to this set head to the comment section now and do it we'll be more than happy to help you out there so with this i wish you all the best for your power bi interview and i'll see you on the next one"}], "React Tutorial for Beginners 2024 | Learn Reactjs 19 in 15 minutes": [{"content": "Hey there welcome to today's video today I'm\u00a0\nexcited to dive into the world of React with\u00a0\u00a0 you whether you're just starting out or looking\u00a0\nto deepen your understanding this video is for\u00a0\u00a0 you we'll explore the Core Concepts of react\u00a0\nthat every developer should know and master\u00a0\u00a0 these concepts are essential for building the\u00a0\nfeatures you see on Modern websites and they're\u00a0\u00a0 likely what interviewers will expect you to be\u00a0\nfamiliar with so let's Jump Right In and cover\u00a0\u00a0 the key ideas behind react react in simple terms\u00a0\nis a JavaScript library specifically designed\u00a0\u00a0 for crafting user interfaces you know those cool\u00a0\ninterfaces on websites like Facebook Netflix and\u00a0\u00a0 Airbnb yep many of those are built using react\u00a0\nit gives us developers a bunch of tools and a\u00a0\u00a0 clear way to organize our work which speeds up the\u00a0\nwhole process of building these interfaces so with\u00a0\u00a0 react making those website designs becomes a whole\u00a0\nlot easier and faster now let's talk about single\u00a0\u00a0 page applications you've probably come across\u00a0\nthem a lot but just in case you haven't let me\u00a0\u00a0 quickly explain in regular websites every time\u00a0\nyou click on a link the browser fetches a whole\u00a0\u00a0 new page from the server it's like flipping\u00a0\nthrough pages in a book but with single page\u00a0\u00a0 applications it's different we start with just\u00a0\none page and whenever you click on something\u00a0\u00a0 instead of fetching a new page we update only the\u00a0\nparts of the page that need changing it's like\u00a0\u00a0 rearranging Furniture in a room instead of moving\u00a0\nto a new house every time you want to change this\u00a0\u00a0 approach makes things faster and smoother for\u00a0\nusers because they don't have to wait for new\u00a0\u00a0 pages to load and guess what react fits perfectly\u00a0\ninto this single page applications model making\u00a0\u00a0 it even easier to create Dynamic and responsive\u00a0\nweb applications now let's dive into components\u00a0\u00a0 they're like building blocks for our website's\u00a0\nvisual design allowing us to break down our user\u00a0\u00a0 interface into smaller reusable parts now how you\u00a0\norganize your application is entirely up to you\u00a0\u00a0 but typically we create separate components for\u00a0\neach part of our UI and then piece them together\u00a0\u00a0 into a bigger component forming the UI for a\u00a0\nspecific page so what exactly is a component\u00a0\u00a0 it's essentially a JavaScript function that gives\u00a0\nback some HTML code but wait there's a little\u00a0\u00a0 twist here we actually use something called jsx\u00a0\nwhich we'll delve into in a moment and here's a\u00a0\u00a0 neat thing about components you can Nest them\u00a0\nas deeply as you want picture it like nesting\u00a0\u00a0 dolls a component can hold another component and\u00a0\nthat one can hold even more components it's like\u00a0\u00a0 building with Lego bricks you can create intricate\u00a0\nstructures by by combining smaller pieces together\u00a0\u00a0 now let's take a closer look at jsx instead\u00a0\nof writing regular HTML tags we'll be using\u00a0\u00a0 something called jsx short for JavaScript XML\u00a0\nit looks a lot like HTML but with a few tweaks\u00a0\u00a0 and extra features for instance you can use curly\u00a0\nbraces to insert variables and even add JavaScript\u00a0\u00a0 directly into your HTML while jsx tags resemble\u00a0\nHTML tags there are some differences for example\u00a0\u00a0 class declarations are written as class name and\u00a0\nevent handlers are added differently now here's\u00a0\u00a0 the thing browsers can't understand jsx directly\u00a0\nso before it gets displayed on a web page it's\u00a0\u00a0 run through a special compiler that converts\u00a0\nit into regular HTML and JavaScript this way\u00a0\u00a0 it can be understood and rendered correctly by\u00a0\nthe browser now let's shift our Focus to react\u00a0\u00a0 router it's what helps us manage multiple pages\u00a0\nin a single page application with react we handle\u00a0\u00a0 URL navigation using a router this router keeps\u00a0\ntrack of the URL and makes sure that the right\u00a0\u00a0 components are shown based on it since we're not\u00a0\nactually loading new pages from the server the\u00a0\u00a0 router takes care of updating the components in\u00a0\nthe browser based on the current URL it's like a\u00a0\u00a0 map that guides users through through different\u00a0\nparts of our app without ever leaving the page\u00a0\u00a0 now let's delve into the concept of state state\u00a0\nis just a fancy way of saying information that a\u00a0\u00a0 component keeps track of we use state to store\u00a0\nand manage data within a component in react we\u00a0\u00a0 use something called hooks like use state to\u00a0\ncreate and manage State let's say we have a\u00a0\u00a0 list of notes we want to display in our app we\u00a0\ncan start by setting up an initial State Maybe an\u00a0\u00a0 empty list then we can fetch some data from an API\u00a0\nand update our state with that new data when we\u00a0\u00a0 update state it triggers what we call a component\u00a0\nlife cycle effect this is like a signal that tells\u00a0\u00a0 the component something has changed and it needs\u00a0\nto update we'll dive deeper into that in just a\u00a0\u00a0 bit now let's talk about props so when you want to\u00a0\nsend data from one component to another you can do\u00a0\u00a0 it using props think of props like passing a note\u00a0\nbetween friends once a component receives a prop\u00a0\u00a0 it can use that data wherever it needs to inside\u00a0\nthat component and guess what you can pass props\u00a0\u00a0 through multiple layers of components if you need\u00a0\nto but sometimes this can get a bit messy we'll\u00a0\u00a0 discuss some ways to deal with this shortly before\u00a0\nwe delve into the component life cycle I want to\u00a0\u00a0 share that I've developed four complete full stack\u00a0\nreact projects on my channel these projects cover\u00a0\u00a0 various aspects of react and are great additions\u00a0\nto your portfolio additionally if you're looking\u00a0\u00a0 for another fantastic Learning Resource I highly\u00a0\nrecommend brilliant the sponsor of this video\u00a0\u00a0 brilliant offers an interactive approach to\u00a0\nlearning math data science and computer science\u00a0\u00a0 Concepts they provide diverse lessons in different\u00a0\nsubjects organized into courses that you can go\u00a0\u00a0 through at your own pace what makes brilliant\u00a0\nstandout is its personalized content which adjust\u00a0\u00a0 to your skill level personally I found their\u00a0\ncomputer science courses particularly helpful\u00a0\u00a0 for understanding algorithms and data structures\u00a0\ntheir thinking in code course is also excellent\u00a0\u00a0 teaching coding principles in a practical real\u00a0\nworld manner whether you're new to coding or a\u00a0\u00a0 professional working on complex projects brilliant\u00a0\nis an invaluable resource for tackling coding\u00a0\u00a0 challenges when you sign up you can take a quick\u00a0\nquiz to personalize the content to your interest\u00a0\u00a0 and proficiency level whether you're just\u00a0\nstarting out or ready to write code for large\u00a0\u00a0 projects brilliant provides support throughout\u00a0\nyour Learning Journey if you're interested visit\u00a0\u00a0 brilliant.org Sahand for a 30-day free trial as\u00a0\na bonus the first 200 people who use the link in\u00a0\u00a0 the video description will also receive a 20%\u00a0\ndiscount on the annual premium subscription a\u00a0\u00a0 big thank thanks to brilliant for sponsoring\u00a0\nthis video now let's talk about the component\u00a0\u00a0 life cycle understanding this is super important\u00a0\nfor any react developer and it's a common Topic\u00a0\u00a0 in junior developer interviews so every react\u00a0\ncomponent goes through a series of phases in its\u00a0\u00a0 life cycle there are three main phases we'll focus\u00a0\non firstly we encounter the mounting phase this is\u00a0\u00a0 when the component is first added to the web page\u00a0\nnext comes the updating phas phase this happens\u00a0\u00a0 when the component needs to change Something and\u00a0\nupdate accordingly finally we have the unmounting\u00a0\u00a0 phase this occurs when the component is removed\u00a0\nfrom the web page now if you're working with\u00a0\u00a0 functional components there's a handy tool called\u00a0\nuse effect that lets you work with each of these\u00a0\u00a0 life cycle phases it's like having a guide book\u00a0\nfor each stage of a component's life now let's\u00a0\u00a0 explore react hooks hooks are like tools that\u00a0\ngive functional components superpowers before\u00a0\u00a0 hooks functional components couldn't hold any\u00a0\nstate but with hooks that all changed hooks\u00a0\u00a0 are basically functions that let us manage State\u00a0\nand do other cool stuff the two most common hooks\u00a0\u00a0 you'll use are use state which helps us set an\u00a0\nupdate State and use effect which lets us manage\u00a0\u00a0 the life cycle of our component but here's\u00a0\nthe cool part react gives us a bunch of built\u00a0\u00a0 in hooks to play with and we can even create our\u00a0\nown custom hooks it's like having a toolbox full\u00a0\u00a0 of Handy gadgets for building awesome components\u00a0\nnow let's talk about State Management while we can\u00a0\u00a0 keep track of data within individual components\u00a0\nthere are times when we need to share data across\u00a0\u00a0 multiple components imagine you have information\u00a0\nabout a logged in user that you need to access\u00a0\u00a0 in various parts of your app like the header or a\u00a0\nprofile section passing this data around through\u00a0\u00a0 props can get messy especially if it's being\u00a0\nupdated in different places so what do we do\u00a0\u00a0 we have a few options One is using the context\u00a0\nAPI provided by react itself another option is\u00a0\u00a0 using external libraries like Redux which many\u00a0\ndevelopers find helpful with these tools we can\u00a0\u00a0 create what's called Global State data that can\u00a0\nbe accessed and updated from any component in\u00a0\u00a0 our app without having to pass it around manually\u00a0\nit's like having a shared storage space where all\u00a0\u00a0 our components can access the same information\u00a0\nnow let's talk about the virtual Dom it's a key\u00a0\u00a0 Concept in react that's worth understanding here's\u00a0\nthe deal react doesn't directly manipulate the\u00a0\u00a0 actual web page the Dom when it updates components\u00a0\ninstead it creates something called the virtual\u00a0\u00a0 Dom which is like a copy of the real Dom so when\u00a0\nwe make changes to our components we're actually\u00a0\u00a0 updating this virtual Dom not the real one this\u00a0\nmight sound extra but it's actually really smart\u00a0\u00a0 react compares the virtual Dom with the real\u00a0\nDom and figures out the most efficient way to\u00a0\u00a0 update only the parts that have changed without\u00a0\ntouching the rest this makes our apps faster\u00a0\u00a0 and more efficient because we're not constantly\u00a0\nrewriting the whole web page it's like having a\u00a0\u00a0 blueprint of your house that you can tweak and\u00a0\nrefine without tearing down and rebuilding the\u00a0\u00a0 entire thing every time now let's explore the\u00a0\nconcept of a key prop when you're displaying a\u00a0\u00a0 list of items in your components there's one\u00a0\nimportant thing you need to remember the key\u00a0\u00a0 prop each item in your list should have a unique\u00a0\nkey prop think of it like an ID card for each\u00a0\u00a0 item without it you might run into errors in the\u00a0\nconsole the key prop helps react keep track of\u00a0\u00a0 which items have been changed added or removed in\u00a0\nthe list this way react knows exactly which parts\u00a0\u00a0 of the virtual Dom to update making your app run\u00a0\nsmoother it's like giving each item in your list\u00a0\u00a0 a special marker so react can manage them better\u00a0\nnow let's see how to use event listeners in react\u00a0\u00a0 it's pretty similar to how we handle events in\u00a0\nregular JavaScript but with a few tweaks in react\u00a0\u00a0 we use camel case for event names and instead\u00a0\nof adding event listeners with methods like add\u00a0\u00a0 event listener we simply pass in the function we\u00a0\nwant to call directly within curly braces in our\u00a0\u00a0 jsx code this means our JavaScript code and HTML\u00a0\nare kind of mixed together which might seem a bit\u00a0\u00a0 strange at first but it actually makes things more\u00a0\nstreamlined it's like having everything you need\u00a0\u00a0 right at your fingertips without the extra steps\u00a0\nof traditional event handling next let's explore\u00a0\u00a0 handling forms in react it's a bit different from\u00a0\nthe traditional way because we try to keep all our\u00a0\u00a0 form data stored in the component State normally\u00a0\nHTML elements like input Fields text areas and\u00a0\u00a0 drop-downs manage their own State and update as\u00a0\nusers type or select options but in react we take\u00a0\u00a0 a different approach we attach event listeners to\u00a0\neach field and update the component State whenever\u00a0\u00a0 the user makes it change so instead of relying\u00a0\non the form to handle everything we use methods\u00a0\u00a0 like onchange and onsubmit to directly update\u00a0\nour state it's like taking more control over\u00a0\u00a0 how our form behaves making it easier to manage\u00a0\nand customize now let's talk about conditional\u00a0\u00a0 rendering in react sometimes you'll need to show\u00a0\nor hide content based on certain conditions in\u00a0\u00a0 your app for example think about displaying a\u00a0\nuser's name in a navigation bar if the user is\u00a0\u00a0 logged in you want to show their name if they're\u00a0\nnot logged in you might not want to show anything\u00a0\u00a0 one way to handle this is by using the logical\u00a0\nend operator another option is to use the inline\u00a0\u00a0 if else conditional operator which allows you\u00a0\nto add extra logic if needed in both cases what\u00a0\u00a0 gets rendered depends on the conditions you set\u00a0\nit's like having a switch that turns content on\u00a0\u00a0 or off based on what's happening in your app now\u00a0\nlet's talk about some common commands you'll use\u00a0\u00a0 in every react project these are like your go-to\u00a0\ntools for getting things up and running first up\u00a0\u00a0 we've got the npx create react app or npm create\u00a0\nV commands these ones are super handy because\u00a0\u00a0 they set up all the basic files you need for a new\u00a0\nreact application next there's the start command\u00a0\u00a0 this kicks off your development server so you can\u00a0\nsee your project in action right away and finally\u00a0\u00a0 we have the Run build command this one's important\u00a0\nfor when you're ready to deploy your app it builds\u00a0\u00a0 a directory containing all your production ready\u00a0\nfiles so these three commands are like your trusty\u00a0\u00a0 Sidekicks helping you build and deploy your react\u00a0\napps smoothly all right that wraps up the Core\u00a0\u00a0 Concepts every react developer should know thanks\u00a0\nfor watching and I'll see you in the next one"}], "Lec 1 | Special Topics in Supply Chain Management": [{"content": "STEPHEN MILES: One of our\nchallenges over the next couple of days is we have\npeople who've come here from all over the world. Thank you very much for\nbeing here with all the snow and weather that has beset\nus here at the last minute. Actually, Jun Mirai\nfrom Japan was caught in a snowstorm at\nNarita airport in Japan and so won't be able to\njoin us until later today. And I'm sure that there will\nbe people trickling in as well. We wanted to keep this\nconvocation as small and focused as possible to\nsee if we could actually make some headway\nand to remind you of the objectives\nthat were set forth by the conference committee. We talked about wanting\nto identify opportunities for collaborative\nresearch, to determine what the core research\ncomponents of some of these opportunities might be,\nand use these sessions that we have today to begin a technology\nroad-mapping process that we hope to follow on with\nsubsequent events leading out of here, which we'll share\nwith you more on tomorrow. But so first, I'd\nlike to welcome you on behalf of MIT\nand the AUTO-ID Labs here at MIT, who's the\nhost for this event. My name is Steve Miles. I'm a research engineer\nhere at the lab. And I'm the conference committee\nchair for this gathering. I'd like to just give\nyou an overview of what's happening over the\nnext couple of days so you can visualize the\nagenda and then also thank some of the people\nwho are here with us today, without whom this\nevent couldn't have happened. So I guess, first to start\nwith the thank you's, there's a conference committee\nthat was very much created ad hoc, people who knew\npeople who were leading researchers in this area. And out of that\nconference committee, there are half a\ndozen key contributors who are here who I'd like\nto acknowledge and maybe ask you to stand as I say\nyour name so that people can see who you are, because, again,\nthese are good collaborators. These are people with whom\nwe've established and begun to think of ways that\nwe can work together. So Dr. Bill Hargrave from the\nUniversity of Arkansas, Bill, are you here? And Dr. Gisele Bennett\nfrom Georgia Tech-- stay standing, because\nI'm going to ask everybody to applaud when-- [LAUGHTER] --Jean Pierre\n[INAUDIBLE],, right? Jean Pierre? AUDIENCE: Yeah, here. STEPHEN MILES: OK. Dr."}, {"content": "Rob Clarke from MSU,\nDr. Dimitrios Kyritsis from EPFL in Switzerland, and,\nactually a nonacademic but who was the person\nwho gets the credit for the idea of this\nconvocation, Mark Roberti from RFID Journal-- so this group of people has been\nvery instrumental in putting together the speakers. Please acknowledge them. [APPLAUSE] And then there's a\nsecond group that's really the core group of\nacademic collaborators who had an equal share in\nputting this event together and who you will\nhear from today. Those of you who\ndon't know them, again, we'd like to recognize\nthem as a group, so Dr. Elgar Fleisch from the University of\nSt. Gallen and Duncan McFarlane from Cambridge University,\nwho's represented by Alan Thorne today-- is Alan here? AUDIENCE: [INAUDIBLE] STEPHEN MILES: Dr. Jun Mirai of Keio, who is in a snowstorm but is\nrepresented by Shigeaki Suzuki, Dr. Hao Min of Fudan\nUniversity, Dr."}, {"content": "Peter Cole from the University\nof Adelaide, and Dr. Sang Gug Lee of the Information\nand Communications University in Korea, the latest addition\nto the AUTO-ID Labs family, together with John Williams\nas the director of the AUTO-ID Labs here at MIT-- so a hand for-- [APPLAUSE] So this group has been working\ntogether now for several years, looking at ways to\ncollaborate to meet some of the opportunities that\ncome from the adoption of RFID. And then finally, we have\ngraduate students here at MIT, who have been\ninstrumental in putting this event together. Is [INAUDIBLE] here? [INAUDIBLE] may be out front. But she gets the credit for\nthe logo, for the posters out front, and has organized\na series of fun demonstrations of RFID technology\nat the MIT Museum tonight for the reception. And [INAUDIBLE],, on AV, doctoral\ncandidate in the program here, together with [INAUDIBLE],,\nwhose last name I won't try to pronounce-- and he's been the\nvolunteer coordinator. So thank you very\nmuch to that group. [APPLAUSE] So just briefly the\noutline for the day, John Williams will introduce\nour keynote speaker. Following a break,\nwe will then go into, this morning, the horizontal\ntechnology sessions, so a session on the\nnetwork technology related to RFID, a session\non the tag technology, and a session on the RF\nand the utilization of RF as we look out in\nthe 10 years to come. And then in the\nafternoon, we move right into industry\napplications for RFID, focused around research areas\nthat have been brought to the various labs\naround the world, so starting with\nanti-counterfeiting that is led by Dr. Elgar Fleisch, which is the flagship project for\nthe AUTO-ID Labs collaboration. But then an\ninitiative led by Alan at the University of\nCambridge in aeronautics-- for those of you who may not be\nfamiliar with this opportunity. Boeing and Airbus have decided\nto redesign their entire supply chain more the model of the\nautomotive industry, where they'll be able to\nexchange subassemblies. And there's a requirement\nfor an active tag to go on those subassemblies\nso that a Boeing shop might exchange a subassembly\nwith an Airbus customer. And for that to\noccur, we'll clearly involve university research and\naircraft parts manufacturers throughout the world and\nwill require a response from the academic\ncommunity that's far more coordinated than\nwhat we've done in the past."}, {"content": "And from aeronautics,\nwe go into automotive. And then we're very privileged\nto have the vice president of Johnson & Johnson, who's\nput together with the EPCglobal health care life sciences\ncoordinator program a session on health care to\nconclude the day. And then at 5 o'clock,\nwe will move up to the MIT Museum for\na reception and a time to meet one another. And also, please, take a chance\nto see the half a dozen demos, which are there showing\ndifferent applications for RFID, I believe,\none of which is an espresso machine, which\nrecognizes your taste in coffee by your cup, as I understand it. And then tomorrow, we have\na very extensive session in supply chain\nand in packaging. And after a light\nbox lunch, we'll reconvene for\napplications for RFID beyond the supply chain\nand then a concluding panel with some of the\nexecutives who are here. And part of the format\nof each of these sessions is to give academics\nno more, academics, no more than 10 minutes\nto talk about really what's key in our\nresearch and then to get some feedback in\neach of the sessions, both from industry panelists\nand from the audience as to the applicability,\nutility, and possible direction for that research. So are there any questions\nabout the logistics? There was one question. Bathrooms are located-- AUDIENCE: That way. STEPHEN MILES: That way. AUDIENCE: [INAUDIBLE]. STEPHEN MILES: For\nlunch, we are going to walk to the other side of\nMass Ave to the student center. And so we might try to do\nthis through the buildings to stay dry. We'll see how the\nweather holds up. But if there are no questions\nabout logistics at this time, I'd like to hand\nthe convocation over to Professor John\nWilliams, who's the director of the\nAUTO-ID Labs here to MIT. [APPLAUSE] JOHN WILLIAMS: Thanks very much. I want to welcome you all here\nto MIT on this typical Boston day. I often wondered why MIT\ngot so much research done until I moved here. And then I began to understand\nthat, in the winter, you're basically locked in. There's nothing to\ndo except research. So that's the explanation\nwhy MIT is pretty innovative. I'd like to thank Steve Miles,\nespecially for putting this on."}, {"content": "Really, it's been\nhis effort along with the organizing committee. But he's done a tremendous\namount, so I'd like to give a-- [APPLAUSE] Before I welcome\nour first speaker, I'd like to just\nremind you, I think, how important this is that. In the AUTO-ID Labs, this\nconsortium of seven labs, we got together as a consortium\nwith the goal of architecting the internet of things. And when you talk about\nthe internet of things, it's really an\ninfrastructure that's going to have to\nlast a hundred years. If you think about the\nrailway system, the power system, our water\nsupply systems, these are infrastructures\nthat support society and are going to have to last. I worry about this\ninfrastructure, that we need to think seriously\nabout it, that it's not just fast-moving consumer goods. It's going to be knowing what\nsomething is, where it is, when it's there,\nand why it's there. And this is going to be\ntremendously important that we future proof this architecture. And I think that's the\ngoal of this convocation, is for us to identify the\nresearch that's necessary so that this infrastructure\nwill survive. So I think it's\ntremendously important that we think about that, that\nit's a global infrastructure. It's cross-countries,\ncross-boundaries."}, {"content": "It's going to be incredibly\ndifficult to do that. So we're very\nfortunate today to have one of the founders of RFID,\nProfessor Sanjay Sharma here, who's going to tell us\nabout research in RFID. So I'd like to welcome him\nand welcome you all to MIT."}], "Lec 4 | Special Topics in Supply Chain Management": [{"content": "STEVE MILES: And so we\nstarted out with a little on the network. And it was a great deal of\nfun to think back and hear some of the origins of where\nsome of the thinking was and to have a chance to\ndiscuss exactly where we are in the evolution of this-- either it's the internet of\nthings, or according to Steve, it's things on the internet,\nwhichever we decide it is. But we're going to\nmove right into tags. And we have the director\nof the Auto-ID Labs at Fudan University in\nShanghai, Professor Hao Min, who will start off the discussion. HAO MIN: OK. And the whole RFID\ntechnology, I think, the first thing starting\nis from the RFID tags. So people are thinking\nabout the tag, and what the performance of\nthe costs of the RFID tag may just influence the\nadoption of this technology, because this is\nkind of the basis for this whole technology. So today I will talk\nabout what's the user requirement for tags right now. And so this will promote\nus to research what the next generation we can\ndo for the tag performance enhancement. And also, I'll introduce\nsome new technologies being developed right now. And there are many,\nalready, some adoptions of this RFID technology. Some users are [? users ?] tags. So it's estimated that millions\nof tags be used right now. But there are still\nsome problems. People want a longer read range. Right now, a [INAUDIBLE]\ntag can reach the range of approximately 5 meters. But people are thinking\nabout whether you need, like, 10 meters. If you reach 10 meters, more\napplication will be wider. The other thing\nthat's really critical is the 100% read coverage. From the report from\nWalmart and the [INAUDIBLE],, no one can reach\n100% read right now, probably 99%, or someone\neven only gets, like, 90%. So what happens if\n1% is still missing? So when we have any\ntechnologies, what we can improve that to get 100% read. Also there's the issue is the\npeople hope to get similar read performance in\nworldwide frequency, because in worldwide, the\nfrequencies are very different, from [INAUDIBLE] to [INAUDIBLE]\nBut different countries use different frequencies. But the tag is already there. The tag will move\naround the world, will be read in\ndifferent frequencies. Because of the RF\nproperties of these tags, the read performance in\ndifferent frequencies will be different. But people want that it should\nbe almost a similar performance around the world. And so in that sense, the\nsecurity and [INAUDIBLE].. Just the previous\nspeakers from the web, they were all talking\nabout securities. If there's no security, I think\nthe RFID application will just get very limited. So just this [INAUDIBLE]\nsay that the hack, EPC hack get attacked from-- I think it's from two days ago. So people pretend\nto be hack members, and they pull their\nposts on the hack. So all the hack members\ngot all these junk mails. So this is the issue, is\nwhen internet comes out, there's no security concerns. Internet comes from academics. So at first, DOD\nsponsor the program, and then grow and grow. Just no one is planning\nall the security issues on the internet. Right now the\nproblems comes out. So right now we're talking\nabout internet of things. We need to really think\nabout the security issues. So what we are doing right now? We are trying to\nimprove the read range. Improve the read\nrange is basically is to reduce the power\nconsumption of the tag chip. If it consumes less power, than\nthe read range will be longer. So what we are doing is we\ntry to improve the conversion efficiency for\nrectifier, because this is the IF signals coming from\nthe reader and to the tag chip. And the tag chip will convert\nthat into a DC, DC power, to a [INAUDIBLE] chip. So if you can improve the\nconversion efficiency, then it's the read range\nwill get also improved. And also, there are some\nspecial technologies being studied to reduce\nthe power consumption. For example, as we called\nsubthreshold digital circuits. If you have a background\non digital circuits, we know why we\nneed a power supply to power the digital circuitry,\nbecause the digital circuitry is a need to switch. But there's a threshold to\nswitch the circuit on and off. So there is a threshold. But if the threshold is\nlower, then the voltage applied to that will be lower. Right now there's a circuitry\nwhich we do not use the regular on and off mechanism. But we use a circuit\ncalled subthreshold. Even if you apply\na voltage which is lower than the\nthreshold, if still controls the circuit on off. Then this will reduce\nthe power consumption. And also there is a\nnew circuit record called [INAUDIBLE] circuitry. Although this circuitry\ntype is a long time ago, but recently we find\nout this probably can apply to the\ntag chip design. There's also some other\nconventional low-power circuit design technologies. This is what we've\ndone in recently. We recently published\na paper here, published paper on the improved\nthe rectifier efficiency by about 30% to 100%. That means double\nthe efficiency. Normally, the efficiency\nis around, like, 20%, 30%. And we can improve\nit into 40% or 60%. Basically we use\ncalled a bootstrap to rectifier to get rid of\nthe VT drop of the circuitry. The second thing\nwe are working on is there's a circuitry called\nadiabatic circuitry, which means the charge in the\ncircuitry can be recycled. But previously, some people\nalready published the papers. They used this\nadiabatic circuitry, but they convert it normally. Normally an electronic system\nis powered by a DC power. So they convert the DC\npower to a power clock, because adiabatic circuitry\nneeds a power clock, and then convert-- and then to power the\nadiabatic circuitry. But in RFID, because we\nalready got the power supply from the radio\nway, radio signal. So the radio signal, it's\nalready powered clock. It's already clock. So we can directly use\nthat as the power clock. So then we can rid of the\nrectifier circuitry, so which eliminates the\nloss of the energy. And also, it's because the\nadiabatic circuitry consumes less power than\nnormal logic, then the whole power\nconsumption of the tag is getting reduced a lot. By doing this, we estimated that\nprobably we can get only 10% of the power consumption\nas normal circuitry. So this means what\nwe can do-- we can increase the read range of\nthe tag by a factor of 2 to 3. Since I already talked about\nthe subthreshold circuitry, this curve shows that\nthe power consumption by a different power\nsupply voltage. If we use the supply\nvoltage of, say, less than the threshold\nvoltage of a transistor, we may get four orders\nof magnitude of reduction of power consumption. But this circuitry has the\nlimit of the working frequency is very low, probably\nonly go to 1 meg. We've got a simulation. If we let the circuitry only\nwork at 1 meg, at 1 volt, a tag chip only consumes 10\nnanoamp, which is only 1% of a normal power\nconsumption of tag. By doing this, we may\nincrease the read range by a factor of even 10. And also, we are trying to\nfigure out what really prevents your read from 100% of the tag. So why these missing tag? The tag is missing. A reason we find\nour way that if you put two tags next to each other,\none is closer to the reader. Another one is behind that. And the one behind that will\nprobably cannot be read. But if you remove the\none closer to the read, the one there can be read. So look like the one\ncloser to the reader masked the tags behind. This is because the\nnormal tag technology, they use a rectifier,\nbecause when the tag is closer to the\nreader, it gets more energy. So the energy goes into the\ntech needs to go somewhere. So it has a circuitry that\nconvert it just to heat. So it consumes high power when\nthe tag close to the reader. Because the tag\nconsumes the power, then the magnetic wave\nwill just stop there, will not go further\ninto the tag after that. So what we do, we are trying\nto have a new circuitry, which try when-- it detects whether\nthe tag got enough power. If get enough\npower, it just tries to detune the tune circuitry\nso that it does not consume energy. It only absorbs\nenergy it needed. All the other energy\nis just let go away. So this can have this\ntag behind that can get enough energy to get power. So this is we are doing. So what we are\ndoing right now is so we can detect the\npower supply of the tag and then compare to\na reference, and then try to tune the [? resonant ?]\ncapacitor of the antenna. And it just makes it work. Also, for the worldwide\nfrequency of things, we also use this adaptive tag-- adaptive tuning waves. So it will detect the power. If get less power, it will\ntry to tune the resonant cap to find a best\npoint, which resonant at the frequency of the reader. So this make the\nperformance get improved."}, {"content": "Also, security and\n[? authentications ?] get considered. So there will be a two-way\n[? authentication ?] is designed to both the reader-- you [? authenticate ?] the\ntags, and the reader tag get [? authentication ?]\nfrom the readers. But this is really\nchallenging because the tag is so tiny chip, and\nit consumes less power. So it needs a low\ncost, low power. But it also needs\nadequate security between tag and readers. We have tried a way\nwhich we integrate some [? authentication ?]\ninto the gen 2 protocol to try to have the tag and the\nreaders get authentication. We have some first\nsimulations down."}, {"content": "It kind of looks like work,\nbut we still need a lot of work to do. OK, that's my presentation."}, {"content": "Thank you. [APPLAUSE] STEVE MILES: And\nour next speaker is Sang-Gug Lee, who is\nan associate professor in the School of Engineering at\nthe Auto-ID Labs at ICU Korea. SANG-GUG LEE: Yeah. In my talk, I would like to\nquickly go through the USN technology in Korea,\nand then we introduce what we are working on\nin Auto-ID Lab Korea. What I mean by \"USN\" is an\nUbiquitous Sensor Network, which is known as a\nwireless sensing network. So the content would\nbe some brief overview of what is going on\ninside Korea in this area, and then in relation to what's\ngoing on in Korea, what we are doing in Auto-ID Lab Korea. Probably some of you are\nfamiliar with this A39. It's called the Information\nTechnology A39 Strategy that Korean government\ncame up with. And there are three\nareas where we are trying to promote services,\nand then three infrastructures, and then nine different\nareas of growth engine that the Korean\ngovernment is working on. And in this A39 strategy, you\nfind that in the service area, the government is\ntrying to provide the RFID-based\nservices, and then also in the structure area by\nutilizing the Ubiquitous Sensor Network. We consider-- the Korean\ngovernment considers-- Ubiquitous Sensor Network as\none of the infrastructures that they would like to deploy\nin Korea, in South Korea. And then through that, hoping\nto be able to deploy these nine different area across engines. And if we're being more\nspecific with the time schedule, each of these nine areas,\nthey have schedules, and then when they would like to--\nwhat kind of technology for the commercialization. And if you look at the\npink-colored icon there, this is the RFID and USN\narea, where the government is trying to commercialize a\nmobile RFID by year 2006, which is before the end of\nthis year, and then hoping to be able to deploy\nthe sensor tag or sensor node technology for the\npublic application. And the concept of a public\nubiquitous sensor network is basically based on\nbroadband convergence network, all these services, where the\nsensor network-based services would be available\nsuch that everywhere, everything with RFID tags. And we sense the sensing IDs\nand environmental information, and then real-time\nmonitoring and the control through the network\nwould be available. That's the idea here. And the applications,\nI'm sure most of you are already familiar with. They are smart buildings,\nfactory automation and monitoring, asset\nmonitoring and management, structural health monitoring,\nand environmental monitoring. All this, we call this as\na public ubiquitous sensor network technology. These are the targets the\ngovernment is thrusting. And if you look at\nthe technology tree, these are the infrastructures,\nstudying the sensor, and then service\navailable there. While I was watching\nthis morning, I was getting this\nphilosophical question. We talk about all\nthese technologies trying to make our life really\neasy and happy and all that. But I'm also, at the\nsame time, finding out how all these\ntechnologies actually make our life miserable. You realize what I mean, right? All these mobile phone\ntechnologies and internet. It seems like it's helping\nus in many different areas. But at the same\ntime, we're finding ourselves running like crazy. It makes our life so busy. It doesn't really\nmake our life happy. But I guess the\nrace has started."}, {"content": "It's just an unstoppable\nrace that everybody has to run all at the same time. But anyway, so these are what\nthe government is trying to do, and this is their roadmap for\nactual milestones and roadmaps and how they would\nlike to implement these technologies for the\ncommercial applications. And in relation to that, what we\ntry to do in Auto-ID Lab Korea is that-- these are what I have just\ntalked about is basically of public ubiquitous\nsensor network technology that they try to deploy. And in Auto-ID Lab Korea\nhere, what we're trying to do is we try to make this\npublic ubiquitous sensor network [INAUDIBLE]\nconnected to the EPC network. So the theme of technology\nthrust in Auto-ID Lab Korea is EPC network base a\nsensor network technology. That's what we're\ntrying to go after. And we have seven professors\ninvolved in this thrust coming from different backgrounds. And our focus area is the\nhardware and communication technology for EPC-based\nnext-generation ubiquitous sensor network, and some of the\nmiddleware technology for EPC sensor network, and then the\nprivacy and security issues, as well as the business model\ndevelopment for the EPC sensor network applications. And these are some\nof the research work that has been done as\npart of those thrust. And this is some\nhardware [INAUDIBLE] we've been working\non the impulse radio development, which is being\ndeveloped for the ranging and locationing. There's some typo there. We've been working on some\npulse generator circuits, very low-power pulse generator\ncircuits on that, and then also very low-power correlator\ncircuits and stuff like that. So basically, we're working\non transceiver designs, modems for some algorithms, and then\nfor arranging and location purposes. At the same time,\nwe're also working on the reactive\nmicroradio technology, which means that the sensor\nresponding to the signal. In other words, the sensor\nis under the sleep mode and responds only when\nit is being waked up by some wake-up signals,\nwhich is, I'm sure, a number of research institutes\nare working on at the moment. Also, this is some of\nthe work that we're working on on the sensor network\n[INAUDIBLE] architecture, where the circle area, I think, the\nwhole [INAUDIBLE] represents the EPC network. And the circle part\nrepresents the area where in order for\nthis EPC network being connected to the public\nubiquitous sensor network system. And we all saw some progress\nis being made in the secret and privacy area as well. And through all this\nactivity-- in other words, [? RF-ran ?] chip sensor\ninterfaces and networking and software, as well as\na business application, privacy and security issue,\nwe're hoping that the research that we're doing would\nbe related to the future standardization that connects\nthe public ubiquitous sensor network with the EPC network. That's it. [APPLAUSE] STEVE MILES: Our next\nspeaker and member of the conference\ncommittee, Gisele Bennett from Georgia Tech. GISELE BENNETT: All\nright, I am going to somewhat switch gears and\ngo as quickly as I possibly can with the 10 minutes\nallocated with, I think, 30-some odd slides. And the whole point that\nI hope you walk away with is understanding the\nimportance of requirements. We've talked about\nRFID, and we've talked about a number of applications. But understanding the\nrequirements and what solution is going to meet your\napplication is really critical. And we had a project\nthat I'm going to focus on, on a container project, and\nlooking at sensors that have built-in-- tags that have built-in sensors\nto monitor the environment, to monitor the\ncondition of an asset. And so this is one\nof my favorite slides because I think it goes\nback to World War II, if I'm not mistaken. And really, you're really\ntrying to find something in all the things that\nwe're talking about. RFID happens to\nbe one mechanism. And of course, the big motivator\nwas the Walmart and the DOD. And one of my\nfocus areas will be the DOD particular application. RFID's everywhere-- comes\nunder different forms, different marketing, but\neverybody's pushing towards it. And really, as I\nindicated initially, asset tracking is\nreally our focus. So if it's RFID, terrific. If it's some other\nmechanism, that's good too. Understanding the requirements\nis a really critical element to all of this. As you can see-- and\nthis is, I think, actually, I have to\napologize, an old slide, because I'm sure the\nnumber of patents have actually increased\nbeyond that in the RFID arena. I don't need to get into\nwhat automatic identification tracking is with this audience. We've got a number of elements."}, {"content": "We've talked about them in\nsome of the other talks. We're going to talk about them-- they're going to come\nup in other discussions. But again, tracking something,\nstoring information, and doing something\nwith that data. All of these elements\nhave come up in the talks. They're all critical\nelements, and they come in different shapes and forms. Biometrics is another\ntechnology that should be looked at for\nsecurity, authenticity. And that's something that\nmight be integrated in-- not RFID, but integrated within\nthe RFID systems, if you will, in data gathering. Now, when I talk\nabout requirements, this is one of my\nfavorite slides, and I contribute this to Nick\n[? Toogis ?] from the DOD IAT office. And an understanding of\nrequires is really critical. So Walmart has been\nthe pin-up CPG that gets referenced as the start\nof all of the RFID flurry, and DOD followed\nsuit soon thereafter. But DOD can't really apply a lot\nof the commercial applications, because if they could, then\ntheir stores would move-- Walmart stores would\nmove every so often. Christmas would be a random\nevent, maybe every five years, not once a year in December. The associates would be wearing\ndifferent types of vests. And a stock-out means\nsomething completely different for the DOD\nthan it does for Walmart. So understanding\nthose requirements and the environment you have\nto work in kind of changes what technology\nyou're going to use and how you're going to\napply that technology. The way we got into this, we\nwere approached by the Navy to solve a problem of managing\ntheir high-value assets and, in particular, the engines. The problem is they\nwere improperly stored and improperly tracked. So when they were able to\nfind an engine that they were looking for,\nthey were supposed to be in pretty good condition. They'd find it\nfloating in water. And thus, what was a\nperfectly good engine now had to be sent\nback to the depot. So you can imagine the\ncost, the readiness issues, all of the implications. And that's assuming you\nfound the container that had that engine. So with that, what we ended up\ndoing is looking at an active RFID tag and looking at\na tag that would monitor the container, which is the\nhousing element for the asset that we were interested in--\nnot necessarily the container, but it was the asset-- and telling you\nwhere it was, what are the temperature conditions,\nwhat are the pressure conditions, was this container\ndropped, where has it been, and ultimately\nlooking into, is it emitting a chemical when you get\ninto Homeland Security issues. And so we developed a tag,\nlooked at common standards, integrated those\nstandards onto a tag. And when you hear\nthe various talks, you're going to hear about\na number of issues of power, range for RFID, durability. And so what we looked at, and\nsome of the interesting things that capitalize, actually, from\nthe computer science world, is how do we network these tags? If we want to keep\nthe power low, that's going to have an\nimplication about how much distance and range we get. Now, these are active tags. So with that, if we can\nimplement kind of a hopping or-- and it's not really\nan ad hoc network, to say, but it looks like it-- where we can hop from\ntag to tag to tag to find the furthest tag away,\nwithout increasing power, without changing anything else,\nthen we can form this network and get a map of where\nall our assets are. And so that's what\nwe ended up doing to get around the power issue. One of my last slides, I'll\nget into future technologies, and some of the things\nthat we need to consider are power scavenging, ways to\nget power from other means. Especially when you're\ndealing with active systems where the power\nis self-contained, you've got to look at\nthat because that's one of the greatest\ntechnology hurdles. And nothing to do\nwith RFID, but it's a technology that has\nlots of research areas. Contact memory\nbuttons are something I pointed out early on. If you want to store asset\ninformation, maintenance history on an item, and\nbe able to gather that without having to\nopen up the container, find the paperwork, all of that\ncan be integrated within your-- earlier there was mention\nabout an architecture. Not only do you have a\nhardware architecture, but an information architecture\nthat you're dealing with. We had a pilot\nstudy, and everything you can think of that\ncould go wrong went wrong. But finding power out in\nthe field is a major issue. Having the ability to\nconnect up to a computer without having to get lots of\npermissions was a major issue. In this particular\ncase, luckily we didn't have any other RF\ninterference conditions. But in a warehouse\nenvironment, what happened in an installation\nof an RFID system, it shut down the\nentire wireless network because they were incompatible. So a lot of things that\nyou've got to look at."}, {"content": "It's not just a slap and\nchip and it's going to work. In our active case, if we had\na forklift or something come in between the containers\nand the tags, and read rate stopped. What we're seeing\non commercial data is that you'll have\nan item that goes from the back stock to the\nstore, and then back again, and back and forth. Well, that's not happening. The accuracy that\nwas discussed earlier is another critical issue. What do you do with the data?"}, {"content": "Extremely important. And if you're not going\nto use the data, then why bother tagging your assets? And so that's another\nelement that, I think, is now getting on the forefront. And there are a lot\nof other, actually, side benefits that came about\nfor this particular project. They were using brand\nnew containers that were unpressurized, and so just\nthe testing of the containers and making sure they're\nprotecting the assets in itself was a side benefit that\ncame out of that project. So a lot of things to consider\nthat we've talked about."}, {"content": "Tag and label\nissue, these things are being discussed by\nvarious standards committees. Please follow those standards."}, {"content": "The guidelines have been\nthought through very carefully. A lot of parameters-- we\ntalked about how far, how fast, how much data, how\nmuch content, memory. Security is another\ninteresting component. And I never thought\nabout somebody walking along the\nstreet with a reader and being able to decide which\ncar they're going to break into based on the contents in the\ntrunk until one of my students brought up that\nas an application or as a problem to solve. And so there are\na lot of things. Privacy, of course,\nis another big one. And other considerations,\nwe'll get into in various talks on collisions. Lots of lessons learned,\nsite surveys, power. The information system\nis very critical. And where I think some\nof the future areas are-- and we've discussed\nthese earlier-- include the various\napplications. The applications are endless-- nanotechnology, power\nsources, packaging, which will be a\nsession tomorrow. How can we embed some of\nthese things in the packaging that you're using\nto ship your assets? And a term that we're using of\nperformance based logistics, or logistical\nprognostics, taking algorithms that we use\nfor predicting failures in equipment, and look at\nthem for predicting failures in a logistics pipeline. These are all things to\nconsider and take a look at."}, {"content": "And lots of work, as Sanjay\nindicated, [INAUDIBLE] [APPLAUSE] STEVE MILES: And\nthen Manfred Aigner, who's the group coordinator\nfor the VLSI and security group at PROACT at the\nUniversity of TU-Graz that has a joint research\nproject with Phillips. And it's actually\n[? Ari Bachtel, ?] who's the head of EPC Global\nEurope, who suggested that we might want to consider some\nof the European experience with encryption in the\nsmart card industry as it might relate to RFID. MANFRED AIGNER: Thank you."}, {"content": "Thank you for the introduction."}, {"content": "So it's a pleasure for me\nto present our results. We are involved in research for\nsecurity tag, reader security on our RFID in, let's\nsay, two or three years, involving from smart card\nimplementations of crypto modules, [? NT ?] stuff. And I'm happy to\nshow you our results. I will talk a little bit of\nour group, what we are doing, and tell you the requirements\nwe defined for our developments, and tell you the problems you\nface as a developer of crypto modules for tag application,\nand show you some of our results we achieved so far. So we are a group of\nabout 50 people doing research on IT security,\nfrom development of crypto algorithms-- Vincent Rijmen, the inventor\nof the Advanced Encryption Standard, is with us-- up to e-government applications. And I am the group\nleader of the group that is doing VLSI, so hardware\nimplementations for crypto. We are doing implementations\nfor smart cards, for embedded\nsystems, accelerator cards for encrypted\nnetworks, [? NT ?] stuff. We are also doing a lot\nof system and chip design. Our major activity at the\nmoment is side-channel analysis, which is a major topic\nin smart card industry. So there are attacks where\nyou measure the current and try to get some\nsecret about the key from the current measurements. This will also be a\ntopic for RFID tags. And we are doing projects\nwith quantum groups. And yeah, this is-- what makes us so\nspecial is probably the strong interaction\nwith the other groups we have at our institute. Especially when\ndeveloping AES, it was very helpful to have Vincent\nRijmen sitting with us together to find out how we can\nserialize the algorithm to comply with the\nrequirements we have in RFID. So we talked a lot\nabout that already."}, {"content": "I will skip that slide because\nSanjay today in the morning all explained this. So there is actually\na need for security. And we say if you put\nsecurity on a tag or on RFID, you should use proper security,\nso lightweight security. In our sense, it's not\nlightweight crypto, so lightweight implementation\nof real crypto, because if you try to\nget in with security in a globalized technology,\nyou will need standardization. And that prevents, of course,\nsecrets in the algorithm. So the standardized\nalgorithms like AES, RSA, ECC, they are approved by experts. So there is some work\nspent in investigations if they are secure. And if you do not\nuse them, you're probable to flaws\nin your systems. And the thing is\nyou should not only look at the algorithms you use. Most [INAUDIBLE] systems get\nbroken because they do not use standardized protocols,\nso most flaws in systems are in protocols. And now I want to\nbring some arguments against and for\nstandardized algorithms. So some people say that if you\nuse standardized algorithms, they are easy to attack\nbecause there are so many publications on attacks. And you do not take into\naccount that your developers of the system. And if you're\ntalking about RFID, there are a lot of developers. They are potentially attackers. The secret is not a secret if\nso many people got involved. And some people\nsay that especially side-channel analysis is more\ndangerous if the algorithms are known and if the RFID tag are\nin the hands of the attackers, and they are in the\nhands of the attackers, I would apply a\nside-channel attack. And if you understand\nside-channel attacks, it's easy to adopt them\nto other algorithms. It's also easier to\nuse them to find out the specifics of\nsecret algorithms. So that's not a question. And some people say that\ncustom-built algorithms use less secure. You lose less resources. And I say that they are\npotentially less secure. If you use them,\nthings might happen. Like, it was\nexactly last year, I think, this break of the\nJohn Hopkins [INAUDIBLE] of the [?"}, {"content": "speed pass. ?]\nAnd we say that proper implementations\nof standardized algorithms are possible for passive\ndevices, for RFID tags. And I will show you\nlater on our modules. So when we started, we\ndefined some requirements for secure tags we\nwanted to develop. And we said right from\nthe start we do not want lightweight crypto. We want real crypto, like the\nsame standard as in smart card industry. We wanted to use\nstandardized algorithms, because the high number\nof tags we will have is enough value that\neven if each tag just protects small value, it's\na good point of attack for an attacker because\nthe high number of tags makes a hack very interesting. But the high number\nof tags logically needs a very clever\nkey management. So we do not have\nsystems like that so far. We didn't want to accept a raise\nof costs, a significant raise of costs, due to our\nsecurity implementations we wanted to have on the tags. And we didn't want to accept a\nreduction in reading distance. So we had to comply with this\nvery small power consumption. And we wanted to be compatible\nwith our already installed infrastructures. So we didn't want to suggest\na system which is never accepted because there are so\nmany readers already out there. And there we\nstarted, and then we were facing a lot of limitations\ndue to the technology. So the main problem is\nthe power consumption. So if we do not accept a\nreduction in the reading distance, we were\nfacing that we have about 10 microamps available. The area consumption,\nwhich is less problem with new semiconductor\ntechnology, but it's still the technology\nthat's now available. We have very limited\nexecution time, in fact, because, well, we\nhave a rather limited clock frequency. So the protocols,\nthat was actually a problem, because\nin the other things, you can say just a client, and\nthe server client just responds or something like that. But here always the protocol is\nalways initiated by the reader, and that's different. And there is no physical\nprotection available possible. And then there were\na lot of publications in the last year\nwhere people state that hash is more inexpensive\nthat the encryption, and that's simply not true. So I do not say that\nthis research is useless, but they should use encryption\nmodules instead of hash models because they are easier\nto implement in hardware. So what are the enhancements\nwe want to propose? For tag identification,\nwe will need an extended personalization. We will need a crypto,\nprimitive, on a tag. And we will need to\nsecure key storage on a tag, which is a problem\nthat is not treated so far. And we need the\ncryptocapability of the reader or a secure access to\na verification server. For read authentication, we\nwill need one additional thing. That's nonce generation."}, {"content": "Nonce a number used once. It's kind of\npseudo-random number. It has to be fresh\nand unpredictable, and that's not an easy\ntask for RFID tags. What are the results so far? In our institute, we\ndeveloped an AES module, which complies with the\nrequirements for a tag. So it uses 4.5 microwatts. That's produced."}, {"content": "That's verified."}, {"content": "That's available. The only thing is there\nis no countermeasures against side-channel\nattacks on this module. That's what we are\nworking on so far."}, {"content": "We presented security\nlayers for ISO 18000. The reason why we've\nchosen this standard is because we were used to the\nISO standards from smart card, and it was, when we\nstarted, clearer to us the concept of defining\nnew comments like this, custom comments, which\nwere foreseen in ISO 18000. In fact, it doesn't\nmake a big difference if we use EPC or ISO. We have protocol security\nlayers for anti-cloning, for privacy enhancement. And this is tested with a\ntag emulator and FPGA basis. And we have isometric\ncrypto modules using elliptic curve\ncryptography, which are usable on passive devices. So what are our future task? We will work on-- we should work, actually, on key\nmanagement and personalization, on testability of crypto tags. Nobody mentioned that so far. This will be a\nproblem in future. So if there is more\nfunctionality in tags, how will you test them? Compliance testing\nis already a topic, but this will be a\nmore interesting topic. We have to deal with this\nnonce generational tag and further research\non isometric crypto. So what are the conclusions? Use standardized crypto if\nyou state that you will. Design secure RFID\nsystems, because you never know what your system\nis used for then. The protection will be also\nnecessary in inexpensive tags in future because you never\nknow the applications, also protection against\nside-channel analysis. And, well, according\nour results so far, I would say the implementation\nof standardized crypto is possible on\npassive devices if you go for a clever implementation. And more research is definitely\nnecessary for integration into running applications,\nto future applications."}, {"content": "This is a list of\nour recent papers. And I just want to mention\nour initiative, PROACT. We are looking for\nresearch, for professorship, and for visiting professors,\nand stronger interaction with the RFID community\nin the next years. And thank you. [APPLAUSE] STEVE MILES: Any questions\nfor this distinguished panel around just the tags\nand the future of tags? If we could ask you to\ncome down to the mics. AUDIENCE: This is\nChris [INAUDIBLE] from the Auto-ID\nLab in Switzerland. I have just a quick\nquestion regarding your work on power consumption reduction. If you look at the works, say,\nof [INAUDIBLE] on transponders, their transponders needed about\n16 microwatts at the antenna and probably about 4\nmicrowatts on the chip before the rectifier. Where do you think your\nwork will take this? Do you think you can\nget significantly, be 1 microwatt, for power\nconsumption of a tag for read access? HAO MIN: Actually,\nyour question is-- you will see that the minimum\npower for the tags, there's IF power. Say it's, like, 16 microwatts."}, {"content": "But the real digital power\nconsumption is, like, a 4 watt. So this means that there's\nonly 20% of the microwave power was converted into DC power. So our work is-- we can get\nthe performance better by doing two things. One is we can improve the\nconversion rate, which is higher efficiency to\nconvert the microwave power into DC power. The second thing is that\nby carefully designing so the digital circuitry would\nreduce the power consumption. So this is a two-way, where each\ngo together to reduce the power consumption. But eventually, what decides\nthe read range is the IF power, decides the whole read range. AUDIENCE: But what do you\nthink this is moving to? We you see that at some\npoint we will be down to, like, 1 microwatts? HAO MIN: I think probably\neven less than 1 microwatts. Yeah. Just like in\n[INAUDIBLE] stations, if we use some\nspecial circuitry, we even can reduce the power\nconsumption by a factor of 10, even 100. We take advantage of that\nthe tag really work in really low operating speed. So for the maximal-- the clock in a gen 2, it's\nonly a 640k, which is much-- even 1,000 times slower\nthan a PC is working. So probably, it's just can\nuse some special circuitry."}, {"content": "You can reduce the power of\nthat to, like, a 100 times. Then [INAUDIBLE]. AUDIENCE: OK. Thanks very much. HAO MIN: OK. STEVE MILES: So\nthank you very much. If we could ask the\nnext panel to come up, in the interest of time."}], "Lec 5 | Special Topics in Supply Chain Management": [{"content": "PROFESSOR: Good morning. I'm often hungry\nfor an audience, so I came here with\neight presentations. So I've condensed\nit down to three, which have been\nmerged here, and I'll try to be through my\nsection in 10 minutes because I have some very other-- very knowledgeable\npeople to speak after me. I was supposed to speak about\nevolution of RFID systems. And I was trying to\ncapture the notion of how these systems evolved\nunder the various constraints that have regulated\nthe revolution and how they might evolve\nunder future research. And there won't be any\ntime to say much about what we're doing at Adelaide. The sorts of topics that I\nthought we might collectively talk about are here,\nsomething on RFID regulations, something about\nantennas, something about propagation and protocols,\nand higher functionality tags. Now, I have other\nspeakers that are very good on some of those issues. So I'm going to talk a\nlittle bit about antennas and also propagation studies. And I'll move to that\nalmost immediately. So talking about\nantenna issues, I'll say a little bit about\nelectromagnetic theory and maybe something about\nhow antennas work, largely through diagrams, talk\nabout near and far fields, and talk about what I think are\nimportant conclusions that you might draw about what you can do\nwith antenna in the near field and the far field. I think there'll be\ntime for me to talk a little bit about the\n[INAUDIBLE] limit on efficiency and maybe show you a couple\nof simple tag designs. I never give a presentation\nwithout showing this slide, which shows an\nencapsulation of Maxwell's equations in the\nsource and vortex interpretation of\nHelmholtz, which I think the more you look at\nit, the more you realize that it contains the secret of life. But it boils down\nto these pictures, which show you the source nature\nof an electric field coming from a charge distribution. Also shows you the\nboundary conditions you have to contend with when it\ngets near a conducting surface. This shows you the vortex nature\nof a magnetic field caused by a current or\ndisplacement current. And again, the\nboundary conditions you have to contend\nwith when that becomes near a magnetic surface. I think this is a picture\nwhich will show you how an electric field\nmight excite an antenna and how a magnetic field\nmight excite an antenna. Unfortunately, these are really\nuseful for small antennas. When the antennas\nget a bit big, it gets to be a bit more\ncomplicated than that. Well, these are the fields\nof a small magnetic dipole. You should only look at the\nred and the blue parts, which show you that there's one field\nthat diminishes rather slowly and is the first\npower of distance, and another field that\ndiminishes rather quickly. That's the third\npower of distance. That shows you that there\nare near fields, which is the blue part, or\nthe far field, which is the red part\nthat we should think about when we're trying\nto design systems to couple to them. We're not going to\ntalk about that slide. This is a glimpse of how\nradar engineers work out power transfer between antennas. And you can see\ntowards the bottom, there's a dependence upon\nwavelength and a dependence upon inverse square\npower of distance. And if you look about that\nand you think about it, you say, well, I'm\ngoing to do best if I have a very long\nwavelength and that means I should be at a low frequency. So the question we should\nask is, why is that not true. And I think you can come to\nan answer on that question by focusing attention\non what happens when you're close to an antenna,\nyou've got stored energy. So I have what I call a near\nfield coupling theory with some of those concepts\nwithin it, but I think these are the\nsignificant conclusions. An antenna can be characterized\nby a coupling volume, not really an effective\narea, and it's proportional to the third\npower of its largest physical dimension. And you can also\ncharacterize it by a quality factor which tells you how\nnarrowband the antenna is. And that, unfortunately,\nthe quality factors are inversely proportional\nto the third power of the distance. So I think that this gives us\na clue as to why we shouldn't always go to low frequencies\nwhen we're designing antennas because the low frequencies\nhave very large betas in the-- have very small betas, actually. So the quality factors go up\nand the bandwidth over which are untenable work\nis impossibly small. This leads you to\nconclusions you can draw about optimum\noperating frequencies. And it's really the lowest\nfrequency in which your antenna will still be efficient,\nand that often happens to be about\nthe UHF region. So that's no\nsurprise, of course, why there's a lot of tags\nworking at the UHF region where range is required. We're also recently interested\nin what the [INAUDIBLE] theorem tells us about over what sort of\nbandwidth you ought to be able to make a UHF tag work, and\nthere's the theorem there in a few slides here about what\nit means in terms of making yourself a bad match over\nfrequencies you're not interested in and a very good\nmatch over frequencies you-- well, as good a match\nas you can manage over frequencies you're\ninterested in. And I think you\nconclude that if you look at the different\nproblems we face, like the USA, which has got one\nbandwidth, the Japanese, which has a smaller bandwidth, but a\ndifferent part of the spectrum, or the European countries,\nwe can ask ourselves, can we do a good match over\nthose frequency ranges. And I think the\nconclusions are, it does depend a bit upon the\ncharacteristics of a circuit. And you're not troubled\nby the theorem, I think, if your microcircuit has\na relatively low impedance of 1,000 ohms in parallel\nwith a picofarad. But once it starts to become\na very low power circuit, it's still about one picofarad\nof input capacitance, but less power\nconsumption, it isn't practical to make the\nantenna work optimally without some inefficiency. So-- but based on\nthose principles, we have designed\nsome small antennas with simple matching circuits. So down at the bottom here,\nyou can see a tag chip, and behind it,\nthere's a capacitance in parallel with it. Up here, there's another\ncapacitance also in parallel with that gap, and that\nprovides a reasonable match between the circuit and\nthe radiation impedance. So that's I think all I\nwant to say about tags, and I think about antennas. And I think Rich\nFletcher will give you something substantially\nmore widespread in that. But I think I've got time to\ntalk a little bit about higher functionality tags. And I think the\ninteresting questions to me seem, can you merge electronic\narticle surveillance and data tags. And I think I'm pretty\nconvinced the answer is not easily for the reasons\nthat, to turn them off, you're going to lower\nthe queue inevitably and you can't get the high\nquality factors that you need in EAS tags\nif you're turning on and off the resonance. But I think we've also become\ninterested in that second topic, turning on\nbattery-operated tags, and I just want to show\nyou some simple results for both low power\nconsumption circuits and what I call zero\npower consumption circuits for those operations. This is what you might do if\nyou have a turn-on circuit down at the bottom right and\nyou have a label antenna, which you want to both\nresonate, because resonance has the desirable properties\nof magnifying voltages. And so you might want to\nresonate your available induced voltage to produce the maximum\nvoltage across the depletion layer capacitance of\na rectifying diode and then use the DC voltage\nto apply a turn-on circuit. There's a couple\nof contexts here. You might want to apply-- get about a volt\nout of the system so you can turn a\ntransistor from desaturation to conduction, or you might want\nto just get about 10 millivolts so you can operate the input\nof a very, very low power consumption amplifier. This shows some\nexperimental work that we were doing to reveal\nthe fact that, if you're working at relatively low powers,\nyou can get a nice resonance curve as you see on the left. But as soon as you start to\nincrease the power levels, the nonlinear\ncapacitance variation with developed voltage of\nthe diode becomes interplay and you end up with that\nkind of resonance curve when the right hand\nside is quite vertical. As it goes off resonance, it\nsuddenly drives itself away in operating frequency. That's the idea of a low power\nconsumption circuit, which will consume about 10\nnano amps and turn on at about a few millivolts. And this is a totally\ndifferent concept in which a vibrating\nmagnetic field might shake a magnet,\nwhich will distort the piezomaterial, which\nwill generate about a volt. The analysis of\nthat involves things like looking at charge, and\ndisplacement, and voltage, and torque on the device. You can relate\nmaterial properties to structural\nproperties if you know the dimensions of the structure,\nand I think eventually produce this expression for\nthe turn-on voltage, which will allow you to work\nout that the concept is feasible at frequencies\nof about 100 kilohertz and magnetic fields of the\nkind that you can use to create a stored energy in the foyer. And the obvious application\nis theft detection. So if I were to try and\nwalk away with this PC and it had such a\ntheft detection tag based on these principles in\nit, it would raise an alarm. So I thank you very much."}, {"content": "I now have pleasure\nin introducing my other colleagues. [APPLAUSE] Yep. AUDIENCE: So our next\nspeaker is Dr. Rich Fletcher, who's a visiting scientist\nhere with the AutoID Labs and was involved back\nat the Media Labs at MIT when Sanjay,\nand Dave Brock, and so on were in the\nbasement over here. Rich was working in\nRFID in the Media Labs, which was a little\nbit more glitzy at the time, I believe. RICH FLETCHER: Yeah. We had more money back then. That's for sure."}, {"content": "All right."}, {"content": "Oh. Hello? OK."}, {"content": "All right. I'm happy to be here\ntoday and give you-- have a chance to\ntell you a little bit about some part of some aspects\nof RFID and some of my work. I am a visiting\nscientist at MIT, but I work also\nwith MIT Media Lab. So some of the slides\nthat I'm going to show you and some of the pictures are\nfrom different projects at MIT as well as some of my\ncompany projects as well. The basic topic or\ntheme of my talk is looking at different\nelectromagnetic issues and how they vary depending\non different frequencies. And there's a lot of slides. I'm just going to go\nthrough it very quickly. But just to give you\nsome sort of flavor of the different RFID\nfrequencies that are out there and some of the issues involved. So as we all know,\nRFID, is the main goal is to send out\nsome sort of signal from the reader to the tag and\nto get some sort of response, either reflected power or\nsome modulation from the tag, to get information or to\nuse this tag as a sensor. And obviously, if there's\nno power or if there's problems with\nelectromagnetics, it affects either the turn-on\nof the tag, or the signaling is corrupted, or you have\nsome other types of errors. I'm going to start\nby just giving you a very brief, fundamental\nintroduction to some of the electromagnetic\neffects that we look at."}, {"content": "Obviously, we have different\nsort of reflections depending on what frequency\nyou're looking at. The signal also\nspreads in space. It's not like a laser that\ngoes in a straight line, so you have spreading loss. And every time you go\nthrough an interface, you get different\ntypes of loss just through the impedance mismatch. There's shielding and\ndetuning of the antennas. At the higher\nfrequency, RFID, you have multipath reflections\nfrom other things in the room, including the floor. When you go through\ndifferent slits, like layers on the\npallet, you also have other types\nof interference. The wave interferes with itself. You probably experience some\nof that with your cell phone. I'm now going to\ntell you a little bit about the different RFID\nfrequencies that exist. Some of these are only used\ntoday for EAS or antitheft applications, so I\ndon't know how familiar you are with those. But I thought it would\nbe interesting just to give you a flavor of that. As we all know, the RFID tags\ncome in many shapes and sizes and they've been\naround since the '70s. At the very low end,\nwe have, at 77 Hertz, which is an extremely\nlow frequency, that's used for library books or\ncertain CDs, antitheft tags, little strips that\nare made by 3M. The advantages of this is\nthat it's a magnetic material. It's a very thin\nfilm, a very low cost. It's good for making\ndifferent types of sensors. The disadvantage is\nthat it's shorter range, and to generate those\nmagnetic fields, you generally need a larger\nantenna and sometimes higher power. That's a picture of what\nsome of those tags look like. I worked in some of these\nareas to make sensor tags. We made item level temperature\nsensors using these materials. Moving up to slightly\nhigher frequency, this is your common 125\nkilohertz sort of tag. It's sort of the\nclassic RFID tag. What's great about it is\nthat it penetrates liquids and other materials\nvery well, so it's used a lot in\nindustrial applications. And has a worldwide frequency. It's pretty easy to find,\nto be able to use it anywhere in the world. The disadvantage is that it's\nalso somewhat shorter range and you need larger\ncoils and antennas. There are magnetic versions that\nwork in this frequency as well. And let's see."}, {"content": "I'll just give you-- I'll play this. This is one of the very early\ndemos that I did at MIT. This was at the\nMedia Lab in 1995. We were looking at using some\nof these magnetic materials as exploring how they could\nbe used for RFID sensors. And there's-- we were looking\nat using it to measure the displacement of a piston,\nand also detect other objects, and also, in this case,\ndetect the squeezing of a toy. So as you squeeze it, you can\nsee the little figure animates over there. So there's a lot you can do\nwith just different types of magnetic materials. This is the more common\nform of these sort of tags. And these are still by far\nthe largest RFID market, even today, used for mostly\naccess control, car mobilizers, cattle tagging. And the cattle tagging\nactually started-- was invented by an MIT here,\nMike Beigel, in the late '70s. Because this works pretty\nwell in proximity to metal and it's pretty\nrobust, we also used it to do some of the early smart\nshelf work back at the Media Lab, this was in\nthe late '90s, where we had to read tags either\nthrough an LCD display or through other\ntypes of materials. Something else which I'll\njust mention briefly. For a brief time while I\nwas at Motorola and also in collaboration\nwith the Media Lab, we developed capacitively\ncoupled tags. Now, most tags work\nwith magnetic fields and inductive\ncoupling with coils. For a brief time, we did develop\ncapacitively coupled tags. And what's nice\nabout this is that it uses electric fields\ninstead of magnetic fields, but what's nice about it is that\nyou can make a printed antenna and it's very\nflexible and robust. It doesn't require soldering. You could rip up the\nantenna and it still works. But unfortunately,\nMotorola, they were losing a lot of\nmoney in the late '90s due to iridium and other\nprojects, so they sold their RFID division. But this technology\nis still out there and it's a pretty\ninteresting technology. Moving up to slightly\nhigher frequencies. HF, for example,\n13.56 megahertz. This frequency-- you need\nfewer antennas on your coil, so you can make\na lower cost tag. You can make it out\nof foil, however, because the penetration\ndepth is still rather thick. You need a thick metal layer. And also, it requires a\ncrossover to do a coil. So it's not as low cost\nas, say, a UHF tag. But it does enable a\nvery low cost reader. You can make extremely low\ncost readers at this frequency. And this is used a lot\nin toy applications. You can buy a reader\nfor a couple of dollars in Hong Kong that's\nused in the toy market. And it's also very\nnice for making different types of\nsensors because the value of inductances and capacitances\nat these frequency range is just right for\ndifferent types of-- for integrating it into a\ntag and for different types of sensors. This is some of--\nanother type of reader that was developed in\nmy lab for exploring how cell phones can be used\nas a reader and also as a tag. So you can transfer data from\none cell phone to another. You can also load up a\nvariety of IDs onto your phone and read it. So this sort of near\nfield applications is what's possible\nand convenient to do at low frequencies. And obviously, another market\nthat's growing very fast is payment. It's growing all over the world. I don't need to really\ntalk about that."}, {"content": "Moving up to higher frequencies. So UHF tags, which\nis obviously what's been getting the most\nattention for supply chain, and this is what's being used in\nEPC, in the EPC world, mostly. It has-- its advantages\nare a very low cost tag. You can now use very\nthin metal conductors. You can make a\nsingle layer antenna, so it brings the\ncost of tag way down. It also has an\nincreased read range. The antennas at this frequency\nhave a larger capture-- a larger cross section, which\nallows you to get a longer range for your reader. But the disadvantages are\nthat there's null spots. Because the wavelengths are\non the order of a meter, you get null spots on\nthat sort of scale. And there are also\nchipless versions of this that work\nfor EAS and I believe Professor Cole was involved\nin an early version of this. The cost of UHF\ntechnology in general has been plummeting,\nwhich is pretty amazing. The tags-- you can now buy tags\nfor less than $0.20 in even low quantities, such as 1,000 tags. The cost of readers,\ndue to the advent of wireless technologies\nin commercial products and consumer electronics,\nthe cost of CMOS radio ICs has been coming\ndown dramatically. And you can now build a reader-- there's a reader I\ndesigned last summer with just a parts cost of $30\nand it could read EPC gen 2 and gen 1 and with a read\nrange of a couple of meters. So moving up to microwave. So microwave, and particular 2.4\ngigahertz or the ISM band, this is very attractive because\nit's a smaller antenna. It's a worldwide frequency. There's many other standards\nand wireless technologies that work there. So you have the\neconomy of scale. So the parts and the antennas\nare already available and you can make RFID\nsystems in this frequency very cheaply as well. But disadvantages, as with UHF,\nis that it's easily shielded. Here's an example of a\nZigBee tag that I designed. And one big advantage here\nis just that, obviously, this is battery powered. However, you can make\na very tiny reader that's very low cost. And the advantage, for\nexample, for a pallet tracking application is\nrather than having a $1,000 reader at\neach portal, you can now have one reader\ncovering 10 dock doors at a fraction of the cost. Then moving up finally to\nthe next frequency range, you have a millimeter\nwave or higher microwaves. And these are very tiny\ndipoles, very tiny antennas. And this is mostly used\ntoday for anticounterfeiting because they can embed\nthe materials into things like passports, or fabrics,\nor other printed media. You could-- because the\nantennas are so small, you can make an\narray of the antennas and you can use-- you can\nmake a phased array so you can steer your beam around. And here's a picture\nof a little reader, and you can see how tiny\nthat antenna array is. There's a 26-- this particular\none is at 26 and 1/2 gigahertz. And what you see\nin the background are some of the printed dipoles. This happens to be printed\non a polyester sheet, but it comes in many\nother forms and it's used for food packaging and\nother things around the world. So in anticounterfeiting\nin currency and so forth. Just to tell you briefly\nof Leena, the next speaker, is going to give you an example\nof some of the work that's being done here in\nelectromagnetics at the AutoID Lab, but I just wanted\nto mention the topics that we generally\nlook at that fall under the category\nof electromagnetics is reader antenna design\nand also tag antenna design. But things like geometry,\nthe materials interaction, and the overall propagation\nbetween the reader and the tag. So one important point\nI'd like to make here is that it's important to\nstandardize all of that, not just the protocol\nand the reader. I'm going to fast forward\nthrough some of this."}, {"content": "Some of the things\nthat we've done is we've built simulation tools. Some of you have\nseen this already. We've built probes\nthat we could use to-- it's a semi active\nor semi passive tag that we can embed inside a\npallet that takes readings of the field, it\nsamples a field, and it talks back to the\nreader using the EPC protocol. So you can sprinkle some of\nthese in with standard tags to give you more information\nabout your reader installation. And we've done a variety of\npropagation studies looking at how the different\nthicknesses, materials, and different properties\nof the material affect the read rates\nand the propagation. And, well, surprise, surprise,\nMaxwell's equations works. And so we've also looked\nat certain geometries for pallet stacking. How we can use\nprinted inks and try to look at low cost\nimplementations, how packaging can be-- the proper packaging\ndesign can be used to improve the read rates. I could tell you more detail\nin person if you're interested. And finally, we've looked at\nhow the evolution of packaging over time, how, starting\nwith slap and ship, you can be smarter about the way\nyou place the tags and the way you fabricate your boxes. Maybe eventually, if\nwe get to the point where the tags are actually\nembedded into the cardboard boxes, we can vastly\nimprove the read rates. And your ROI level depends\non your application and depends on the\nparticular company. So in conclusion, as Sanjay\nand everybody else has said, there's a lot of work to do. But I just wanted to\nmention that there's much more to do than just\nprotocol and tag IC design. And there's also--\nthe EPC sort of RFID is just a very small slice of\nthe potential RFID technologies and frequencies that\nare available out there. So there's a lot that\nwe can look at as well."}, {"content": "I think I'll end there\nand we'll move on."}, {"content": "Thank you."}, {"content": "[APPLAUSE] STEPHEN GRAVES: Thank you, Rich. So our next speaker-- we\nthought since this conference was about academic\ncollaboration that we might share with you the\nresults of a collaboration between Tampere\nUniversity of Technology and the Rama Research\nUnit and the Auto-ID Labs. And Leena Ukkonen is\nhere to share with us some specific work in\nthe antenna-design area that she's worked on over\nthe course of the last year between the two institutions. LEENA UKKONEN: So good\nafternoon, everyone, and it's great to be here today talking\nabout the research on antenna designs. And we have been collaborating\nwith the MIT IDEA Lab since 2001. And our collaboration began\nthrough the Auto-ID Center Ergonomic Alliance. And, well, the\nmain research focus on RFID at our\nresource institute in Tampere University of\nTechnology Rama Research Unit is tag antennas. And I was here two years ago as\na visiting PhD student working on tag antennas for challenging\nobjects like objects containing metals and liquids. And this collaboration\nhas been continuing also after when I went\nback to Finland, and it's been very\nfruitful and great to work with the MIT Auto-ID Labs. And about today's presentation-- well, since the main\nresearch focus of our lab is tag antennas, I'm talking\nabout omnidirectional tag antenna for passive UHF\nRFID of paper reels. And this has been our\nmain research topic at Rama Research Unit in 2005. And first I'm going\nto tell something about the challenges in applying\npassive UHF RFID in paper industry because\nat the moment there is an urgent need in paper\nindustry for an identification system that would carry\non the identification code throughout the whole\nsupply chain of the reel because at the moment when all\nthe bar-code systems are used, they are placed on the surface\nof the reel, on the wrapping, and all the\nidentification is removed when the wrapping is removed. And then you cannot know anymore\nlike the origin of the reel and that kind of things,\nwhich would be important-- for example, printing companies. So that's why in\nour approach we are placing a tag on the paper reel\ncore under the wrapped paper. And that has a lot of effects\non the RFID system performance and the tag-antenna\nperformance that have to be taken into account. And, well, of course\nbecause we are operating in the industrial\nenvironment, there is all kind of\nbackground noise that has to be taken into account,\nand also the environment can be kind of hard-- for\nexample, cold environments and such. And of course there are a lot\nof different paper qualities and cardboard, and\nwe would like to have a tag that would function with\nall of those different paper qualities and also\nwith cardboard. But the biggest\nchallenge so far has been developing an\nomnidirectional tag antenna which is indisplaceable-- for example, in\nlift-truck handling. Because as you can see\nfrom here, the guy who's driving this truck,\nhe just grabs the reel, and the\nidentification, which is carried out\nusing a radar unit that would be integrated\ninto this truck, it has to be automatic\nso the driver doesn't have to look for any\ndirection where the tag is. It has to be able to\nbe read omnidirectional around the reel. And, of course, in general\nin paper-mill environment, if you just can identify the\nantenna with only one reader antenna, that would\nbe also good anyway. Well, next there is\nsomething about this concept of omnidirectional\nreading and also of the structure\nof this paper reel. So here you can see the\nvertically orientated reel. And first there is\na reel core which is fabricated of hard cardboard. And then the tag is\nplaced on the core, and then the paper is\nwrapped around the core. And typically these\nthicknesses of the paper layer that's wrapped around the core\nvaries between 500 and 600 millimeters. And also the length\nof the reel can vary, or it varies from something\nlike 300 millimeters up to 2.5 meters. And, well, the\nomnidirectional reading means that the paper\nreel or the tag can be identified\nomnidirectionally 360 degrees around the reel. So you don't have to\ncare where the tag has been placed on this core. So you can read a\ntag around the reel. And, well, in this tag-creation\nprocess, there are many steps, and I'm going to\nbriefly describe them. So first there is modeling. This picture has been taken from\nthe simulation software that is based on finite element method. And you can see that is a\nvery real-life structure. So there's the reel. The tag is placed on\nthe core, the reel core. And then the paper is\nwrapped around the core. So it's like a real\nindustrial paper reel. And here are some\nradiation patterns of those tag-antenna\nmodels that we've been modeling during this project. So those pictures on the\nleft are some earlier stages of modeling. So because we try different\nantenna geometries and tried changing some\nthings and some geometries on those designs affects\nthe radiation pattern. And the bigger\none on the left is the stage we are at the moment. So you can see that the\ntag antenna radiates into all directions\naround the reel. And, well, then there is\nalso, of course, measurements. And these pictures here are just\nsome basic measurement setups with network analyzer to see\nhow adding paper on a tag affects, for example,\nresonance frequency. And, well, because our\ngoal was to develop an omnidirectional\ntag antenna, we developed an\nomnidirectional model with which we can test the\nomnidirectional reading of the tags because when\nwe go to the paper mill, we want to have as\ngood tags as possible. So we developed\nthis model, and we could test the\nomnidirectional reading from all the directions. So we have 16 measurement points\nand four different distances. And if the tag was identified\nat all the directions at all those distances, it\nwas omnidirectional also inside a real paper reel. And here is the\nomnidirectional tag antenna, which we call the\nC tag, because when it's wrapped around this\ncore, it formed a shape of C. And here you can see\nthe antenna design flat. And, well, here it is in\nthe paper-mill environment mounted on the core before the\npaper is wrapped around it. And we did a lot of\npractical testing with this antenna\ndesign, and here you can see our\nmeasurement equipment. And we used Alien Technologies\nEuropean reader unit, Alien Technology straps\nas a microchip. And the reader was based\non new ETSI regulations, and it had two-watt\nERP transmitting power. And here are some pictures\nfrom the measurements. So this is basically how the\nread ranges were measured in a paper mill, and we\ncould move the reader units and the reader antenna\nand roll the reel on the floor so that we could measure\nthe omnidirectional reading. And here is some more\npictures, and here you can see how the tag\nis placed on the core. And we didn't actually kill\nany tags on this process, so that was kind of\ngood news that it went through the process. And, well, now I'm moving on\nto the measurement results. So we measured in the paper\nmill coated printing paper with reel diameters varying\nfrom 1,200 to 1,300 millimeters. And here is the data\nof the read ranges that were achieved\naround the reel. So here you can see that\nit's read omnidirectionally. And the read ranges\nhave some variation around the reel, which was also\nexpected from the simulation results. And, well, I'm going to\nbriefly tell something about identification\nof cardboard reels. And, well, they said\nthat it's been impossible with conventional tags. So we tested our tag also\nfor cardboard identification. And there is some\nmore challenges with these cardboard\nreels, which are larger diameter and also the\nmore layered and inhomogeneous structure of the\ncardboard, which increases the boundary-length effect. So we did some\npractical testing, and the tag antenna\nwas the same that we used with the paper reel. So it was not yet optimized\nfor the cardboard. So the goal of the\nfirst testing was to identify the tag through\nthe cardboard reel, which we did, as you can see here. And also, yeah, kind of the\nmost surprising and good result was that we could identify the\nreel 180 degrees around it. So we achieved better results. That was like the goal\nof these first testing."}, {"content": "Well, there is still a\nlot of work to do on this. But to our best\nknowledge, this is the first omnidirectional tag\nantenna for passive UHF RFID paper reels that can be\nread omnidirectionally with standardized\nRFID equipment. And it has been tested with\ncopy paper and coated printing paper, and also\nthe cardboard has been tested with 180 degree\nidentification around the reel. So in the future, we will\ndevelop an omnidirectional tag antenna also for\ncardboard reels. And we will test and\ndevelop the antenna also for American and\nAsian UHF RFID bands so that we could achieve\na global tag that could be used around the world. And also longer read\nranges will be achieved. So we've talked about\nthis with industry people, and they say that a minimum\nof 0.5 meters from the paper surface would be required. And also the tag\nhas to be evaluated in harsh environments-- for\nexample, in cold temperatures. And, well, there is some other\nresearch project also in 2006. So we'll continue also\ndeveloping the tag antenna for metallic- and\nliquid-containing objects. So basically we will continue\non the miniaturization of the [INAUDIBLE]\nbadge-type tag. So thank you for your attention."}, {"content": "Thank you. [APPLAUSE] STEPHEN GRAVES:\nThank you, Leena. And our final speaker on this\npanel is Dr. Alan Levesque."}, {"content": "He's a colleague of\nDr. Kaveh Pahlavan at the Center for Wireless\nInformation Network Studies. Kaveh was, I think, a chair and\nan IEEE Wi-Fi committee at one point or something, but-- ALAN LEVESQUE: Yes. Actually Kaveh's been\ninvolved in wireless issues since the early days\nof wireless LANs. Those of you that know the\nbusiness at all in the greater Boston area know that\nabout 15 years ago-- I like that slide."}, {"content": "Someone said hope to\nhype to implementation. I like that. About 15 years ago was the\nhype phase of wireless LANs, and a lot of that activity was\na number of startup companies actually in Massachusetts. And my colleague,\nKaveh Pahlavan, was actually involved with\nseveral of those startups. So that's a good-- STEPHEN GRAVES: But\nin any case, they're doing some very interesting\nwork in location-based tracking that we thought would\ncomplement this session nicely. ALAN LEVESQUE: Exactly."}, {"content": "Thank you, Steve. Steve's provided-- I\nshould do that, yes. Thank you, Steve. Steve has provided a\nnice introduction for me. How do we move to the\nnext presentation? Good."}, {"content": "Thank you. Very good. I'm painfully aware\nthat I'm the one who's keeping us all from\nlunch now, so I'm going to do some real-time\nediting as I go along here. Kaveh, in fact,\nintended to be here, but he has a\ncommitment in Japan. So based on the weather report\nthat Steve gave this morning about Japan, he may have\nswapped a snowstorm in Cambridge for a snowstorm in Japan. I'm not sure. As Steve said, our emphasis\nin the last few years in the wireless center at WPI\nhas been on location sensing. We also use the\nterm localization. And because of some of\nthe previous presentations and obviously the background\nof knowledge that many of you will have, I'll be able\nto skip over some of this. We have actually been\nfocused in recent years primarily on public safety\nand military applications, partly sponsored by DARPA, by\nNSF, and with some membership subscription-type\nsponsorship from member companies in the center. I'll leave this up long\nenough to point out that several people made\nmention of the history of this technology going back\n50 years to World War II, and everyone has a little\nbit different take on it. My take is the fact\nthat that era introduced the use of what are called\ntechnically net broadcast radios, push-to-talk radios. The devices were\nso-called walkie talkies about the weight of a\nbrick and about the volume of two or three bricks. And that was the beginning\nof really radio networking. And that certainly provided\nefficient communications for soldiers in the\nfield, but immediately it was recognized that that\ndid not give you information about where the soldier was. Push-to-talk radio, the\nspeaker gets on the net by pressing the button, and\neveryone else is in listen mode and hears the speaker, but\nwe don't know where he is. All we know is that we\nhear his voice signal. And those radios, just to\nset a historical background, used analog voice over\nanalog frequency modulation. And those of you that work\nin the communications field know that analog FM has a\nthreshold characteristic. The received voice is either\nvery good or it's very bad. And it has a threshold\ncharacteristic, and you don't know\nreally anything about what the received signal\nstrength is or the received signal-to-noise ratio is. So in these 50 years,\nwe've come a long way from that primitive technology. I'm not going to go\nthrough all of this, but I want to-- halfway down, I\nwant to mention the era of 1997 when the interest began in\nurban and indoor geolocation. DARPA had a program that was\ncalled Small Unit Operations Situational Awareness Systems. And situational\nawareness basically says how do you find\nthe warfighter that is in a hostile\nphysical situation? How do you locate him\nand communicate with him? We had a piece of\nthe research work in that project\naddressing specifically the radio-propagation problems\nin the indoor environment. And the reason I mention it\nis because the objectives-- the government's-- the\ncustomer's objectives for that project simply were not met. And the fundamental reason\nwas the complexities of radio-wave propagation\nin the indoor environment, and that's what killed it. You could wrap all\nof the software that you wished around that\nand all the user interfaces that were all very nice, but\nbecause of the characteristics of indoor radio\npropagation, you could not get a precise fix on the\nlocation of a warfighter in many of those hostile-- urban fighting is\nthe obvious scenario. About that time\nthere began to be some commercial developments. Pinpoint evolved into\nanother company name. I don't quite recall. Pinpoint/Wearnet came out\nof the body LAN technology which was also sponsored\nby DoD, the concept to embed sensors\nand communication devices into the uniforms\nof service people and be able to use those as part\nof accomplishing the mission. We'll just skip over the rest."}, {"content": "Talk a little bit-- won't say very much about this\nbecause so much, obviously, has been said, and you\nfolks are all well aware of concepts of asset tracking. Putting tracking golf balls\nin there, that was my idea. I figured I could save\nmyself some money if I could find all those golf balls\nthat I'm losing in the woods all the time. Actually someone does make a\ngolf ball with a little radio transmitter in it, and I'm\ngoing to try it one of these-- but it doesn't have\nID characteristics. So there's another\nresearch area."}, {"content": "OK, try to move on. Again, I think I'm\npreaching to the choir here, use an old slide, but\neverything is a terminal today. From communications\nnetworking point of view, we don't necessarily\ncare too much about what the device does. Either it's a terminal out\nat the edge of the network or it's an intermediate node\nsomewhere within the network, and perhaps it serves both\nfunctions in some situations. This great variety\nof applications that keeps growing, of\ncourse, has fostered support for standardization,\nand other folks earlier have talked at some\nlength about standards and the importance of standards\nfor making an industry segment grow. And this is just our own\nway of characterizing some of these standards,\nboth ultra wideband and lower-frequency\ntechnologies are being looked at for Wireless\nPersonal Area, WPAN, one of our areas of interest,\nWPANs, Wireless Personal Area Networks. And, of course, the\nIEEE 802.11 initiative really created the renaissance\nfor the wireless LAN industry. Obviously value\nin standardization and the ubiquitous use of 802.11\ndevices, so-called Wi-Fi-- of course, that's just a label\nfor a certification process. But 802, the promulgation of\nthose devices and the economies of scale that have pushed\nthe prices down, of course, to make that an important\nelement to be looked at in location estimation. Here again this kind\nof figure lots of folks use, and it just characterizes\nthe different technologies, cellular technologies,\nwireless LAN and wireless PANs, against the dimensions\nof scale that are relevant to those technologies. Let me get to the areas\nof interest that have been motivating some of our work. Navigation for fireman-- a\nprofessor early in the morning had a very good\nexample, I thought, about a hypothetical case\nof a fire in the building. We all have tags, and he\nspoke about the issue. Well, suppose\nthere's a miscount. About seven years\nago in the city of Worcester in\ncentral Massachusetts there was a very bad\nand deadly fire in which an unused building caught fire. And when the fire\ncompany arrived on scene, a local businessman\ncame out and said that he saw two people\nrunning into the building. And long story short, the\nfire captain on the scene sent close to 20 of his\nfirefighters into the building, and six of them\ngot completely lost because of the smoke\nin the building and the structure\nof the building-- several floors, a number of\nsmall rooms in the building. And six of them got lost,\nand they died in that fire. It was a very tragic event. And after that was\nall over, it was discovered that the\ninformation was incorrect and there was no\none in the building. And so six firefighters\nlost their lives putting out the fire\nin an empty building. We at WPI actually have now\na funded project from the US government through\nSenator Kennedy's office, and we are looking at the\nuse of wireless technology to try to deal with the problem\nof tracking firefighters in such dangerous situations. One could also call to mind\nthe recent tragedy at the Sago Mines in West Virginia,\nand there is already public discussion of how\nvarious technologies, including wireless technology, might have\nbeen used to say let's move on. This is the kind of concept\nthat the warfighters or the firefighters\nwould like to have. They would like to\nhave a display-- we call it a\ntactical display that would present some\nkind of representation of a building, for\nexample, and be able to locate\nfirefighters or warfighters within that building. This refers to small unit. I spoke about that, the\nsituational awareness that DARPA was interested in. This I think is important. The current DoD interest is in\nusing signals of opportunity to be able to accomplish-- to take advantage of\nwhatever is out there in the ether, whatever\nfrequency bands are available. And so that is a current area\nof interest for us as well. Other interesting\nresearch problems, location-based handoff,\nlocation-based routing, and ad hoc networks. And, of course, on\nthat earlier list there is at least one company-- Newberry, I believe--\nthat is in the business of location-based authentication\nand security technology. There's been a lot of discussion\nabout security issues earlier. So I'll finish up by\ntalking about the two categories of approaches for\ndoing location estimation, and one is received signal\nstrength, of course, which is used in cellular networks. The second-generation CDMA\nnetworks, for example, already use received\nsignal strength estimation. Advantage-- the\nhardware is simple, and that approach is not\nparticularly sensitive. I should say the accuracy is\nnot sensitive to the multipath and/or bandwidth. It does not require\nsynchronization because it's incoherent. It's incoherent\nsignal processing. However, it, in\nmost cases, will not provide the accuracy that's\nrequired, for example, for some of these public-safety\napplications. Suppose, for example, you could\nachieve a location accuracy of one foot, and you're\ntrying to locate a firefighter inside a smoke-filled building. Well, you may have spotted his\nlocation to within one foot, but you don't know if he's\non this side of the wall or this side of the wall. So if your algorithm\nsays he's over here and he's really\nover here and you have some kind of a\nsystem that supports this to try to help him find\nhis way out of the building, he's in the wrong room. So one foot of accuracy\nmay seem very precise, but in that kind\nof an application, it's not precise enough. Just an example. To achieve greater\naccuracy, you can resort to time-of-arrival techniques. And fundamental\ntime-of-arrival techniques are not particularly\nnew, but making them work in a\nmultipath environment is a very difficult problem. An advantage is that\nif you can do it, you can accomplish rather\naccurate positioning with only a few\nreference points, and it also doesn't\nneed training. The problem is that while\nit doesn't need training, what it does need is\nsynchronous operation. So in communications terms, you\nhave to build a coherent signal processing system. That adds to complexity. And you also need a\nsynchronization process to do that."}, {"content": "Let's move along. Just say briefly\ntwo general classes of time-of-arrival algorithms. One is distance-based\nlocalization with a few reference points. And the other, perhaps\na more general way of thinking about it, is a\npattern-recognition approach where you deploy many reference\npoints on a regular grid, and then you can use a\nvariety of pattern-recognition techniques to be able to\nget an accurate estimate. Getting to the end here-- a good technique in the\npattern-recognition branch-- let me call it that-- is the\nnearest-neighbor algorithm. And I should mention Ekahau\nthat is in the RF tag-- ID tag business. And they also have a\nvery now highly developed software product which the\nEkahau positioning algorithm, and that's very recent. And so that represents the state\nof the art with that approach. Our work in the center\nhas been focused on developing an\nextensive laboratory test bed with instrumentation--\nmeasurement instrumentation, channel simulation,\ninstrumentation, and focusing on the evaluation. This gives us the\ncapability to evaluate a variety of\nlocalisation algorithms under a wide range of realistic\npropagation in environments. I'll just move ahead. Coverage, of course,\nand range reading is a topic that's been discussed\nby a few speakers already. Bandwidth-- it's a common belief\nthat increasing the bandwidth steadily increases\nthe resolution, and therefore ultra wideband\nis the right solution. The problem is if you\ngo up in bandwidth, you reduce the coverage. So there's a\ntrade-off issue there, and we don't see ultra wideband\nas the optimal solution. The last topic is\nvery important one. UDP refers to\nUndirected Direct Path. It sounds like an\noxymoron, but you visualize a transmitter, a receiver."}, {"content": "You're inside a building. And in many instances,\nthe line of sight path from the transmitter to the\nreceiver is not detectable, and all of your energy is coming\nfrom the multipath components. And what we're finding\nat this point that is very often the Achilles\nheel for time of arrival based positioning estimation system. And that, in fact, was\nthe central problem that caused the failure\nto meet the objectives in the [INAUDIBLE] program\nseveral years back. So we continue to\nfocus our research now on algorithms that will\nallow us to operate in a condition of\nundetected direct path, and that very often\nmeans making use most of the time of the\nmultipath components. And we're looking at techniques\nlike tracking, which works fine if the transmitter is mobile. However, that doesn't work if\nthe transmitter is not mobile. If it's mobile, you\ncan do tracking, and you can work with\nthe multipath components and the direct path, which will\noccasionally appear in a time record of measurements. We're also looking at use\nof diversity techniques."}, {"content": "And some of that is being done\nin cooperation with Draper Lab. And I realize we're\nrunning out of time. So just beating the drum\nand saying that localization is still an important\nresearch area, and we regard it as\nan unsolved problem. Thank you very much."}, {"content": "[APPLAUSE]"}], "Lec 3 | Special Topics in Supply Chain Management": [{"content": "STEVE MILES: So\nwe will move right into the first of the\ntechnology sessions now, which is focused\non the network, the Internet of Things. And our first speaker\nis Steve Bratt, who is the CEO of the WC3,\nwhich is also based here at MIT. STEVEN BRATT: Thanks\nvery much, Steve. It's going to be\ndifficult to follow that wonderful talk by Sanjay. And I only have about\n10 minutes to tell you about a lot of cool things that\nare going on at W3C right now. And so I'm going to be\ndoing it a very light touch, just hopefully going to\ntweak your interest enough to come and ask\nmore questions, come and talk to the W3C about\nsome of the work we're doing. And the title of\nthe talk is RFID, Things on the Web of\nData and Services. And as a slight contrast to an\nidea of an Internet of Things, it's things on the internet. And that's the way\nI like to get people to think about the use of Web\ntechnologies as you go ahead and address the very complex\nresearch problems that Sanjay laid out before you. Oh, and by the way, the\ntalk is also linked. It's on the Web. No surprise. And it's linked\nfrom our homepage. If you don't remember the URI\nhere, go to our home page. There's a public presentations\nlink right from the home page. And you'll find my talk\nhopefully pretty easily. So one slide about\nthe W3C itself. How many people here have never\nheard of the World Wide Web Consortium? Don't be embarrassed\nif you haven't. So anyway, World\nWide Web Consortium was founded by Tim\nBerners-Lee just 11 years ago now in December in 1994,\nstarted here at MIT. We now have 18 offices\naround the world. We have about 400 members,\ncorporate, university, organizational. We have also a vision to-- a mission to provide a vision\nfor the development of Web technologies into the future. And also, we engineer\nthe standards that make the Web work. And I'll tell you more about\na couple of those later. But the Web really\nhas changed a lot from the time Tim first\ninvented this thing. And it's really become\na really integral part of our fundamental\ninfrastructure. I thought one of the good\nquestions that was brought up this morning, I\nthink by John, was you need to engineer an\ninfrastructure that's going to last not for\na year or two years, but maybe for 100 years. And an interesting question to\nask is, is the Web like that? Is the Web the next telephone? Is the Web technology\nso pervasive now and so good that it's going to be\nwith us for a long time? And that's going to be\nan integral question to ask yourselves\nand whether you want to gamble on\nusing Web technologies as a foundation for your\ndata infrastructure or not. So we'll talk\nmainly about the Web as it's moving from\na Web of documents to a Web of data and services. I'll focus on that. But there's also a number of\nother things we're working on at W3C, of moving Web\ntechnologies to everything, including phones,\nand cars, and planes, and other kinds of places,\nand a Web for everyone. As we expand to a\ndeveloping world, we're going to be taking a\ngreater and greater advantage of the benefits that the Web\nhas brought the Western world up till now. So one thing I think\nis clear, to me anyhow, is that the Web really increased\nthe usefulness of the internet for everyone. The internet had been\naround for a long time. People used it,\nacademics largely and government and so on. Web technologies, through\nsome very simple concepts that Tim instituted and pulled\nfrom many parts of the computer science world, HTTP as a\nprotocol, but more importantly the use of uniform resource\nidentifiers, or URIs, or IRIs, whatever you\nwant to call them, URLs as unique identifiers and\nas unique identifiers of where something is and a\nunique identifier to distinguish that\nitem from other things. Very simple concept that's\ncritical for the Web and really critical for you\nguys, of course. And then also HTML\nis a simple way to link things,\nto link documents to other documents or parts of\ndocuments to other documents. That's it, really simple. And just with those\nsimple concepts, we really-- and standardizing\nthese in the global way really revolutionize the way that\nwe can identify and find retrieved documents of all kinds\nand sorts and around the world. So if the aim is to put\nRFID things on the internet, at least I hope the group-- and there are a lot of research\nproblems in this in itself-- will carefully look at\nthe existing and emerging Web technologies before going\noff and inventing something new. I think that's a logical\nthing you should do."}, {"content": "And I hope that what\nI tell you today will motivate you\nmore to do that. So the new work\nthat's going on at W3C in the area of Web services and\nin the area of the Semantic Web really are going to expand the\nWeb to include more machine understandable resources\nof a variety of kinds. That's really important. And one thing that\nit's going to do is it's going to enable you to\nhave greater interoperability amongst the things that you\nknow you want to link together. So it makes sense for\nyou to go off and develop schemas and other\nkinds of things that make sense in your community. And that'll probably work\nfor the group of people you've brought together\nat that particular time and in that particular place. But if the goal is to\nperhaps be able to enable linking to things you\nhaven't foreseen-- and that's really what made\nthe Web powerful, isn't it? People didn't really\nappreciate all the things you would be doing with the\nWeb today when Tim first invented it 15 years ago. But I think one of the\nkeys is providing-- using these globally\ninteroperable standards will help you to link between\nresources you have not yet decided you're going\nto want to link. So two areas of emerging\nstandards-- and I'm only going to have time\nto talk about one today. One is the Semantic Web. And a very simple\nelevator or bumper sticker is that that's\nreally a Web of data. So it's really providing\ninteroperability at a data level and allow you to\nbetter understand, search for, share, reuse, aggregate\ndata on the Web. And then there's Web services,\nwhich probably more of you are familiar with. So I'm not going to\nspend any time on that. But I've got a lot\nof extra slides at the end of this\npresentation really to enable a Web of\ninteroperable programs. So it's basically\nproviding some kind of-- if you want to think\nabout it in a simplistic way, it's APIs to build the\naccess, expose applications that you have, and be able\nto provide access to those. We won't talk about\nthat much more today. So in terms of the Semantic\nWeb, there's also-- like everybody's got a stack. And the Semantic\nWeb really starts about where this RDF core red\nbrick is there in the stack. But it's based firmly\non the use of URIs. And that's really\nimportant because URIs become the fundamental way to\nidentify a unique identifier for anything in the\nSemantic Web world, just like for you guys RFID tags\nor RFID numbers are critical. There's no reason an RFID number\ncan't be made into it a URI. It's really simple. And then also,\nthere's other kinds of standards that are\nin the stack which enable you to express\nrelationships between data. And we'll talk about\nthat one later. That's RDF. Also, a number of other\nstandards here, too. Another key will be OWL."}, {"content": "You'll see the OWL thing there. It's a Web Ontology\nLanguage to be able to set up a description\nof some domain of knowledge, maybe a domain\nrelated specifically to RFID or specifically\nto consumer goods or whatever it is, and then\nalso other kinds of standards that are being developed now. Now, those other ones\nI've mentioned already are already standards. But there's new\nones being developed to be able to do queries,\nput rules on the Web, logic and trust, security things,\nand other kinds of activities. And I only have\ntwo minutes left. OK, so RDF, the Resource\nDescription Framework, simplify all the\nwords on this page. Whereas HTML provided an ability\nto link documents to documents, RDF provides an ability\nto link data to data. And the links are\nnot just saying, this links, but an explanation\nof how that one thing relates to another thing. So subject, property, value. So for instance, I could say\nthat my talk, the subject might be talk has\npresenter Steve Bratt. And every one of those\nitems itself has a URI. So the talk has a\nURI, the concept of has a presenter has a\nURI, and I have a URI. So all those things are\nnot only linked together, but they're on the\nWeb and accessible, as highly accessible as\nyou'd like them to be. So the simple picture is\nthat this is the current Web. It's all gray and boring, but\nbasically extremely powerful. Being able to link one\nresource to another, we found how powerful it is. The Web of the future, we can\nnot only link concepts together but also say how\nthey are linked. So Jane Doe is\nattending a meeting. It's not just Jane Doe's\nlinked to this meeting. There's a simple relationship. The linkage is that\nshe is attending it. And perhaps the meeting\nsays she's also the chair. And there's also information\nabout her personal data, information about the location\nof the meeting and so on. All that information\nis contained using standards to syntax. And also, ontology is\ndeveloped by domain experts. So in this case, it may be\nan international organization of meetings that sets up an\nontology for what meetings are all about. In your field, you can\nimagine, getting together the diverse kinds of people you\nwant to be able to use RFID, you're never going\nto get them all to agree on a single ontology\nfor describing everything that's related to RFID. So another idea is to do a\nmore ground up, a bottom up approach to this. We allow communities of people\nwho already have places they meet, more generally\nlikely to agree on things, to come to agreement\non their ontologies, and then expose those in RDF,\nand be able to more easily link them than you can do now. So I'll show two more\npictures, and then I'll have to skip the last couple. The simple enterprise\nintegration facts today is that between things\nlike stock control, and parts, and issue tracking, and\nhuman resource management, these organizations, these\nconcepts, and these domains all need to talk to each other. And you still have a\nlot of big business out there and trying\nto be able to integrate these very diverse\nkinds of communities together within even\na single enterprise, not to mention\nacross enterprises. And the idea behind\nusing the Semantic Web is you create a bus\nbasically, a bus that would allow you to use,\nbased on the standards in that arrow there, RDF,\nResource Description Framework, OWL, HTTP, and other\nkinds of standards, to be able to access a set\nof all information and data that are exposed\nusing a common syntax, and then operate on those\nthings with things like the rule standards that are\nbeing developed, the query standards that are\nbeing developed and so on. So last slide, there's\na number of links on here to some\nresearch efforts that are going on now in\nthe Semantic Web. Many of them do apply to some\nof the domains represented by people in this room. And I encourage you to look\nthrough some of these links. Come talk to me. Come talk to us at the W3C. And we'd be happy to\nhelp you get connected with other people who\nmay be able to help you in the research\nthat you're doing."}, {"content": "Thank you very much. [APPLAUSE] STEVE MILES: So Bernie Hogan\nis the CTO of EPCglobal. BERNIE HOGAN: OK, Steve? GUEST SPEAKER: Presenting. BERNIE HOGAN: Good morning. I, too, would also like to thank\nJohn and Stephen for today, inviting us here today. Chris Adcock,\nPresident of EPCglobal, wanted to be here today. But due to scheduling conflicts,\nhe was unable to attend. But I'd just like to give a\nlittle bit different definition of the EPCglobal network. I was here to talk about it. And really, this is what I've\nlearned over four years, what EPCglobal is about. Thank you. And my view of the\nworld is that EPCglobal is about a network or a\ncommunity of trading partners that want to share EPC read\nevents in a standardized way to improve their\nbusiness processes. The technology is important,\nbut it's an enabler."}, {"content": "And if I've learned\nanything over four years-- I was looking back\nover it in order to prepare this presentation,\nover the past four years, and I went back\ninto the archives. And those of you that\nare familiar with Tom Sawyer and Huckleberry Finn,\nOK, well, Sanjay was Tom Sawyer, and I was Huckleberry Finn. And I would come up on\na regular basis in 2002. And we would sit in\nStarbucks Coffee shops and drink lots of lattes. And we would draw on napkins. And he was convincing\nme how much fun he was having painting the fence. And so I bit into it. And I felt like at\nright about this time here, that I grabbed\nhold of a bullet train, because there was such\ntremendous momentum coming out of the AUTO-ID Center. And we had no idea what\nwas in store for us. Coming from the GS1,\nand EAN, and UCC, we wanted to look at the next\ngeneration of identification. But we just didn't know it\nwas going to be at this scale. So these are some\nslides that I pulled that were presented\nto the AUTO-ID Center Board of Governors. And this was the transition\nplan from the AUTO-ID Center. There was an entity\ncalled AUTO-ID, Inc. at one point in time. Everyone kind of seems\nto have forgotten today. And we're going to migrate to\nthis thing called EPCglobal with linkage to AUTO-ID Labs. This was the plan."}, {"content": "We didn't deviate too\nfar from the plan. Phase one, Sanjay's\nmessage to me was, we've taken this phase\nof the research just about as far as we possibly can. We need to get this\ninto the real world. We need to get standards. We need to get companies\nutilizing this technology to see what works,\nwhat does not work, and where to take the next\niteration of research. And only through\nreal implementation, we'll be able to identify and\ncorrect the system's issues. So this was the plan. Now, you have to\nremember back then, UCC and EAN, we were two\nseparate organizations, not tightly linked. And there was a long\nhistory of constant, for lack of a better word,\nbattles between the two organizations, and\nphilosophical issues on how things should be done. So UCC and EAN,\nthe agreement was that UCC would lead the\ncharge and negotiate with MIT on behalf of this thing. But MIT was insistent\nthat this be truly global, not just a US initiative,\nbut also multi-sectoral. They did not want\nthis to go into-- just be buried into retail, be the\nnext generation of the UPC. So these were some very high\nchallenges for us to reach. And so we worked out the\nroles and responsibility. I don't know if you remember\nthese slides, Sanjay. But we worked out the\nroles and responsibilities, who does what. Well, MIT wanted to continue\ndoing research, development of new technology,\npublications, and also outreach for government\nfunding and working together on a collaborative way. UCC and EAN, we were going\nto do what we did best, our core competencies,\nadministration of the systems, commercialization--\nkey point, OK, we were going to bring us\nto the commercial world-- manage intellectual property\nif it is manageable, marketing communications, standards\ndevelopment, and training and education. So this is what we laid out. And this, I lifted it actually\nfrom the contract last night. These were the numbers\nand the milestones that we were expected\nto hit, EAN and UCC. And I had a very sinking feeling\nin my stomach at that point in time because I had to\nconvince my boss at the time that we needed to do this. This was the right thing. And we were signing on\nbehalf of the US and UCC. And we were supposed to\nget 20 trading partners up utilizing EPC in 2004. And we were also\nsupposed to make this global and a\nmulti-sectoral approach. So these were the numbers. And I was probably never\nmore scared in my life when I convinced my boss\nto sign these numbers. And they looked\nquite staggering. So what have we\naccomplished to date? Well, EAN and UCC is now\nunder one organization. We're now GS1. We work very hard. We have a common\nleadership, common approach. It's now GS1. So I'm a member. I work for GS1 US. There's some other\npeople in the room here from GS1 other\ncountries around the world. And we've also aligned\nour portfolio of standards and service offerings. We have four primary service\nofferings today, the bar codes, electronic commerce--\nit's EDI and XML-- global data synchronization. And here I am to talk to\nyou about EPCglobal today. So the mission, to be true\nto what MIT wanted us to do, is just to take a global\nleadership role in development and promoting multi-industry,\nuser-driven-- key point, user-driven, because if this\nis not relevant to the user, it will not have impact. If you've been\ninvolved in standards processes before,\nif you don't get the support of the\nindustry, you will not get the implementation. So we keep on holding back to\nwhat the users want, and then we develop the standard. Building standards for the sake\nof building standards' sake is not interesting. We wanted to deliver added\nvalue to our customers and stakeholders\nthrough our activities and drive it through the\nGS1 member organizations. There are 102 GS1\nmember organizations around the world today. We want to be the\ntrusted authority on technical standards. We're not a technical\ndevelopment organization. Technology is an\nenabler, but we want to provide the forum where\nthe technologists can sit at the table and\nwork on standards through our processes,\nand to effectively manage public policy. Public policy, what we\nlearn from AUTO-ID Center, is probably as important\nas anything we do, because if this is not\naccepted by the consumer, it's not accepted by the\nlegislators, this will not fly. It will not move forward."}, {"content": "And we have a lot of activities\nin that area of public policy. So where do we stand? You remember the other\nnumbers I put before? It shows the amount of\ngrowth around the world and the breakdown by region. Well, we far exceeded\nour expectations."}, {"content": "And now we have a\ndifferent challenge. It's community management. How do we manage\nthese communities? But globally, as of January,\nwe have 733 trading partners around the world. The most unbelievable\namount of growth is coming from the Asia region. There's government-funded\nprojects in the Asia region in Hong Kong,\nSingapore, Japan, and Korea. They're coming on very strong. And people-- not only do they\nwant to build this technology, but they also want to improve\ntheir business processing leveraging this technology. So it's quite rewarding\nto where we are. You can see in two\nyears where we've come in a very short period of time. And you can see the\npercentages and the breakdowns by region and the amount\nof growth around the world. This is very encouraging. And it's a lot of hard work. But we want to get\nback to our mission. Initially, what was\nin the AUTO-ID Center, there was a majority of-- well, a large portion\nof the companies that were involved\nin the AUTO-ID Center were solution providers. They were looking for\nthe new, new thing. They were looking for\nthe next opportunity. They wanted to build\nthe marketplace, which is very important and\ncontinues to be important. But if we're going to be\nrelevant to the marketplace, we need the involvement\nof the end users. And so you can see, as of\nthe latter part of last year, we have 53% of our total\nmembership is end users. A great industry support, some\nof the companies in the room, Walmart, Target, Albertsons,\nDepartment of Defense, Metro, Tesco, so on and so forth. And we're also getting\nsupport outside of the traditional\nretail sectors. So this is coming to true. Getting back to the\nmulti-sectoral approach, this is a breakdown. This is a US\nperspective, but it shows across the industry sectors. Not only is it just consumer\ngoods, food and beverage, health care life sciences. There's a great balance\nacross these communities. And what they're now finding,\nthese communities, they all start as separate groups\nwhere they all want to talk about their problems. But they start to\ncross-pollinate with one another. And they say some of their\nproblems are the same. And there's good interaction. We have a structure in place\nand a governance structure. But it really starts\nwith a business problem. The Business\nSteering Committees, we organize the communities\naround Business Action Groups. We have a Consumer Packaged\nGood, Health Care Life Sciences, and we recently\nlaunched a Transportation and Logistics. There's regional adoption\nprograms in Europe and Asia. And now we have some\ndiscussion groups in aerospace, food and beverage,\nautomotive, and apparel. So eventually, these will\nbecome Business Action Groups as we move forward. So the structure is in place. We have a continuous improvement\nprocess to organize things. It's great challenge, but it's\na great thing to be involved. Here's the standards\nprocess, where we are to date\nwith the standards and what have been delivered as\nfar as the specifications, what have been ratified\nby the boards. We have ONS, ALE, or Filtering\nand Collection, Tag Data Standards. We have several generations\nof that specification and, of course, Gen2. But the most important\npart of and the real value of a standard is what occurs\nin the certification process. Last September, we\nannounced the certification of Gen2 interoperable\nproducts, which tags work with which readers. Or better yet, which ones\nconform to the specifications."}, {"content": "Now as a result of\nthat, you're seeing prices starting to\ndrop significantly, and we're moving forward. So we're building a marketplace\nfor the solution providers. And we're building opportunity\nfor trading partners that give them freedom of\nchoice on which solutions they wish to procure. We have a governance\nstructure that is balanced and\nacross the sectors. Not only do we have retail. We have transportation and\nlogistics, health care, technology, public sectors. We have regional\nbalance globally from Latin America,\nEurope, and the Far East. Sanjay serves on the\nBoard of Governors providing linkage back to labs. And we have a\ncouple other sectors that have come on board--\nconsumer electronics, as well as aerospace. And the Board will expand\ndepending on industry activity. I want to commend\nElgar, and John, and Nick Ferguson\nup top for the work that they've done\nin the last years. I mean, if there's\nany area that we could have improved\nover the past two years, it was the linkage to the labs. I mean, EPCglobal had\nchallenges, startup issues. The labs they had were-- there was a change in\nrole and how things work. I think we're on a great course\nnow with the new structure. I commend the work that's been\ndone to date in that arena. We've created better\nlinkage between the labs and the research. These are some leadership\nroles that Sanjay serves on the Board of Governors\nand the Architecture Review Committee. John Williams also serves\non the member of the ARC. Elgar serves on the\nBusiness Steering Committee. Also, some of the other\nresearch members, Peter and Dan, Peter Cole and Dan Engels, the\nactive members of The Hague. And other members of\nthe research community are participating in\nthis thing, but we need to align this\nbetter going forward. It's a great opportunity. And as this academic\nalliance comes together, we need to extend the outreach. Looking into the future, well,\nmy list isn't too far from Sanjay's. And we both did\nthis independently. So business justification,\nreturn on investment. There's a lot of\ntrading partners that are still struggling\nwith trying to find the value and return on investment. There are trading\npartners that get it, and there are trading partners\nthat are still struggling. Huge research opportunity\nto help them find the way. Business use cases, they need\nto have clarity to do that. Information sharing, I think, is\none of our biggest challenges, information sharing because\nthere's proprietary solutions. Many of the retailers today have\ntheir own proprietary portals. And getting them to\nembrace an open standard will be a challenge. You have legacy constraints. People have large investments\nin EDI and other systems. Lack of trust, getting\ntrading partners to trust. I don't know if this\nis a research topic. I don't know how to get\npast the trust issue, but it is probably our\nbiggest challenge right now. Inertia is another challenge."}, {"content": "\"We've always done it this\nway,\" is what we hear, and getting people\npast that model. There's also existing\nrevenue models in place in health\ncare, life sciences, and fast-moving consumer goods. People sell information\nto one another. Well, if you create\nan open standard to share information\nover the internet, what happens to the revenue? We have to deal with it."}, {"content": "Nobody wants to talk\nabout that, but it's a reality what we have today. It's somewhat the\nelephant in the room. What we need is\nunbiased research on alternative technologies. I've been approached no\nless than three times from alternative technology\ncompanies and saying, I have the technology\nthat will go past Gen2. But we cannot have a\nsingle vendor solution. We need unbiased research to\ngo look at these technologies and come back and\nhelp show a way. Security, you could\nhave put a whole slide on security, encryption,\nauthentication, password management. The list goes on\naround security. It's such a broad topic. Sensors, active tags. Intellectual property,\nhow do you deal with it? We're dealing quite a bit\nwith defensive patents today or, use my words,\nnuisance patents. But we need to\nwork through that. We're not about companies\nthat truly have an invention that they want to protect. But a lot of the\nintellectual property issues we deal with are just,\nin my opinion, frivolous and as well as public policies. And these are all opportunities\nfor research to move forward. In summary and\nconclusion, the community, we've moved from hope, to\nhype, to implementation. There are deployments there. You can read the reports\non what's happening. The community grows\nstronger every day. It's getting more complex. When you look at the numbers,\nit's only-- we have over 700. It's quite staggering. How do you organize 1,500\npeople around the world to develop standards? These are challenges, but I\nthink we're up to the task. Physics and standards\nchallenges are being overcome. Prices are coming down. The benefits of visibility\nare starting to be realized. We need to get more public facts\nabout these business benefits. But the community has\nmoved decisively forward from preparation\nto implementation. So the companies\nthat are involved, they see the opportunity. And they're off to the\nrace for business benefits. We need to get the\nresearch community involved to bring in more people, get\nthis technology to everyone on a global basis. Thank you very much. [APPLAUSE] STEVE MILES: And\nthen John Williams, the Director of the\nAUTO-ID Labs here. JOHN WILLIAMS: Great. Thanks so much, Steve."}, {"content": "This is a little like\nbeing in a car race here. I feel I've got about 3\nminutes to present 85 slides. No, let me start\nzipping through these. Sanjay got me interested\nin RFID about a year ago. My background is not in RFID. Before this, I built\ninformation systems. So I'm coming to things\nfrom the point of view of, how do you build systems that\nare globally sustainable? And I think there's some very\ninteresting things happening at the moment in\nthe world, certainly in the world of software, that\nthings are changing yet again. We had the internet revolution. Recently, Bill Gates and\none of his VPs, Ray Ozzie, sent out a memo-- it\nwas October 30, 2005-- sent out a memo saying, guys,\nthe world has changed again. We've got the internet\nhappening yet again. They were blindsided. Microsoft were blindsided\nin the '90s by the internet. Now something else is happening. And it's this world of services. They're very afraid\nthat Google has a totally different business\nmodel to providing services. And I just want to say how\nGoogle's model actually impacts RFID, because I think\nit's interesting when you think about\nwhat happens when you know where everything is. And I think that's what's\nhappening with RFID. We have the ability\nto know where a good portion of the physical\nthings in the world are. And what happens to\nbusiness models then? And Sanjay was intimating\nthat, yeah, it's going to change the\nway we do business. And I'd just like to add to\nthat thought because I think it really is going to happen. It is happening. My background is software. I'd like to say something\nabout software trends. Security is a major problem."}, {"content": "There was a recent report. It's the fastest-growing\narea, business area, is fraud. If you want to get\ninto a good area, fraud is a great\narea at the moment. It's growing at\nabout 1,000% a year."}, {"content": "I was talking to Hao Min. And he was saying that The\nHague was recently hacked like yesterday, that\nadverts were being sent out over the email list. This is a serious trend."}, {"content": "The internet was not designed\nwith security in mind. We can see that\nwe're probably going to have to be secure, that\nwe can't allow people to see the drugs in our pocket\nor that you're walking out of a store with OxyContin\nor whatever it is. The other thing I'd like\nto note is that change, we have to build for change. One thing we've\nlearned about software is we can't expect people to\nfollow a standard rigidly. You can't force people into\nadopting a standard that's going to stay static. We're going to have\nstandards that are changing. And that's going\nto be a problem. We have to design for that. So we've got a\nsystem that's global. This was some work we did\non incorporating Google Maps so that we could zoom in. We could type in EPC\ncode in and zoom in down through the layers\nof aerial photographs into a store, onto the\nshop shelf in the shop. That's possible now. We can integrate\ninformation systems that it's distributed\nacross the globe. Now, I think the\ninteresting thing is that that allows\ndifferent business models. As I said, Google is a\nphenomena that we're only just beginning to understand. I don't know if you've\nheard about AdWords, but it's a $6 billion\nindustry at $0.50 a shot. So what they're doing-- this is one of\nGoogle's products-- that what they're\ntrying to do is make anyone can buy\nand sell anything. They're matching\nup search queries with hundreds of\nthousands of marketers, these people on\neBay selling things that they'll match them up. And they do it in a\nfraction of a second. And they do it with a\nvery different software architecture. They don't use databases. They have everything in memory. Everything is cached. They have something like\n500,000 machines at the moment. Nobody knows exactly how many. But they're not using\nrelational databases. They have a very\ndifferent approach. I don't know if you've\ntyped something in and made a mistake in Google. And they suggest what\nthe correction should be. And they don't do it\nbased on dictionaries. What they do, they do it\njust based on the fact that somebody else has\nmade that mistake before. They've recorded it, and\nthey've recorded it what you're likely to type in next. So they have a very different\nkind of set of algorithms to normal. And Eric Schmidt, who\nis the CEO of Google, wants to match every single\nitem in the world to a buyer. This sounds very\nmuch like RFID to me, that you need to know\nwhere are those items, who do they belong to, et cetera. So I think there's going\nto be a lot of research in integrating services. The EPC codes are going to\nbe a critical part of that, but then we're going\nto add other data in. And we're already beginning\nto see some of the issues that that causes with data\ncoming up through-- we've got through EPCglobal. We've got standards up to\nabout this EPCIS level. So this is dealing\nwith EPC reads. Now, above that,\nwe've got things like search and discovery,\ndiscovery services. We're unsure what those are,\nand we need to define them. So this whole layer above\nONS, above EPCIS, we really need to think seriously about. That will involve\nfeeding in other data, matching it with\nthese EPC reads. So at the moment, we're\nthinking about, OK, this will be data from\nthe ERP systems. But it may be data from\nGoogle Maps, other databases, from government\ndatabases for example. So we're trying to build a\nsimulator in the AUTO-ID Labs to understand what this global\nnetwork is going to be like. We feel that we need to simulate\nit to get some idea of where the bottlenecks are going to be,\nwhat the research that's going to be needed. We don't know where this\ndata is going to reside. The EPC reads, are they going to\nbe pushed way up beyond EPCIS? Are they going to reside\nbelow the EPCIS layer? It's unclear how much data\nis going to be flowing or where it's going to be. So at the moment, we're\ndefining this architecture. A lot of the standards\nfor the middleware-- and when I talk\nabout middleware, I'm talking about middleware,\nthe TCP/IP and above level, not the middleware between\nthe reader and the tag. And I think we're\nalready beginning to see there's some confusion\nabout the use of words, of semantics, things\nlike discovery. For us in the Web service\nworld, discovery mechanisms means something different\nto discovery, I think, in the EPC context. So we're going to need to sort\nout some of these dictionaries. So some of the\nissues that I think are important, one\nof the most important is security, for\nsure, that we've got to figure out how\nto secure these systems. And it's not so much a--\nit's a technical problem, but it's at all layers. You can't just secure the tag to\nthe reader, that at some level, security is about\nrisk management, that you can provide a\nvery, very secure system, but it means that you're not\nconnected to the outside world. This is what the\nNational Labs do. They just disconnect\nthemselves from the internet. If you're connected\nto the internet, you're going to have to do\nsome kind of risk management. And that means that\nyou have to know who's taking what responsibility. And we don't have that\ndefined at the moment. Brian has been doing\na lot of research in how the legal\nissues are going to be resolved with\nRFID, of ownership and how that's transferred. So the security issue, I\nfeel, needs to be addressed. It's a systems problem and that\nwe have to view it that way. We're going to need to\nunderstand the business drivers, the software,\nand the hardware. Scalability and\nrobustness, as I say, Google are building\nglobal systems. And to do that, you have\nto look at how you scale."}, {"content": "You have to scale. There's this order n. As n increases, you better\nnot be n squared or analog n. It's not going to work. Caching is going\nto be a real issue, that we were talking to people\nin warehouse management. And they were saying that they\nneed response times of way under a second. So that means you can't\ngo out to the internet to get something. They're down to like one\nhundredth of a second, that they've got\nconveyor belts running. They need to know what this\nitem is, how big is it, what's it made of, et cetera. And you need that immediately."}, {"content": "So you're going to\nhave to cache things. Caching, I don't\nknow if you ever tried to do kind of delete\nwhat's cached in your computer or in your browser. It's difficult. Shooting\ndown caches is difficult. You've got multiple\nlevels of caching. And it's not easy to\nkeep that consistent. And so the kinds of systems that\nwe're going to be looking at, I think, are not going to be the\ntraditional database systems. We're going to\nhave to be thinking of these new kinds\nof architectures. And lastly, I'd\njust like to mention Steve's issue,\nnot Steve's issue, but semantics and the\nidea of Semantic Web. It's a real problem\nunderstanding schema, that it sounds like an easy\nissue, but I give you a schema. EPCglobal, say,\npublishes a schema. Now, what happens is\nI get that schema. I say, yeah, I understand it. I understand what\nshipping date is. I'm going to start\nprogramming things. And then it turns out\nthat different companies have a different understanding\nof what shipping date means. Maybe it means when it's\nactually out of the dock door. Maybe it's not. Maybe it's just\nready to be shipped. So these are real\nissues as to, how do you understand the\nsemantics of the information that we're passing around? I think I'll finish there."}, {"content": "That's a good place to finish. [APPLAUSE] STEVE MILES: So\nmaybe five minutes of questions for the panel. This is a quite\nunusual opportunity to have this range\nof experience. Any questions for our panelists\non the network and EPC network? Oh, actually, if you would\ncome down the microphone, please, to ask any questions. [INAUDIBLE] AUDIENCE: I have a question. Actually, with\nthis audience it's, I think, a very apt question."}, {"content": "And I don't think I'd get\nthis answer from anyone else. So how do we solve\nthe schema problem? I mean, I think the\nSemantic Web actually I think is just very\nexciting on that front. But do you think eventually\nit's going to go away? Now, is it going to converge? Is that going to\nbasically reduce the scale of the problem?"}, {"content": "JOHN WILLIAMS: Steve. STEVEN BRATT: This\nmicrophone work? Yeah, it does. Well, that's a good question\nbecause XML schema is really embedded in a lot of the\nbusiness world already and has been\nrelatively successful. But most of the companies find\nit very difficult to deal with, for the very reasons\nthat John mentioned is that the understanding what\nthe elements mean in a schema are not easily transparent. And so the Semantic\nWeb is trying to address that\nproblem by making sure that there's a much richer\ndefinition of everything also on the Web. So everything that's\non the Web, there's also definitions\nof what it means."}, {"content": "What units do you use? How do you define exactly\nwhat you mean by ship date? Those are hard problems, too. But the idea behind\nthe Semantic Web is that you give\npeople better tools for more richly expressing\nthe meaning of everything that they want to\nput on the Web. So whether that will solve\nthe problem, I don't know. It's a lot of complicated\nproblems including research problems involved. But that's what the aim\nof the Semantic Web is. JOHN WILLIAMS: Yeah. I mean, I think to some\nextent, it's a human problem, that we as humans think we\nunderstand what a term means. And it depends. But we use something\nlike Web services. It's pretty fuzzy\nas to what exactly that means, what it includes,\nand what it doesn't include. Now, as humans, we can get\naway with that fuzziness. I'll ask Steve, so did you\nreally mean this or not? Now, the problem\nwith computers is that they're automatically\nmaking decisions for us that we may not like. And we have no-- or we may not understand\nthat the error has been made. That's the problem\nwith the machine. And at the moment, what we do\nis we sit down in committees and agree on a precise\ndefinition of the term, that, say, in security,\na security token has a very precise definition. And you better be aware of\nthat committee and exactly what that definition\nis and not assume that you know the meaning. But I think it's a real issue. I think change is an\nissue when schemas change. And they undoubtedly\nwill change. I think we have to think\nseriously about how we handle those changes. STEVEN BRATT: Versioning. JOHN WILLIAMS: Yeah, versioning. BERNIE HOGAN: I think John\nand Steve both touched on it. One of the other challenges\nis that the association of information, it's\ngoing to be different. John kind of alluded to it in\nhis presentation about Google, but how you associate data,\nbecause as Sanjay mentioned, you eliminate the line of sight. And so you have all this\ninformation coming in today. So you may have a\ncarton that's associated with a pallet in one instance. And then a few minutes\nlater, it's disassociated. How you manage that\nand the systems and the solutions\nout there, it's going to be a huge\nchallenge going forward. AUDIENCE: My question is\nrelated to what Bernie said, information sharing. Maybe this is a\nslightly different view from using immediately\nGoogle or putting-- translating RFID or\nEPC tag into URI. Basically, the way\nI look at it is RFID tag itself is just\none piece of information. Here what matters\nis a holistic view of where things are, right,\nwhereas the Google is not a holistic view. You type a word."}, {"content": "It shows you a list of things\nthat you, the human searcher, will need to integrate them, OK? And then also, in\nthis holistic view that RFID is supposed to\npresent, each part of it is owned and obtained\nby different companies. And most of the\ncompanies don't want to share those information. So they are caught\nin the middle. They know if they\nshare information, there's going to\nbe great benefit. On the other hand, no\none wants to share, give total control of the\nactual information captured by their readers. So I'd just like to hear\nyour comments on that."}, {"content": "Thank you. BERNIE HOGAN: But\nthere's another-- STEVEN BRATT: [INAUDIBLE] BERNIE HOGAN: There's\nanother dimension to that. What we're experiencing\nwith the communities that are involved with EPCglobal is\nmigrating from a push model. If you look at EDI today,\nit's pretty much a push model. I'll send you all\nthat information. And getting people to\ngo to a pull model, so I can query for the\ninformation on demand, that really touches on the trust\nissue and the security issue. Not only do I trust you as a\ntrading partner and a business partner. And it's going to take a\nwhile to get people past that, businesses past that issue. And it's not just\na technical issue. There's a business process\nissue involved with that. JOHN WILLIAMS: Yeah. I think when-- there has\nto be some advantage to you sharing your information. I mean, the example with\nGoogle is, for example, they provide Gmail where they give\nyou two gigabytes of storage. In exchange, they're\nwatching your email. They're running\nalgorithms on your email. Now, they're going to\nstart offering data storage in a big way\nbecause they want to just see what you're doing. As I say, Eric Schmidt's goal\nis to know where everything is. He'll offer free\nstorage in exchange for just taking some of that\ninformation and processing it. So I think it's these\ndifferent business models that are the fascinating thing, that\nyou can give away something for free because you're getting\nvalue from the statistics that you're running\non that data. AUDIENCE: One comment. Here the trust is not\nabout individual users, which can be easily bought. I can be bought by [INAUDIBLE]. But if it's a company, it's\na very different story. That's what I'm\ntrying to suggest. JOHN WILLIAMS:\nRight, but I think there has to be some\ndemonstrable benefit. And I think there will be. So the ROIs need\nto be explicated. AUDIENCE: All right."}, {"content": "Hello."}, {"content": "My name is Kevin Fu\nfrom UMass Amherst. And I have a\nquestion about where the Internet of\nThings, what role it may play in the next\ngeneration internet. So David Clark here\nof MIT is talking that the internet\nis broke broken, that we need to design it from\nthe ground up and start over. So could you tell\nus what you think is the best place for the\nInternet of Things to play, what's its best role in the\nnext generation internet, in two seconds? STEVEN BRATT: I'd\nsay just quickly, I mean, it strikes\nme again that I like to think of\nthings on the internet, whether it's this generation\nof internet or the next. And that clearly, compared\nto the first generation of the internet or the Web\ngeneration of the internet, things are going to be-- there'll be a lot\nmore things that are available through\nwhatever means, whether it's the Semantic Web\nor some other technologies, on the future internet. And so I guess you'd need to\nbe able to design for that. Now, of course, moving\nto IPv6 was one example of being able to\ngreatly increase the number of possible IP\naddresses so that everything, every grain of sand in the world\nor whatever could have one. But there's a number of other\nprobably design principles that Dave Clark has, I'm sure,\nthought about more than I have that certainly would need\nto be taken into account. Just the sheer volume\nand the security, again, associated with what those\nkinds of huge volumes of things are I would think\nwould be important. AUDIENCE: Hi."}, {"content": "My name is Ying Li. I'm also from UMass Amherst. So I have two questions."}, {"content": "The first was technical. So in the software stack that at\nthe bottom has this device data and then filter, transform,\nand data that's published, where does XML come into play? Is there any benefit\nof pushing XML down to lower layers of the software\nprocessing environment? BERNIE HOGAN: In the\nEPCglobal stack-- I'm not sure stack is the\nright word these days-- but it allows for\nan XML binding. There's not a significant demand\nfor that at this point in time because it's pretty low level. And you could express\nit in different ways, although some companies\nhave the preference."}, {"content": "And the standard\nallows for that option. Where the XML places is as-- I think it was in\nSanjay's presentation-- EPCIS data that's stored\nat the repository. So the EPCIS standard\nallows for a way to store information and\nretrieve information. The opportunity in the\ninformation sharing and the-- that will be expressed in XML. How that's done, it's still\nvery early in the process to describe how that's\ngoing to be done and how information is\ngoing to be associated. AUDIENCE: Oh, so you\nthink it's rather a representation for storage? BERNIE HOGAN: Yes, that\nis the thinking today. Yes. AUDIENCE: OK."}, {"content": "BERNIE HOGAN: [INAUDIBLE] AUDIENCE: All right. So the other\nquestion is, so when you talk about the Internet of\nThings through Web technology, is RFID data management, does\nit represent any new challenges? Or is it just an application\nof the current Web services in Semantic Web technology? Does it represent anything\nfundamentally new? JOHN WILLIAMS: Well,\nyeah, it's not there yet. I mean, we don't\nknow how to do this, to build a global network. I think Sanjay was\npointing out that we've got all these challenges of,\nhow do you control costs? How do you make the\nchips cheap enough? How do you get enough data?"}, {"content": "How do you get reliable data? How do you inference\nfrom incomplete data? We don't know how\nto do those things. AUDIENCE: I see. JOHN WILLIAMS: And, yeah, the\nalgorithms aren't there, that-- AUDIENCE: OK. So it has certain\ncharacteristics of data uncertainty,\nsecurity issues involved. So they do present\nsome new challenges beyond other applications. OK, that's a good answer."}, {"content": "Thank you. JOHN WILLIAMS: I don't\nknow how to do it. AUDIENCE: Yes, hello. Good morning. My name is Harold Boeck. I'm with the University of\nSherbrooke and also the \u00c9col Polytechnique. I probably believe I'm the\nonly marketing professor that works in an RFID lab. [APPLAUSE] Thank you. And during your\npresentation, you talked about the\nresearch opportunities in terms of business\njustification and ROIs. And this is specifically\nwhat I'm interested in. And you mentioned that\ncertain companies get the ROI, whereas other companies\nare still looking for it. What do you believe\nis the reason that the companies who\nhave not found the ROI are not finding it? BERNIE HOGAN: I'm going\nto be dangerous here. I think you will\nnotice a relationship between the companies that\nget it and their success. If you look at the\ncompanies that are lead-- these are leadership\ncompanies, that they get it. They've applied the resources. Some of the companies\nthat are involved have been involved\nwith the AUTO-ID Center since 1999 and 2000. It's no different\nthan other standards if you look at the companies\nthat led the way for UPC codes for in the early days. UPC was the standard for 10\nyears before it took off. But there were\ncompanies who were involved working the\nstandard, and deploying it, and discovering it. Some of the other companies\nare further along in the curve. I think there's a community\nin the middle that's not sure. And then you'll have the\nindustry laggards, as we say. The point I was\ntrying to make is that we need to focus\nfrom a communication standpoint on the\ncompanies in the middle to help tip them over. No different than anything\nelse like the World Wide Web, or email, or any\nother technology, we need a tipping point to\ntip them over to get it. To John's point, there has\nto be a compelling business opportunity that will just get\nthese companies past the trust issue, past the security issue. And that's going to happen."}, {"content": "And I think it's really\nin the marketing area. It's a messaging area. The technology is-- to me, is-- I think we've solved some of\nthese problems many, many times over. But it's a messaging-- and\nlook at some of the work that-- MIT is doing it with their the\nbusiness cases and University of Arkansas with Walmart\nand those type things. STEVE MILES: So actually,\nif I could interrupt here because we're eating\ninto our break time. And tomorrow we have a\nsession on supply chain that is very much\naimed at asking those very types of questions. So I'd like to thank\nour panelists very much."}, {"content": "[APPLAUSE]"}], "Lec 10 | Special Topics in Supply Chain Management": [{"content": "PROFESSOR: OK, very\nbriefly, EyeSpot, a project we are doing in\nthe Senseable City Lab. But let me tell you a couple of\nwords about Senseable City Lab. It's a new research\ngroup here at MIT. It's in between the Media Lab\nand Urban Studies and Planning. Well, what do we do there? What we do is look at this. That's yesterday's wireless. That's Marconi's\nCape Cod Station. Look at all of that steel,\nand engine, and the energy, and amount of effort,\njust to transmit a few bits of information\nacross the Atlantic. Now look at today's wireless. It's more like this. And you can actually transmit\nmuch more information using that. And what we're doing is\nrethink in a creative way the interface between\npeople, mobile technology, and the city. Now concerning\nEyeSpot, this project is based on the MIT campus. That's what you see there. So most of you are familiar\nwith Boston and MIT. And the interesting thing is\nthat if you talk to anybody at MIT, they will tell you there\nis a big revolution happening. And this revolution is in\nthe way people live and work. And it's brought by laptop\ncomputers and Wi-Fi. So if you look at this, this is\nhow people used to work here. And this is more like today. Now, this is a bit\nbiased, as you see we. Could find the worst possible\nimage of a computer room-- dull, boring, natural light. And this is a sunny."}, {"content": "Day it's not like today."}, {"content": "But the thing is that\nthere's a big change. And what we are trying to do\nwith EyeSpot is try to define and see how the\nchange is happening, and quantify this by allowing\npeople to locate themselves, actually, with a\nfew-meter accuracy using the Wi-Fi network. And so it's a unique\ncase study that we are starting to monitor,\nwhere we got 20,000 people in the MIT campus,\na big urban chunk, with 3,000 access points. That's quite a bit. If you think that\ncities like Philadelphia actually are planning to cover\nall of the city with Wi-Fi in the next couple of years,\nand that's less than 3,000, less than the number we\nhave of access points. This high density means that we\ncan locate everybody with just a few-meters accuracy. This is just a map with\nsome of the access points. And actually, I'll show you\nthe two maps for the project you'll see at the museum. Oops. The first type of map\nis a map that shows you, in real time, the activity\ngoing on the network, and how many people are\nworking in different parts of the campus. So you get this type\nof map changing. You don't see very well here,\nbut you'll see it better at the museum. And the second map is this one. That's like the\nheartbeat of MIT. Now, what you see here\nis total activity going on on the campus in real time. And you see the past week. So what you see\nhere is all of MIT. And that's actually\na standard day. You see people coming\nin, working 9:00 to 5:00, and then actually,\nstudents keeping on working quite late, even during IAP. And then you've got a peak\nand a minimum at 6:00 AM. And then you've\ngot the next day. It's interesting-- Friday,\nSaturday, and Sunday, when most of the 9:00 to\n5:00 activity disappears, and actually, you've got just\nthe remaining part of the curve happening, activity is slipping\ndown on Friday evening-- people going out--\nand on Sunday, when you're starting\nto panic again about Monday and the next week. And then there's another peak. And then what you\nsee here is you can do this in every\nroom on the campus. And what you see here, for\ninstance, is this actual room. And you see most of the week,\nit was pretty no activity. And look at today. And today, you can get\nactually up to 25 users, which is the number of laptops-- I counted earlier today-- of people connected to the\nwireless internet here. And you see it here."}, {"content": "That's today, Monday. It will appear in a minute. Anyway, you'll see\nthe demo at the-- yeah, here you see the\nspike, today's activity. And you'll see the\ndemo at the museum. Just a couple of things\nabout, just to conclude-- another project we have is\nactually with a soccer team. Being Italian, I am very\nproud of this project. And it's with AC Milan. And here, we are tracking,\nactually, players with a couple of centimeter\naccuracy in real time, and then developing artificial\nintelligence algorithms in order to study their\nmovement and optimize strategy. So we get this type of\ntraces, and then analyze them. And then, we've got a number of\nother projects really dealing with technology and space, how\ntechnology and space interface. This is funded through a\nSenseable City Consortium that's sort of bringing together\nthe different, the key actors, being part of this\nrevolution from network operators, hardware\ncompanies, urban hardware, and public administration. But that's it."}, {"content": "I think if you'd like to have a\nchat, it will be at the museum. Thanks a lot. AUDIENCE: Thank you very much. [APPLAUSE]"}], "Lec 9 | Special Topics in Supply Chain Management": [{"content": "MICHAEL ROSE: Just\nthanks a lot, Steve, for filling us in\non this agenda. We thought it was important\ntoday to get on this agenda, because this group, the\nHealthcare Business Action Group, has been together\nfor 18 months or so. And we're actually\nat a point now where we're developing some\nbusiness requirements and moving it onto the Hardware\nand Software Action Group for some guidance. And some of our colleagues here\nthat are going to talk today are going to get into\nsome of the details around various areas we think\nwe can use some research help. And clearly, as\nan industry, this is one area that we're\nstarting to recognize where we do need to start\nreaching out and getting some direct help from you all. So we thank you for\nthe opportunity today to talk to you. And hopefully, this will\nbe an ongoing dialogue that we entertain\nwith this group. Well, I though we'd just do a\nquick round of introductions. In addition to me, we have\nBob Celeste from EPCglobal who's the director of the\nbusiness action groups, and he's responsible\nfor facilitation of the business action groups. Ted Ng, who's a director\nwith McKesson, in spite of what it may\nsay in the agenda. I don't think, Ted, you've\ngotten the promotion yet, but maybe next week to the vice\npresident and board member. But hopefully, that'll\nhappen next week, Ted. But it's nice to know, you go to\nCambridge, you get a promotion. And Chuck Schramek, who\nis with EPCglobal now, but he's actually on loan\nfrom Johnson & Johnson. And about a year\nor so ago, J&J, we took we took a decision\nthat we thought it was very important to help\nthe development of industry standards. So we graciously let\nChuck go to EPCglobal to help facilitate this\ndevelopment of standards in the health care industry. So as you see on\nthe agenda here, Bob's going to give us\nan overview to what's been going on within the BAG\nthe Healthcare & Life Science Business Action Group. Chuck is going to talk\naround item-level tagging. We had hoped to have\nTom Pizzuto here today, who actually was our member from\nthe Healthcare & Life Science group from Wyeth who was\nour chairman of that group. But Tom couldn't get up here\nbecause of business reasons. Ted's going to cover\npedigree and the update around messaging\naround the pedigree. And then we'll certainly\nentertain any questions. So we'll all participate\nin the panel discussion. I just thought maybe I would\ntouch on a couple of points here."}, {"content": "Why are we doing this? It's not like fast-moving\nconsumer goods, where you've got Walmart\nand Target saying you must and the Department of Defense\ncoming out with mandates. The reason why we're\nvery interested in this is because of some of the\nregulatory requirements that are starting to emerge. So we had the FDA back in 2004\nthat issued some guidelines around widespread adoption\naround RFID at the item level to begin tracking\nand tracing products. But the reality is, the issue\nthat we're really dealing with is patient safety. This is an issue that this\nindustry takes very to heart, because frankly, we're\nall affected by it. We want to ensure that the drugs\nand advices that you receive are genuine products. We don't want to\ngo down the path-- and clearly you all\nunderstand the issues that we could encounter if\ncounterfeit products are out there. So we're very, very\nconcerned about this. So we're also responding\nto the movement within the states, where\nthey're adopting what are called pedigree regulations. Right now, we've\ngot a large number of states that are moving\nin this particular area, and this requires chain\nof custody tracking of the ownership of products. And as a result,\nboth the FDA and-- well, the FDA particularly\nfeels that this could be a key element to the\ntrack and trace of products. So the movement within the\nstates and the FDA, we thought, was very important to be able\nto collaborate with them to show some movement in the\nindustry with the development of our standards. But this is not just\na US issue, and there are some other motivations\nthat are going over in Europe-- particularly\nBelgium, Italy, and I know we've seen some movement\nin some other countries as well, where they're expecting\nserialization of items. Now, to be very clear,\nBelgium and Italy, they're not expecting RFID\nto be applied to a product. All they're asking for is\nserialization of that item-- mass serialization. And the movement within\nEurope is primarily driven because of\npharmacy fraud. That's the reason why they're\nlooking for serialization of items, because\nthey want to know when a particular\nproduct is dispensed that gets reimbursed and\nonly for one time, not multiple times. So these are some of the\nemerging regulatory drivers that we're starting to see\nwithin the pharmaceutical industry. This is a model that\nthe FDA'S proposed, and we keep this in mind as\nwe continue our discussions. And I think this\nis very important. It actually fits with a\nlot that was said earlier, by the other industries\nthat presented today of, to really enable safe\nand secure supply chain, you need to be able to\ntrack and trace product. But you also need to be able to\nauthenticate product as well. So in here-- I've got a bit of a build here. But to move us along, we\nthink it's very important that authentication is\nneeded, to be able to ensure that the product is genuine. That's when it becomes\na foundational element for any track and trace system. Clearly, the pedigree allows us\nto be able to monitor and track that chain of custody, and\nchanges as that product moves through the pipeline. So when you look\nat authentication, there's also a couple of\ndifferent elements of that. We're looking at the\nproduct identity, but we're also looking at\nthe physical characteristics of the product as well. So it's not simply does it have\nan ID number, because if you think of RFID at\nits basic element, and mass serialization\nat the basic element, we've been able to identify that\npackage and the number that's been assigned to that package. What you don't know are about\nthe contents of the package. Was it potentially tampered\nwith, things like that. So that becomes gets\nus more into the areas of physical features. Certainly in the\nfuture, it would be nice to see if RFID could\nplay a role as a method to ensure that the product\nhas not been tampered with. But right now, in its current\nincarnation, it cannot do that. But we think over\ntime, it would be nice to see what\nrole RFID could play there to ensure that the\nproduct has not been tampered. On pedigree, we talk\nabout track and trace. So where is the product\nand where is it headed, and where was the product. So it's the element of\nlooking where it is, but then also being able\nto look backwards. And that's a very\nlarge issue for us. Our supply chain is not directly\nfrom manufacturer to retailer. There is a distributor\nin the middle there, and there can also be\nsecondary distribution as well. So what you find\nis our supply chain can be a bit complicated when\nyou start factoring in also returned goods. And the retail pharmacy may not\nship those returned goods back to the distributor who\noriginally distributed it to them. So you get into some very\ncomplicated situations to figure out where the product\nwas and from whence it came. So as we conducted our work\nover the last 18 months, this is the base\nmodel that we've been working with\nand trying to develop some specifications for our\nindustry based upon this model. So with this, I'm going\nto turn over to Bob, after an intro of\nwhat's driving industry. And Bob's going to talk about\nthe actions within the HLS BAG and how we've organized\nour efforts and some of our current activities. So Bob? [APPLAUSE] BOB CELESTE: Thanks, Mike. So I just wanted to\ngive an overview of why health care is looking at RFID. Basically, it's to form the\nsafe and secure supply chain."}, {"content": "So you're seeing things up\nhere about item-level tagging, electronic pedigree, track and\ntrace, product authentication. And the group actually\nhas a very regimented way of going through this-- of developing the\ncapabilities that they'd like-- the scenarios, the\nuse cases, and then any variants on those use cases. Our first capability\nthat we started with was pedigree management. And as Mike talked\nabout, that was primarily driven through the regulations\nthat the industry had. So I'm actually going to spend\nmost of my time on this slide, and I'll give you a\nsecond to soak it in. We're hoping that the Auto-ID\nmembers in the audience will take a look at some\nof these checkmarks. These are areas that we feel\nthat Auto-ID can help us out with. Basically, this slide is\nbroken into a few areas. The blue areas is our structure. So for each one of these blue\nareas, there's the co-chairs, and they all report up\ninto the tri-chair area. So you see the strategy work\ngroup, information, technology, R&D, and process work groups. Last year, when we\nlooked at things like pedigree management, one\nof the things we realized was-- and we've talked about a\nlot today with security, and the requirements\nare pretty clear. The people in the supply\nchain, and sometimes customers, would need to read\nand write the tags. The bad guys don't\nneed to do that, and that's our requirement. To that point,\nbecause we don't have a very simple, cost-effective,\neasy mechanism for securing the tags and the information\nin the supply chain, there's a number of\nareas you see here where we're looking at security. In the areas of item-level\ntagging, how to physically secure a tag? In the areas of\nserialization, how do we secure the number on a tag? In the areas of decommissioning,\nhow do we ensure that a tag does not get removed from a\ndiscarded bottle and re-enter the supply chain-- things like that. So security actually\nhas broad ramifications as far as the areas\nthat we're working on. Some of the areas\nthat we'll be working on in 2006 that are interesting\nare track and trace. And I want to make a difference\nin your mind between what track and trace and what pedigree is. So pedigree-- and I think Chuck\nactually coined this phrase today. With pedigree, if\nyou think about it, it's a business to\nregulate our message. Pedigree can be\nstarted by paper. They can be started\nthrough a EDI transaction or through an XML transaction. Not easily courierable\nfor an industry. So we're also looking this\nyear at the track and trace, the vocabulary that\nthis industry will use, and the understanding\nthey'll have at a business level of what all of the events\nmean when you move product with RFID tags on them. The other areas I'll point\nout that will be new is-- and we originally saw\nthese as variants, so we'll be doing a\nlittle bit of a GAAP analysis between the\nrequirements we have today and when we bring companies\nthat focus in these areas. And those areas like cold\nchain, medical devices, which will start up soon, and\nbiologics, and how they affect and how they add\nrequirements to our pack. So I think I'm maybe\ndone with that."}, {"content": "That was my one slide. So we'll give Chuck the floor. [APPLAUSE] CHUCK SCHRAMEK:\nOK, I'm here again subbing for Tom\nPizutto with regard to item level\ntagging in the work that that work group has done\nfor the Healthcare & Life Sciences BAG. This group consisted\nof a series of about, I guess, 32 individuals that\nwere part of the Item-Level Tagging work group. 10 were from Fast\nMoving Consumer Goods, 10 were from Healthcare\nand Life Sciences, and 10 were from the Hardware\nAction Group within EPCglobal. In addition to that, we had\nDan Engels from Auto-ID center here working with us. And we also had an individual\nfrom the Architectural Review Council within EPCglobal. So that group was pulled\ntogether from the three BAGs to represent those BAGs and\ntheir interests with respect to turning requirements\ninto solutions, or at least into understandable requirements\nfor the Hardware Action Group to act upon. The objectives that\nwe had before us-- again, because we were basically\nbusiness action groups, was to really define\nthe business scenarios and requirements\nthat would drive the way we intended to deploy\nand adopt RFID technology. We started out with\nbusiness scenarios, again focusing on item level. Case and pallet had\nbeen pretty well covered in a fine fashion from FMCG. Item level had not. It was paramount importance\nto Healthcare & Life Sciences. And yet, it was also a\nsignificant importance to both FMCG and DoD. Went into tremendous detail\naddressing the operating environments in which item-level\ntagging would have to perform. Min/max read and write\nranges were critical to us, because it was a very\ndifferent set of scenarios from pallet and case. Security requirements, which\nyou've heard all day long, were as important for us as\nthey have been for all of you, both in the other industry\npresentations, as well as the Auto-ID Lab's work that\nyou've been doing to date. Privacy features, critical to\nHealthcare & Life Sciences. Again, we have significant\nconcerns there that, unless we educate the\npublic well enough to understand the value of\nthis technology and its use in their best\ninterests, it will be a challenge getting adoption. We have a number of products,\ncontrolled substances that, in pharma industries, we\nhave to move out to customers. These need to be shielded and\nsecured so that others do not know what is being sent. We don't want these\ntags going out live to customers and consumers,\nbecause they in turn are paranoid about knowledge of\nmedication consumption being made available to\nthose around them. And lastly, memory features-- this area is how do we\nwant to use the memory that will be on the chip in the most\nefficient and effective way? The progress we've\nmade to date-- and again, this group\nhas been working, I guess, since about\nlate July of 2005. We delivered a series of\nitem-level requirements to that group. It was about 32 pages\nworth of requirements, spanning both all three-- HLS, FMCG, and DoD, as well\nas 60 business scenarios that we felt well\ndemonstrated the way this technology needed to\nwork for those three groups. On January 16 and 17,\nwe narrowed that down. We winnowed all the way down to\nabout seven critical scenarios, that it was the intention\nof the Hardware Action Group and the vendor community\nsupporting Hardware Action Group to do demonstrations\nof those seven scenarios. And those seven scenarios\nare the ones listed here."}, {"content": "They span, again, the\nthree Business Action Groups I'm here to represent. Hanging garments on\nmobile metal hanger racks creates a real dilemma\nfor the apparel industry from a reader perspective,\na tenant perspective. A dock door portal, same thing."}, {"content": "If you look across all\nthree of those groups, we need to be able\nto read accurately. And particularly when you\nget to the health care side, we are not comfortable\nwith 99% reads. We really do need 100% reads. Apparel point of sale,\nagain, was a scenario we picked, because\nit represented one of the more challenging\npoint of sale scenarios that needed to be\nshown to be effective. DVDs in adjacent shelf slots,\nin that particular scenario, again, represent close packing\nof a product on metal shelves-- a dense reader\nscenario, more or less. Vials and ampoules in a case-- here again, this\nwas a read scenario. And for us, this represents\nprobably one of the smallest products that we'll\nbe tagging, and it represented a challenge for\nus both on a read and write perspective. The speed at which these move\nacross the packaging lines is of concern, so we\nneed real accuracy there. AUDIENCE: [INAUDIBLE]. CHUCK SCHRAMEK: Exactly, and\nprobably the most challenging scenario that the Hague faces,\nin the work ahead of them, is that of the retail form\na mixed tote scenario. This is where the wholesalers\nwill put together-- particularly the wholesalers\nwill put together a code of product\nthat could represent a dozen or better\nmanufacturers' products, all tagged differently, all\nkinds of different products of all sorts of form\nfactors, of materials, and of contents, all together\nthe orientation of antennas, its random orientation. So it's going to be truly\na challenge for them to demonstrate that one. And then the last one,\nthe vial and ampoule write scenario was the\nonly write scenario that we did including this,\nbecause we felt that this was the one that represented\nthings very close together, very small form factor,\ngoing at a particular pace. The areas of central\nfocus and concern for us in these demonstrations\nthat are coming up are really to look at\nperformance considerations. And again, this is the\noperation of the tags, operations between\ntags and readers, operations of fixed readers, and\nthe presence of mobile readers. Again, this is in the retail\nend of the supply chain that we're addressing. And lastly, operation in\ndense reader environments. There again, inventory\ntags that are occurring in the retail\nsetting, either in the store or in the back rooms, with lots\nof readers, lots of antennas, tons of products. Security considerations-- here,\nas you've heard all day long, the interest here is in\nmanageable, affordable, and non-intrusive\nsecurity solutions."}, {"content": "We just can't afford the cost\nof the detail or the latency that might be introduced with\ncertain types of encryption and decryption requirements. So this is an area\nthat we really would like to work very\nclosely with the labs on. Memory read/write locking-- this\nis hopefully a solution for us as we move forward with\nrespect to shielding certain pieces of\ninformation that we need to shield as the products\nmove through the supply chain, yet make them visible at\ndifferent points in the supply chain for efficiency\nand effectiveness of those operations. And lastly,\ndecommissioning of tags-- Bob mentioned that\nearlier in the discussion. This is an area that, again,\nfor the Healthcare and Life Sciences group is of\nparticular concern, because we want to\nmake sure that we have a way of either killing\nor partially killing tags as it goes out to the\nconsuming population, and possibly bringing it back\nif we want at a later date, when we want to use it in a\nmuch more progressive fashion. The dense reader environment\nconsiderations-- here, we're having trouble, because\nevery one of those scenarios is unique to the business\nor the operation that's performing them, and how we're\ngoing to replicate something like that. And our task coming up is\ngoing to be a real challenge. But we do have dense\nreader scenarios in these various locations\nthroughout the supply chain, either at the dock doors, at\nthe racking and the distribution centers, at the shelves\nin the retail store, and even at customer checkout. So it's a concern for us\nin terms of interference. And lastly are the\nmobile readers, entering the field of\nstationary readers. Again, the Nokia\nphone example of being able to go in and to read a\ntag, our concerns were, well, is this really going to impact\nthe intelligent shelf readers that may be in place or not. And it seems like\nfrom the research that's been done\ntoday by Auto-ID Labs here, that's one that\nwe're not going to have to worry too much about. Now, I'm not going to\ngo into detail on these. But these are just to\ngive you an example of the depth of detail and the\ncomprehensiveness in which we defined our business scenarios\nto capture the operating environments in which these\ntanks would have to work. We measured everything\nfrom where in the scenario it would occur to\nthe number of tags that would have to be read\nat that particular point, the interrogation region that\nthe readers would be impacted by, the tag antenna sizes\nand the variety of those tag antenna sizes, maximum read\nranges, maximum tag velocity-- or in cases when they're\nstationary, how long are they stationary to allow a read. The reader reliability--\nagain, this is something that we feel\nvery strongly about being as close to 100% as possible. And then the last one were notes\non the materials and make-ups of what these scenarios\nwere all about. A good example, again, going\nback to the retail pharma tote, here again, you're looking\nat liquids, blister packs, tablets, syringes, full\nmetal packages, all in a tote to be read with random oriented\nplacement in that tote. So these are very, very\nchallenging demonstrations."}, {"content": "We know, hopefully, that most\nof it will be successful, and we'll move forward\nwith getting solutions. And where we are now\nis we're proceeding in this second phase, which\nis turning all of this over to the Hague\nfor them to take on. There is a little\nwrap-up work that's still expected of\nour group, and that's to get a better sense of what\nthe hospital demands are. Unfortunately, today, we haven't\nhad hospital representation as directly and\nstrongly as what we've needed in the HLS community. But we do need to get\nmore information there, because that represents\na possible frequency issue for us, and\nlikewise more complexity. So the Hague is going to\nconfirm that the technology vendors that are going\nto work with them in these demonstrations,\nwhich they think will be somewhere between\nsix and 12 companies working with us. We want to make sure\nthey cover the seven scenarios in an equal\nfashion, so they don't all go after scenario one\nand five, and the others don't get addressed. That's going to be the\nchallenge for the Hague to make sure they work that out. They're going to do the work\nbetween February and March 22, at which time,\nthese demonstrations are going to be provided back\nto the three business action groups-- HLS, FMCG, and DoD. And then we will see where we\ngo from there in terms of what they can do and what they can't. So out of that, we expect\na lot of opportunities to come back to you, and ask\nfor assistance in drilling down on how we\ncan turn something that doesn't look like it\ncan work today into something that can six or nine months,\na year, year and a half out. OK. [APPLAUSE] TED NG: Hi, I'm Ted. I'm going to try to go\nthrough, with you guys, what we've done in the BAG for the\npedigree side of the business. Again, Mike opened up with\nvery leading drivers of what's causing us to do this. And really, the driver\nis not pedigree, but the driver is really\npatient safety in health care. OK, again, our\nlightening rod for us to move forward in\nthe pedigree space was caused by the\nFDA report that was issued in February of 2004. So it appears that we do\nhave a counterfeit problem, and how are we going to\ndeal with that problem? And the report indicated\nthat the best way to do that was to use\nRFID-based systems. So I think you guys have\nseen these products."}, {"content": "One side is the good product. On the right side-- I mean, left or right side,\ndepending on the product, is showing you the\ngood [INAUDIBLE] and the bad products. So how are you going\nto tell the difference? So they're saying\nRFID is the way to go. The problem is we have too many\ncooks in the kitchen for us. We would love to comply. I think the industry\nwants to comply. I think it's the right thing\nto do for us, the right thing to do for the nation. But if you look at\nthis, 30-plus states have written pedigree laws now. And I represent\nMcKesson, and McKesson's a wholesale distributor\nof pharmaceutical drugs. We cannot afford to write\nsystems for 30 different states, basically. So we really need to\nhave a consistent model. The model that Mike showed\nwould be great to have. But unfortunately,\nFlorida, the first one that's coming up\nfor us, which we put a lot of effort\nbehind July 1st of 2006 indicated that we have to be\nin compliance to their model first, and then followed\nby January 1st of 2007 is California. And then there's various\nlaws already in place, like in regulatory\nprocess like Nevada, like Texas, already in progress\nand writing regulations that may interfere with\nwhat has already been proposed by Florida. So the real benefit\nfor us, so to speak, is that the models have been\nproposed by the other 29, 30-plus states here differs\nsignificantly from Florida. So there might be\nsome consolidation, so to speak, hopefully\nby the models. And what we're looking for is\nsome federal interaction here that will allow us to do that. So really, I'll talk a\nlittle bit about the solution is identification and tracking. But how do you do that? I have not heard\nanything today that tells me that says that we\nneed to do the entire supply chain in all products\nin the supply chain. That's not what we\nneed to have done here. We've got to do some type of-- I don't think there's sufficient\nRFID capacity in the industry to solve and meet all of the\ndemand necessary to enable the entire supply chain of\npharmaceutical products. So there's got to be some\napproach, some thinking, that says that what's a rational\nadoption approach here? Is it high demand guys only?"}, {"content": "Is it the top 100? Is it the guys that\nare most counterfeited? So we need help and\nneed some guidance here."}, {"content": "But again, we're forced by\nthe Florida pedigree law to enable the\nwhole supply chain. But if you look at-- well, going into what\nFlorida is all about, that model is\nfundamentally flawed, in the way we would think\nabout a pure chain of custody and a pedigree model. So the Florida pedigree model\nsays it's not the data element, so to speak. It's what you're trying\nto accomplish here."}, {"content": "And we need this basic stuff."}, {"content": "And they don't even talk\nabout serialization. Obviously, when you\ncounterfeit product at an item level,\nor a bottle level, and they're looking at tracking\nonly a lot and expiration date, so to speak. So there was other\nindustry groups that have come together--\nnot just states. I didn't show you the\nwhole picture here. There has been\nNABP, then there's other groups in uniform. There's our uniform\npedigree council that we put together by\ndifferent industry groups representing various\nfactions here. But they came up\nwith a different set of data elements associated with\nwhat should be in a pedigree. So really, what they\ndid was in those groups was define quote,\n\"the data elements.\" Obviously, data\nelements are of value. Because I'm an IT guy, I\nwork with data elements."}, {"content": "But what's more important? What's the process associated\nwith that definition of what a pedigree is? Who updates the data? Who says what data should\nbe there once you update it? How do I share the information. And then from the process\nside, where does it start, who is responsible,\nwho will enforce? Again, we talked about what\nis authentication here. It's not just a number. It's also the over and\ncovert measures associated with it, and the\nlinking of the two sides of the pedigree\nmodel, with a right side and a left side, in\na logical data model. So who pays for\nthe system, which is even a better question. If regulatory\nmandates this stuff, obviously, there's going to be\na huge increase in cost here. And McKesson deals with\nliterally thousands of local pharmacies. And in different\nparts of the country, we call them mom and pops. If we put in an\nRFID system they're associated with a more\nsophisticated system, can these pharmacies be able\nto authenticate the product when we receive it? Do they have the cost\ninfrastructure to do so?"}, {"content": "I know we need standards. I'm also part of\nthe Information Work Group working within the BAG. What we really need to have\nis the information model, the logical models, the\nflow associated with it. You guys, I know\nsome of the speakers here today attempted\nto bridge into that without talking about it. But if we really need the\nrules as an IT organization to do synchronization,\nvalidation, ownership, retention, security,\nFlorida says we have to keep the records\nfor a pedigree for three years. We generate-- oh no."}, {"content": "I'm not going to go through\nthe numbers of McKesson, but there's a lot of orders. We generate an order for\neach customer every day of the year in Florida. We do an overnight\ndelivery model. So how many pedigrees do we\nneed to create for every product there? We cannot afford to create a\npaper pedigree-based system, and Florida allows a\npaper pedigree system. And there's no integrity in a\npaper pedigree system, really. This is a little bit about\nevolution of hardware, software, and integration. But if you look at all the\ndifferent architectural stacks and stuff, really,\nthat middleware is expanding to the edge, and\nmiddleware is spanning upward into the enterprise. So there's no static model,\nso to speak, of technology. Here we are, moving\nfrom gen 1 to gen 2. So we're talking\nabout agile readers. We're talking about--\nwe haven't really come up with a sense\nof understanding what are the firm\nfrequencies associated with what we're going to do? How are we going to write\nthese tags, and what should be on the tags? And then we have the performance\nissues with item-level tagging still to decide. So I think Chuck\nand Mike here really said the same things here."}, {"content": "We really need your help here. Help us resolve\nthese issues here, so that we can get this\npatient safety issue addressed. So a summary of issues here. Again, top of the list is\nreally for us in health care. We're guided by HIPAA rules. We're guided by many different\nprivacy and security practices already. We need one that allow us to\ndo what we need to do, and not lose the confidence of what\nwe have in our health care consumer base today. We cannot allow that to erode."}, {"content": "So again, where's the data? How are we going\nto hold that data? The health care\nindustry model today, it's held very closely\nto the vest, the data. So we need to come\nup with models that allow us to expand beyond\nour current supply chain model, and to say what is the\neconomics associated with it. Product identification\nschema-- again, we talked about it in many\ndifferent forums here today, a local schema versus\nthe global schema. Obviously, the manufacturers\nhave a global business. And yet the NDC number,\nwhich is the primary product identifier here in America,\nis used for claims processing. And a pharmacy, for them not to\nhave that number on the product is a big deal. And also, I don't\nknow if tagging was talked about by Chuck,\nobviously, in detail. Forward and reverse\nlogistics participation-- obviously, if you look\nat the Florida model, it doesn't start with\nthe manufacturer. It starts with the wholesaler. There's a few supply chain\nparticipants missing there in the middle here-- [CHUCKLES] --Including logistics guys. So we believe that, really,\nthe best pedigree model contains all participants\nin the supply chain. There can't be\ngoing from one step to step three, and step two\nmaybe if you have the time. [CHUCKLES] So what we're looking for is\nreally state and FDA ePedigree model. We're looking for\nan electronic model. Electronic models, I\nbelieve the pedigree model itself is a pretty uniform\nmodel, in the sense that the problem that\nyou can address, in terms of a pedigree model\nfor health care can apply to consumer\npackaged goods. It can apply to the clothing\nindustry, for example. So the model is\npretty clear, but we need to define one\nacross industries. And I think that would\nadd significantly to the adoption\nassociated with it. Tag frequency,\ntechnological maturity-- I caution, as a business,\nwe have x number of dollars each year to invest\ninto technology. What we need to have is\nan evolutionary step."}, {"content": "We can't slowly evolve. We've got to have\ncheckpoints along the way. I was talking to John\nearlier over lunch about the concept of we have to\nset the future vision of where we're going to go. Where are we going to\nbe two years from now, three years from now, wherever\nthe slice and dice is? But the end of year 1, we\nhave to say to ourselves, we're all going to\nbe x 12 by this date, and be able to accomplish\nthese type of things. So that's what we're looking\nfor here as an industry, because we can't do this\nincremental adoption approach, because it's a huge\ninfrastructure we have to change overnight. Took us 30 years to get\nthe barcode maturity. We cannot wait 30 years\nfor RFID to become mature. We all understand the product\nof identification strength and the need for\nproduct identification, and the glory associated with\nunderstanding what else we can do with additional\nbits of information that we never had with\nthe linear barcode. So again, what's a rational\nadoptions approach? For McKesson, a\nwholesale distributor, what percentage of product\nhas to be enabled for us to make that a non-parallel set\nof technologies, so to speak? I can't be scanning product\nwith barcodes in one hand, and then using\nRFID on the other. It doesn't save me any time. If I close the tote and\nI have to open it back up to scan those products that are\nnot RFID-enabled versus guys that are barcode-enabled. So we have to reach a level\nof maturity in adoption as quickly as possible. And that would be everybody\nin the supply chain again. And that goes back\nto the cost dollars. It goes back to the\nevolutionary aspects of it. So it's really not a\nfocus on technology. I don't want to make-- I know we're a\ntechnology group here. We really need to focus on\nthe long-term vision in terms of a business set\nof requirements, and saying we're going\nto sell safe and secure, we're going to sell\nreverse logistics or operational efficiencies. One make may follow the\nother on a logical basis. But when you develop\na sense of solutions-- and when I say ePedigree, to\nme, that's a business scenario, and what are supporting\ntechnologies that allow us to get to step\none, and then further out, a second step\nassociated with it? And that's all I\nreally had to say."}, {"content": "I want to keep to our time here. So I'm sure we're up here\nfor a few questions here. [APPLAUSE] STEPHEN MILES:\nThank you very much. If we could come\ndown to the front of the room for any questions? You just handed us a big menu. [LAUGHTER] Picking up on themes\nfrom throughout the day. AUDIENCE: Hi, I'm Alfonso\nGutierrez from the University of Wisconsin, Madison. In the beginning of\nthe presentation, you had a big slide saying that\nit's for the patient's safety. We're working in some\nof the blood products on a supply chain, and a big\nissue that we're dealing with is the patient identification. This is called the patient. And at the end of\nall of this, it's a patient who is receiving this. Different than the\nconsumer products, we have to tie in\nwith the patient. It's not only the\nconsumer that we can just send the product out and we\ndon't care who bought it. Who's dealing with this? Who's skinning this cat\nof patient identification? Should it be in parallel\nwith all of this? [INTERPOSING VOICES] MICHAEL ROSE: I think\nit's an excellent point. I think this whole area\nof linking prescriptions to devices-- because the common\nconnection is the patient. So you really don't see anyone\nsolving that problem right now. One hope on the\nhorizon, though, is there's quite a bit of\nmovement in the government."}, {"content": "Dr. Brailer's\norganization is looking at helping improve\nthe health care system, the application of IT. That's an area that is\ntaking a look at this. So we would expect\nsome of the work that's going on there may\neventually move in that particular direction. The other area,\nthere's an association called NAHIT, National\nAssociation of Health Information Technology. It's certainly\nwithin their brief, and they commissioned\na study last year around the adoption of RFID\nand other auto-ID technologies, not just for products, but\nalso for patients, too. So I agree with you. It's an issue, but it's\nstill very fragmented and needs to be brought together\nin a more coordinated manner. TED NG: Also, from\nmy perspective, RFID and patient identification\nprobably don't go together on the same chip, so to speak. [LAUGHS] I think that there's a privacy-- the HIPAA laws. I don't know if you\nguys are familiar with the federal HIPAA-- Health Care Insurance\nPortability Accountability Act. That requires that data\nbe secured, and have privacy concerns associated\nwith the linking of personally identififable information. So I don't think it'll\nbe on the product that you get from the pharmacy. There might be a pharmacy\nnumber or a script number. But again, your\npatient information will be stored by\nthe pharmacist. Additionally, again,\nthere's another agency called HIMS, Healthcare\nInformation Management and-- the two S's. [INAUDIBLE] Systems, Service-- yeah,\nsystems along that line. Again, electronic\nmedical records is the solution here\nsomewhere along the way. Also in the federal\nbodies here, they're looking at trying to develop\nelectronic medical records. BOB CELESTE: One\nof the areas that may be able to help you out with\nis that in Ireland does quite a bit of work [INAUDIBLE]. And they've got serialization\nin the spring of [INAUDIBLE].. AUDIENCE: We're all from the\nsame school [INAUDIBLE] in MIT. Actually, we are doing some\nmarketing research of the RFID in the drug tracking business. We found, actually, the\nbusiness is growing very fast-- the market size. Like, in 2006, it's\ngoing to be 25%, and in 2007, it's\ngoing to be 30%. In 2011, it's going to\nbe 40%, and it's going to exceed, like, $1 billion US. And I noticed that you\nhave a summary of issues. One thing I'm specifically\ninterested in, we obviously didn't get a chance to explore\nmore, is the data network-- I mean, central distributor,\nand also the secure access, when you have all this networked\nsoftware located everywhere, probably globally. Is there any initiation\nor any research in that area, like\nhow you guarantee that data is right before\nyou get it into the network? BOB CELESTE: So\none of the things that we'll be doing this year\nis the track and trace effort. And that will, first\noff, define a vocabulary that the health\ncare industry will use to identify\n[INAUDIBLE] events and understand them from\na business perspective. Part of that again,\noverlaying everything, is the security aspect. And so [INAUDIBLE],, in\nthis case a software, actually, we're running\nsecurity around the network to ensure that data is safe. But that's part of\nour 2006 effort. TED NG: Again, it's part of\nthe development of the entire-- if you're looking at\njust any pedigree alone, you're going to have to define\nbusiness process rules as to-- at what point is it-- what part of record, so\nto speak, [INAUDIBLE] pedigree record is created. Who creates a track,\nwho creates a trace, who creates the authentication\nside, the product identification side? And what part of\nthe supply chain, in terms of the supply\nchain participants, are required to update this? Because if you want\na good pedigree, all the players have to\nplay, and each of them have a role to play. STEPHEN MILES: I've got\na question regarding chemical and thermal\nstability of drug compounds. Can you say anything\non tests you've done with respect to\ndifferent frequencies of different RFID systems\nand their outcomes? MICHAEL ROSE: It's\nan area-- and I know the Auto-ID Labs here at\nMIT have proposed to study. It's an area that's still\nout for investigation. Without divulging\nanything, some companies have done some work\nthat we're aware of. But it's not been\nmade public yet. And I think what's\ngoing to be required-- and I think you're\ngoing to see, the FDA has a meeting on\nFebruary 8 and 9. It's going to be another call\nfor companies to come forward with public data. They made that call at the\n[INAUDIBLE] HDMA meeting back last fall. I think the challenge\nthat companies have around this\ninvestigation is, what's the design of experiment? What's the endpoint\nof that experiment? And that's what everyone's\nstruggling with right now. AUDIENCE: OK, thanks. STEPHEN MILES: So thank\nyou very much to the panel. [APPLAUSE]"}], "Lec 13 | Special Topics in Supply Chain Management": [{"content": "GUEST SPEAKER 1: Good morning. It's a real pleasure for\nme to be with you today. Appreciate Steve and\nBill inviting me here. For those of you that don't\nknow me, for the last six years, I've been managing the Gillette\nCompany's EPC initiative, and now I'm leading the\nstrategy development and pilot implementation of EPC for\nthe entire Procter and Gamble company, including Gillette. I look around the\naudience and I'm pleased to say that I see\nlots of familiar faces. And I don't know\nwhether that means I've been doing\nthis for too long or that this technology is\nreally a game changer that's going to dramatically alter\nthe way trading partners go to market together. I'd like to think\nit's the latter. And I hope to share\nwith you this morning some learning and some plans\nthat Procter and Gamble has to implement this technology. Procter and Gamble and Gillette\nhave been supporters of RFID and specifically the electronic\nproduct code for many years, going back to the co-founding\nof the AUTO-ID Center, initially right here at MIT. And that led to the development\nof the electronic product code, which has now been turned\nover to EPCglobal and GS1. And I'm thrilled\nwith the association that EPCglobal and AUTO-ID labs\nhave to pioneer new research and continue to support the\ndevelopment of the technology. In 2005, Procter and\nGamble and Gillette merged to create one company. It gives us the\nproverbial opportunity to put one and one\ntogether and get three. The shared learning\nof these two companies has been combined to\ndevelop a unified approach to implement EPC. And the commitment of both\nthese companies has not changed. In fact, it's accelerated. It's all about transforming\nthe supply network using the EPC to\ndeliver better customer service so we know where our\nproducts are at all times. To generate less\nloss, less shrink, less theft, less\nproducts being misplaced and not being able to be found. It's there to help us\nhave fewer inaccuracies, going from assumed receipt to\nEPC-verified receipt of goods. And finally, and\ncritically important, to give us greater product\navailability for consumers. As consumer marketeers,\nProcter and Gamble's number one complaint is consumers\nwho come in to store and can't find the products\nthey're there looking for. And we feel the electronic\nproduct code can address that. So let me share with\nyou some learning from Gillette and\nProcter and Gamble. Both companies have\nbeen actively engaged in pilots for a\nnumber of years-- pilots both in-house within\nour four walls and pilots with our retail partners. We've done this to test,\nimprove the technology, develop scalable solutions,\nvalidate the business case, and drive deployment to\ndeliver business benefits now. We can't do everything\nwe want to do at once, but we need to keep moving in\never-increasing increments, starting with, what I\nheard referred to earlier, as low-hanging fruit. Capitalize on that, generate\nvalue, realize the benefits, and then capture that learning\nand move to the next stage. Our research, much of which\nwas done with team members from MIT, showed that the\nlion's share of the opportunity is the collaboration gains that\nexist between trading partners. We call it the\ncollaboration zone, whereby managing\ndeductions management, electronic proof-of-delivery,\nif you will, increased on-shelf\navailability, reduced inventory and improved working capital,\nwe can generate serious business benefits. So we began in January\nof 2005, with one strategic retail partner to\nprove many of these benefits. Our pilots led us to discover\nthat promotional and new item compliance was a\ntremendous opportunity. Low-hanging fruit, if you will. One pilot we did was with\nour Venus disposable razors. These are our female\nshaving system. This was a new item launch."}, {"content": "It was supported by\nheavy advertising. It was largely\ndelivered in displays because it was an advance of\nthe regular planogram cycle. And EPC-enabled displays\nwere shipped to stores. And what we found was\nabout 1/3 of the displays that we shipped to stores\nnever got to the selling floor as scheduled. Maybe they got there late, maybe\nthey didn't get there at all. What's important to realize\nis, as a consumer marketing company, we had significant\nmarketing programs, we had invested lots of money\nand significant marketing programs, as did our\nretail trading partner-- things like advertising,\nin-store coupons, promotional discounts, to\ngenerate awareness and generate trial of this new product. Because a large\npercentage of the displays never made it to the selling\nfloor when they needed to, the late execution resulted\nin a 19% reduction in sales. However by using EPC, we're\nable to generate alerts to tell store associates or tell\nour own retail merchandising force that the displays were\nnot where they're supposed to be and get them fast\ntracked to the sales floor so we could eliminate idle\ninventory, we can be in stock, and we can generate\nincremental sales. After that, we did\nanother test promotion with our Braun Cruiser\nelectric shaver. This was done in the\nFather's Day time period in the month of June. This also was a promotion\nthat was supported by significant advertising. It was a very\ntime-sensitive program because it had to\nbe on the selling floor in a limited\nwindow of time so consumers could buy\nthese razors as gift items to give at Father's Day. No one wants to buy a Father's\nDay present after Father's Day. What we found was very\ninconsistent execution of the promotion in the 19\nstores that we tested in. In fact, only six\nstores actually delivered the promotional\ndisplays to the selling floor on time. Five of them put the displays\nout on floor at Father's Day or after. And the majority of the\ndisplays-- eight displays-- were delivered\nsometime in between. What this meant is\nthat the stores that had the electric shavers\nout, ready for sale, on time, had a 61% greater sell through. So the opportunity clearly\nthere for a time-sensitive advertised promotion to use EPC\nto deliver these displays where they're supposed\nto be in accordance with the promotional plan\nthat the retailer established. Now, let me talk to you about\nthe launch of a new Oral B toothbrush from Gillette. This was a great new item. It is a battery-powered\ntoothbrush. We had research showing that\nadvertising would generate strong consumer demand. And that was the results\nwe were getting in all of our pre-market testing. These displays, when they\nwere delivered to store, had a target of\nmaking it from receipt at the back room of the store\nto the store floor in 3.8 days. However, as you can see, the\nresults were very inconsistent. And we were able\nto track this using reads from the\nelectronic product code. On average, the store executed\nthis promotion in 8.8 days-- significantly beyond the target\nthey had set for themselves. What this means as far\nas a business impact is that after the displays actually\nmade it to the sales floor, there was a five times\ndaily sales increase. So the important\nthing to keep in mind is, when the displays were\nsitting in the back room and the advertising\nwas running, they were not working effectively. But as a result\nof having the EPC tag on the displays\nin the test stores, tracking those displays,\nbeing able to respond to automatic alerts\ngenerated from the program, and moving those displays to\nthe floor when they needed to be there, we were able to\nincrease the sales potential five times. In the test stores, the impact\nwas around 600 lost toothbrush sales for inconsistent\napplication, resulting in about\n$3,000 of lost revenue. If you multiply that\ntimes the opportunity across an entire\nchain of hundreds, if not thousands of stores,\nthe lost revenue impact can be a half a million\ndollars or more. So the ability to\nuse it to drive value on promotional\ndisplays is clear."}, {"content": "Lastly, this past\nholiday selling season, we were able to tag a number\nof our fast-moving holiday displays. Duracell batteries, for\nexample, Braun electric shavers, what's known as a\nTag body spray, which is a hot-selling new item\nthat was launched last year, and our Mach 3 power shavers. We tagged these displays\nand delivered them to 500 stores where they could\nbe tracked for compliance. What we found is that 100%\nof the Mach 3 power stores were compliant within\n14 days, versus over 20 days of dwell time in\nnon EPC-enabled stores. The EPC-enabled tag displays\ngenerated a 91% sell through versus a 71% chain\nwide average, showing the impact of EPC\nbeing used to make sure the displays got\nto the selling floor when they needed to. And lastly, EPC-enabled\nBraun palettes had a 30% higher compliance\nto the sales floor than the rest of the chain. So by applying EPC\ntags to these displays, we were able to significantly\nimpact the ability to move these displays to the\nselling floor in accordance with the promotional plan, which\nresulted in incremental sales for the retailer and for the\nmanufacturer, but most of all meant that the customer who\ncame looking for these products because they had\nseen advertising, they had received a\ncoupon, were able to find the product that\nthey were looking for and left as satisfied shoppers. So the rationale for the\ndisplay compliance business case is very simple. It starts, first of all, from\nthe high value of the display versus the small tag cost. It clearly does not make sense\nto put a $0.10 tag or a $0.05 tag on a $0.39 can of beans. However, if you take a $0.15\ntag or a $0.20 tag and put it on a display that has\n25, 50, 100 items, then the cost-value\nratio is much better. We also found from our\nresearch, that 15% to 40% of stores that were\nreceiving our displays were not compliant with the\npromotional plan that had been established by management. With EPC, we can correct that. We can create alerts that\nallow store associates to know whether displays are\nin compliance or not. And if they're not,\ngo find them and make sure they get to the\nselling floor on time. By having displays\non the selling floor in accordance with\nthe promotional plan, we saw a 20% sales lift. And lastly, practically\nspeaking, tagging displays is easier. They're bigger, it's easier to\nmanage physically, read rates and tag survival rates are\nhigher because they don't go through break\npack operations, they're delivered straight\nto the selling floor. So the key benefits\nof moving forward against display compliance\nas a low-hanging fruit are very clear. It's an opportunity for the\nmanufacturer and the retailer to increase incremental sales. Improved availability\nof the product, improved visibility\nof the product, on floor when\nconsumers are shopping, looking for that product, or to\nencourage an impulse purchase. And the fact of the matter is,\nin the consumer goods market, many, many of our products\nare impulse sales. Men go to retail not\nnecessarily to buy razor blades, but they see them,\nthey pick them up. The same with toothbrushes. So having the\ndisplay on the floor generates incremental sales\nthrough impulse purchases. Greater shopper satisfaction,\nwhich drives brand loyalty. The fact that I came looking\nfor a Procter and Gamble product and was able to find it keeps\nme a loyal user of that brand, rather than being encouraged\nto switch to another brand because the toothbrush\nI was looking for, the antiperspirant I wanted,\nor the detergent I wanted was not on the shelf. It's a better\nmarketing investment. It avoids spending advertising\ndollars, couponing dollars, promotional dollars\nagainst empty shelves, or creating excess\ninventory because you built displays that never made\nit to the sales floor on time and have to subsequently\nbe broken down or returned. And finally,\ndecreased labor cost. It takes less time to\nfind lost displays. Time that then can be turned\ninto more productive activities within the retail environment. Display compliance is one of the\nlow-hanging fruit opportunities that we can go after right now\nto create immediate business value. Creating value now to drive\nadoption, to drive tag volume is the cornerstone of what\nProcter and Gamble and Gillette has created as the EPC\nadvantage strategy. This is an evolving\nexecution plan based on our experience,\nour knowledge, and our focus on delivering\nvalue through the use of EPC. Our pilots, our shared learning\nand value confirmed to us that we can help both our retail\npartners as well as ourselves work together to better meet\nand satisfy the shopper, to meet their needs and satisfy them. Our EPC advantage\nstrategy is designed to deliver business benefits,\nboth to Procter and Gamble and our retail\npartners right now. Said another way, we can't\ndo everything at once, but we can move forward and an\never-increasing, incremental way to build value,\ngenerate learning, and then plan future activities. Our EPC advantage\nstrategy consists of tiering our products\nand our product scenarios, like display programs, based\non the value proposition that they generate and\ntheir specific product characteristics. For example, at the\nvery top of the list, the top tier is what we\ncall advantage products. These are higher value\nproducts with strong business cases for EPC. It could include\ndisplay modules which support time-sensitive\nmerchandise like I just described in the\nbusiness case learning. The mid tier, what we would\ncall testable products, these are products chosen to\nsupport continued testing. We haven't wrapped our arms\naround the business case just yet, or maybe some\ntechnical challenges that we hope to overcome. And finally, are the\nchallenge products. These are the ones\nwhere we really struggle to establish clear value. These are the ones that have\nsignificant technical hurdles to overcome. They need more work. Tiering our products\nin this way then allows us to develop\nspecific approaches focused on value creating to enable\never-increasing volumes of tag products. For example, the top tier\nof advantage products can deliver benefits now. These are the products we\nwant to move forward with, we want to move forward\nquickly and capitalize on those benefits,\nand create value. These are scenarios like\npromotional displays or specific products like Crest\nWhitestrips or Gillette blades and razors. The testable products\nneed to drive new learning and they need to be\ntested and piloted between Proctor and Gamble\nand our retail partners to decide the next round\nof products and scenarios that we undertake. Products like Swiffer\nor Braun appliances would fall into this category. And finally, they are\nproducts that are challenged. These need continued research. These need the help and support\nof researchers like yourselves. Manual toothbrushes, Pringles,\nCascade, antiperspirants and deodorants fall\ninto this category for Procter and Gamble. So I'd like to close, now, by\ntalking a bit about the guiding principles for our work and how\nyou can help us in the future. As Simon said, we\nwant to continue to play a role in understanding\nhow EPC technology creates value for our stakeholders. Continue the learning. This is a journey and we're\nvery much at the beginning. It's important that we\ncollaborate with our trading partners to identify, quantify,\nand secure EPC-enabled benefits through process optimization. It's all about changing or\nadjusting our work processes using EPC data and generating\nEPC alerts to tell associates or manufacturer merchandising\nforces how to take action to generate incremental value\nthrough EPC for themselves and increased shopper\nsatisfaction for the customers that come looking\nfor our products. We need to continue to review\nour internal processes. Our current internal\nfour walls efficiencies make internal benefits\nat the pallet and case more incremental to our\noverall effort rather than breakthrough. The breakthrough is really\nthe collaborative space, the collaboration zone\nbetween Procter and Gamble and our trading partners. As you heard earlier, we\nneed to drive reductions in cost, and importantly,\nother EPC infrastructure that's required to\ncreate a total system approach in EPC solutions. We need to support\nthe advance of EPC performance, development, and\nstandards through EPCglobal. Tremendous progress has\nbeen made in this area and AUTO-ID labs have been\nkey in helping to drive that. Generation II is\njust one example of the progress that's\nbeen made, progress that's going to allow us\nto accelerate what Procter and Gamble and\nour retail partners can do in the years to come. Importantly, we have\nto improve read rates and overcome barriers to\nscalable implementation. Good read rates can be\nachieved at different points in the supply chain,\nbut they're not there across the entire system. The applied tag performance\ninitiative through EPCglobal is a great step forward in\nhelping to address this. We're still at a point of\nlimited pilot implementation. We need to know how\nto implement broadly across all of Procter\nand Gamble's products, all of our physical\ninfrastructures, all of our systems,\nall of our customers. And finally, we want to use\nthe EPC advantage strategy to secure ongoing benefits. Finally, how can you help us? You heard it before\nfrom the other speakers, low-cost tags are critical. We'd like to get to\na $0.01 or less tag as soon as we possibly can. Increase tag quality,\nrobustness, and longevity, better understand electrostatic\ndischarge and other factors that might impede the\nlongevity and the functionality of our tags. Tag deactivation\nsolutions are important and how they fit into an\noverall EPC architecture. Solutions to incorporate the\ntag into corrugate materials are a very, very\nimportant way forward that will help us lower\ncost and streamline the implementation of EPC. We need system solutions\nbuilt into EPC appliances, so we don't have to rely\non multiple integrators to help us deploy\nour implementation. Spontaneous real-time exchanges\nand distributed EPC information management is important--\ndiscovery systems that allow us to take a product\nthat's off track, that we don't know why it's where it is,\nand find out what it is, where it belongs, and get it\nback on track in the system. Further robust studies on\nEPC-enabled retail availability improvements. And finally, consumer education\nand informed public comment on the use of EPC. All of these, in time,\nwill help deliver the vision we have for an\nEPC-enabled supply chain that can literally transform\nin a step-change fashion the way we can go to market\nas Procter and Gamble with our retail partners. Thank you."}, {"content": "[APPLAUSE] [CHATTER] [? GUEST SPEAKER 4: ?] While\nhe's hooking up your laptop, I'll-- GUEST SPEAKER 1:\nYeah, thank you. Is this battery here, or-- [INAUDIBLE] GUEST SPEAKER 4: Yep. GUEST SPEAKER 1: OK. OK, thank you. GUEST SPEAKER 4: It's a little\ntight on the [INAUDIBLE].. [CHATTER] GUEST SPEAKER 1: Good\nmorning, ladies and gentlemen. I'm very delighted to\npresent DHL experiences with RFID technology\nhere at MIT in Boston. I'm working in DHL in Germany\nas sector head for consumer retail, and I will focus in\nmy report on these subjects. Listening to the requirements\nof our CPG fashion and retail industry,\nwe design and implement supply chain solutions for\nthe benefit of our customers. RFID is one important enabler\nwho can support our supply chain solutions. Let us start with the main\nopportunities for RFID, which we see for supplier logistics\nservice providers and retailer in the supply chain, followed\nby the executive summary about that subject. The main opportunities for RFID\nin the supply chain-- and I have shown here the supplier\nplant, the DC of the logistics service provider, the DC of\nthe retailer, and the outlet. And the green color is the\nopportunities for the supplier, and there we see the\ncounterfeiting protection, which we very often see\non the fashion side, the enabling of tracking\nand tracing on item level, and the better customer service\nthrough proof of guarantee and proof of delivery. In the warehouse, we can\nsee out of our trials that we have more efficiency\nin the warehouse management, we have a better flow\nof goods, and we have faster and securer inventories. We have optimized picking\nin value added services, in kitting of textile\nand apparel, and consumer electronics. And we have all full\nvisibility of each single case. If we look at the\nDC of the retailer, then we can see a clear and\nbetter warehouse visibility. They can manage a higher\nnumber of stock-keeping units to reduce the stocks and\nminimize the shrinkage. The cross-docking\nshipments can be optimized, and we have a better and\nefficient recall of goods through RFID. On the shop floor, there is the\nreal-time monitoring of sales. The last 50 yards are\nmuch better managed, efficient supply from\nback store to shelf. There is a permanent monitoring\nof goods in the outlet, and for the shopper\nconvenience you can have technical assistance\nlike a personal shopping assistant who is giving\nyou your last buy what you have done in the store. And last, not least, speeding\nup the check-out processes in the cashier area. Out of that, we want to give\nthe executive summary up front. We see RFID is the\nsolution for the future. We could see in our different\ntrials cost reduction, quality improvement, and\ninnovative new services like [? tech ?] [? fit ?]\nwhat we have done. The entire supply\nchain will benefit from the use of\nthis new technology after complete implementation\nand global coverage. We know that barcoding\nhas needed about 25 years, and we see that we have\nto go quite a long run, but we are not looking\nat the 25 years. RFID will improve the\nprocesses in the supply chain. This is for sure. We could prove fast\nand accurate handling in unit tracking speeds up the\nlead time in the supply chain. Secure data and shipment quality\nprovides more transparency, and continuous test\nin various pilots leads to stability\nof subprocesses. But RFID today cannot be applied\nfor all logistics processes. RFID technology\ndevelopment is not mature yet to steer the\nentire supply chain. There are still major\nissues to be solved, such as harmonization\nof standards, 100% read rates, and technical\nissues with liquids and metal. What is the supply\nchain today like? And I'm referring now on\na barcode supply chain. You can see here on\nthe top of the chart. You see the product flow,\nthe physical product flow from the manufacturer via the\nDC and the cross-docking point of the logistics\nservice provider to the DC of the retailer\nand then to the outlet. On the bottom part\nof that chart, you see the information\nflow from the data warehouse of the manufacturer via\nlogistics service provider to the retailer. And you see there is\na lot of processes to be done on the physical flow\nand on the information flow. I will not go deeper in\nall of these processes, but I want to show you on\none process, the loading process, the difference\nbetween barcode and RFID. You see here there\nis a pallet loaded in a warehouse on\na truck, and you can see that this is\nautomatically scanned. And then when you see when the\ntraffic light went from red to green, it is at the\nsame time booked out of the warehouse management\nsystem into the truck. Later on, the retail partner\nreceives automatically a [INAUDIBLE] in dispatch\n[INAUDIBLE] format. If you look at the same\nprocess in RFID process, then you can see there\nis a small difference. You have full visibility\nof each single case. You can see here that the\nreader has read all the cases on the pallet, and we\nhave here one special case that the driver is leaving\none box on the shop floor. And here you can see now all\nthe other pallets are loaded, and the only thing. The black box was not seen. It went to the gate. Now the dispatcher is\ncalling the driver, and the driver has to return. And he will run back and-- [LAUGHTER] And he takes that black box,\nbring it through the reader, and now you can\nsee the reader has seen that everything is loaded. And here [INAUDIBLE] clear the\nspecial delivery is now loaded, and he can go to\nthe retail site. What can we learn out of that? We have the full visibility\nof each single case, which we don't have on\nthe scanning side when it is only\npassing the scanner. We have a completeness\ncheck, and we have a fast allocation\nof not-identified goods by a simultaneous check. What have we done in DHL? Which RFID pilots? We have done a pallet\ntracking, case tracking, and item tracking. We have looked at inventory,\norder fulfillment, and condition monitoring\nfor temperature, humidity, and shock and\nvibration, and we have looked at value-added\nservices which we can do out of the RFID technology. I will not go to\nall the industries. I will only touch now\nthe fashion, retail, and CPG industry in\nthe following pilots. What we have done\nin three pilots was, the first one, flow\nsteering of multi-use trace, then a fashion pilot\nin France, and together with metro in the\nFuture Store Initiative, we have done another\ntrial in Germany. On the flow steering\noff multi-use trace, in our telecom warehouse\nwe have tagged multi-use trace with two tags, and what\nwe learned out of it-- or what was the test results. We got a 100% tracking\naccuracy inside the warehouse, and we could reduce\nthe assets by 50%, and we had 20% faster\ncontrol processes. On the pilot fashion\nlogistics in France, there we tested hanging\ngarments and flat packs, and there on the control\nprocess of hanging garment we could see we are four\ntimes faster when we take in the goods than was barcode. We could do the inventory\nof 20,000 hanging garments in 30 seconds, and we had\na significant improvement in control of flat packed items. What were the key learnings? We learned that we had to\nlimit in the carton 40 pieces. There we need\nfurther improvement, and we were not clear if HF or\nUHF is the right technology, and the customer once\nthere are resolved. And on the fashion items,\nwe have quite a lot of privacy issues in\nEurope, and we are hard working on it with\none group in Brussels and one group in Berlin. The third pilot was\nmetro future store, and I must say here that we have\nonly generation one in Europe at the moment. We are longing for that\nwe get generation two, but we think they are\nstill here in America and they don't want to\ncome over to Europe. [LAUGHTER] And pallet tagging only shows\nnearly no improvement out of our learnings and to compare\nto barcode on the manufacturer and the [? LSB ?] site. But the key learnings\nwere pallet tagging is a useful step\nforward in order to focus on the\nstandardized supply chain. You can exchange master data,\nEDI messages, and delivery control with your partners,\nand you can do the information flow, and the alignment\nwith a business partner is one of the key\nprerequisites to go forward. Another learning which we\nhad was the smart label. We call it smart\nlabel, and it is a label which is a flack label. It's a little bit\noff the pallet, and this improves the\nread rates to 90%. What is the benefit allocation,\nwhat we found out through RFID? We could say to work with\na pallet was a good start, but we didn't see significant\nadvantages against the barcode. When we went then\nto the box level, then we could see\nsome advantages on the physical subprocesses\nand on the control side, and the best results we got\nwhen we went to the item level, and they're on the\norder picking side, and on the stock optimization\nwere the best benefits. What are the main issues\nfor RFID as a supply chain? We could see that the\nmanufacturers have very often their RFID labeling on\nbox level separate as a separate operation. There are still technical\nissues to steer the production, combine the primary\npackage with a lot number, and there are still\nhigh investments for RFID infrastructure, tech\ncosts, reader, and middleware and we are now changing\nfrom gen 1 to gen s. There are investments again. There has to be\ndefined appropriate IT tools for enterprise resource\nplanning and production planning systems. On the logistics\nservice provider side, we see again high investments\nfor RFID infrastructure. If you want to have\nyour warehouse ready, you have to tech the locations. Reader, middleware, and\nothers needs to [? buy. ?] We have to define appropriate\nIT tools for control systems, and what I showed you\nin our fashion trial that the data capture\nof bulk were still limited to small quantities. On the retail side,\nwe see there in Europe the read rate below 100% and\nonly a few goods are tagged. Item level tagged goods\nare only in pilots, and the read rates\nfor mixed palettes, which we use in Europe,\nare still below 100%. There are privacy\nissues for the consumer, and a lot of the employees are\nanxious to lose their jobs, especially on the cashier side. Then we have to solve the\ndisposal of used RFID tags. What is our assessment as\nlogistics service provider for RFID? We want to be in line with the\nnecessities and prerequisites of integrated supply\nchain management, and we are clearly\nsaying we have to adopt the RFID technology\nbecause on the long term we will have a\nlot of advantages. To share the investments\nand implementation costs on the short term, I\nthink we can bear it if we see on the long term\nthe profit perspectives. However, what we see-- the\nmain potentials of efficiency and cost saving due\nto RFID implementation in the supply chain will not be\nrealized by logistics service providers. We see more advantages on\nthe retail side and maybe on the supplier side as well. Our RFID positioning today\nis because we have clearly seen the tech lab label\ncosts are going down. The performance\nis getting better, and I think we as DHL\nare between deploy and implementation, and we\nwant to go further right. DHL after the purchase of Excel\nis now the biggest company in the logistics market. We are market leader\nin ocean freight, we are market leader\nin air freight, and we are market leader\nin contract logistics. This area is a very\nfragmented market. We have only there 5%, but we\nwant to redefine that market, and we think RFID can help us. And therefore, we as DHL\nhave a very strong commitment to EPCglobal because,\nas I have shown you, DHL has to serve\ndifferent industries with different\nproducts and services in all geographies of\nthe world, and we have to face different standards. We need one global standard\nused by different industries for all products and\nservices, and therefore our commitment to\nEPCglobal is very strong because we believe\nthis organization can be the enabler of our vision. Thank you for your attention."}, {"content": "[APPLAUSE] Yeah. [CHATTER] GUEST SPEAKER 4: OK. Brian, just remember we\nhave about 20 minutes. GUEST SPEAKER 2: Yes. GUEST SPEAKER 4: I think\nBrian's got like 200 slides. GUEST SPEAKER 2: I'll\nonly show half of them. GUEST SPEAKER 4: All right. GUEST SPEAKER 2:\n[INAUDIBLE] All right. I'll set myself up. OK."}, {"content": "Good. So, before I start, I\nwanted to make a disclaimer because, as you know, this is\nbeing taped for OpenCourseWare, which means the audience\ncan be very big, and actually I'm afraid of what\nmy great grandchildren will think when they see this because\nit will be there on the web. I just want to tell\nthem that I know I'm not going to say things\nthat will be true, OK? I'm going to make\nsome wild claims, and I just wanted to warn\nthem when they see this. OK? And I guess in passing I'll also\nwarned all of you, but anyway. OK, good."}, {"content": "So, let me start by summarizing. I added in the\ntitle one thing that was not in the agenda, which is\nbuilding an academic discipline with a question mark, because I\nthink that's one of the issues that we need to\ndiscuss as a community. I'm leaning toward saying yes."}, {"content": "But if you had\nasked me last week, I would have probably said no. So that's probably why\nI put the question mark, because I'm still not sure. But I thought in terms of\nbeing a bit provocative. Let's take the\nposition that, yes, and then let's see\nwhere this takes us. OK?"}, {"content": "Good. So this is the session that\nwe had today, the people, and if I summarize what I've\nheard from industry thus far, I think the retailers\nare sort of seeing these as a serious opportunity. It's sort of like an\niterative process."}, {"content": "It's evolving. There's many different\napproaches that they are using, many different things\nthat they are trying, but I think what's important\nis that they are extremely collaborative with\ntheir partners but also with academia. So I see this as an opportunity. They obviously need the\nsupplier to help them. They're trying to focus on\nareas where they see some gain, but they need this help\nwith these deep issues that I've listed here, which\nare areas where I think we academics can help them. I think very important in\nbuilding their business cases, but there's a few other areas\nwhere they're talking about. What's important also is\nthat, even though this is sort of a supply chain focus,\nI think what's important-- and that's another of the\nclaims that I wanted to make-- is that they have a very\ninterdisciplinary need. If you look at the list of\nthings they need help with, they're just not\noperational things. They need things that have to\ndo with things that all of us have been talking about\nover the last two days, and I will talk\nabout once we finish. So I think that the supply chain\nneeds interdisciplinary help on many, many issues. OK? And if we look at\nmanufacturers, you know, I think there's been again\na focus on the supply chain, and I find this interesting\nthat the manufacturers-- and I haven't seen\nany manufacturer really thinking about\ntheir products changing. They're still\nthinking, OK, I'm going to manufacture the same\nthing and just how it works through the supply chain. And if I look at retailers,\nit's a bit the same thing. It's not like Walmart\nstores are going to change. They're just going\nto be more efficient. There's going to be\nless out-of-stocks, OK? But the reason I think\n[INAUDIBLE] of further change, and I think we as\nacademics can maybe push a bit the envelope\nbecause I think both problems that our\nmanufacturer will change because the channel in a way\ndrives what is sold through it, and also I think new store\nformats we will emerge. And in terms of where\nmanufacturers need help, I think it's very similar\nto what retailers need. So I think the needs\nare very, very aligned, and it's not that there's some\nthings that help manufacturers and others that help retailers. I think we're getting a\nvery consistent message from everybody here as to\nwhat are the areas where we should be doing research. OK, good. So, in many ways, I think\nwe're at the very beginning of an industry, and I\nuse that as an analogy. And this, they used to call\nit a horseless carriage, very much like some people still\ncall RFID a wireless barcode. And if you think about the\nchanges that had happened or that happened after the\nfirst cars were invented in over the last 100 years,\nyou can see the kind of changes that we will see\nwith RFID, and I think it will be actually a\nbigger change than that one. OK? So, here's a couple of examples. Also with advertising we're\nseeing lots of changes. This is one of the\nfirst steamboats, and we had steams and sales. OK, so many, many changes. Here again, just to\nsummarize, I think it's important that\nthe supply chain needs a lot more than just\nsupply chain research, and I think that in many ways if\nwe figure out the supply chain we'll figure it out everything. That's why I'm saying\nthis is NP hard. It's sort of borrowed because\nI'm a computer scientist. In computer science,\nthey say problems that are NP hard-- if\nyou solve one of them, you've solved all of them. So I think that happens\nwith the supply chain. And also [INAUDIBLE]\nBased on what I've seen, I think it's the supply chain\nproblems are written off, but if you solve problems\nin the supply chain you'll probably be able to solve\npretty much any other problems. But, again, question\nmarks because I'm not really positive. And if you look at\nhow we started-- as a line borrowed from\nSanjay and a lot of the work I've been doing\ntogether with him. You know, if you look\nat the opportunities, you can see also that\nthis matches beat what the supply chain needs. OK? So I think there's\nhere a nice match, and that's why I'm suggesting-- let's see if we can\ngo to the next one. OK, seems to be stuck. It's a very good slide. I know."}, {"content": "The computer is smart. OK. AUDIENCE: [INAUDIBLE] GUEST SPEAKER 2: Excuse me? AUDIENCE: [INAUDIBLE] GUEST SPEAKER 2: No. Well-- AUDIENCE: [INAUDIBLE] GUEST SPEAKER 2: OK, good. So, that made it."}, {"content": "OK. So a suggestion I would\nmake to all the researchers here is to use the supply\nchain as your laboratory, and I think they\nseem to be open, and I think they've\nproven they're open. I think it's sort of\nlike an NP hard problem, so I would suggest we do that,\nand that's what we've done, and I'll give you an\nexample of some research that I've done that I started\nin '97 by looking at a supply chain issue that\nI think apparently doesn't have to do a lot\nwith the supply chain, but, as you will see, it ended\nup having a lot to do with it. OK? And just to begin, some of\nthe issues with the supply chain that, again,\nSanjay already mentioned. You know, there's\nlots of issues, but perhaps the place where it\nseems to be harder to work on is at the edge of\nthe supply chain. That's where the problems\nare a lot bigger. Customers come in, and they'll\ndo all sorts of nasty things to you, so that's where\nI think it's very hard, so I think that's a\nresearch laboratory if you want to think long\nterm what may happen. If you want to\nthink short term, I think there are some areas\nwhere they are better, like some of the portals that\nwe saw with DHL and receiving. I think there may be easier\nto do progress there, OK? So one of the areas\nwhere I did some research was trying to look\nat a grocery store and see here what were some\nof the legal issues that would have to be overcome. And so here's a little video."}, {"content": "You see a little grocery cart. We actually implemented\nthis and did some tests, and the products\nobviously have RFID, and they send a little\nadvertising because they know what's on the cart. Right? So we said, OK,\nlet's try and analyze what are some of the\nlegal issues here, and we came up\nwith a list, which I think you can then go\nto many other applications and say, OK, what\nare the legal issues of this sort of embedded\nintelligence or this product internet. And it turns out that they\nhappen to be a subset of this, OK? So that's why I think it's\nuseful, and so we have at list. We actually even wrote a\nbook and a bunch of papers on some of the issues, some of\nthe legal issues that happened. Here are some of the issues\nthat we came up with. So again, I think that\nworking on the supply chain, you can really do progress\nthat can help RFID in general, and again here, if we\nlook at the opportunities, I think what we are seeing-- and Sanjay had these three\ncolumns, the technology, the applications,\nand the analysis. I don't know if these are\nthe three columns that we should use going\nforward, but I think it's a good starting point. What we see, even on this little\nexample on the supply chain, is that these three disciplines\nare somewhat related. So, the technology\nthat you use, depending on what kind of\nRFID tags you use, you can think of one application\nor another application. So that's sort linked the\ntechnologies [INAUDIBLE] the applications, and\nthe kind of analysis whether legal issues\nare important or not is also related. So I think that one needs\nto look at the three things, and that would be one of\nmy bases for suggesting that we need a discipline. But before doing\nthat, let's see."}, {"content": "Maybe we need to go to-- ah, you\nwere right here on the arrow. It really doesn't\nlike this slide."}, {"content": "OK, good. So, the other area\nwhere we did some work was on the supply chain, and we\nactually looked at receiving, and we looked at processes\nin different warehouses and in different stores\ncollaborating with Walmart, Gillette, and many others,\nand trying to understand how things worked. And what we did is a very\ndetailed almost maniacal kind of analysis, where we were\nobsessed for understanding everything, and actually even\nwith Richard and a few others we spent hours videotaping\nand mapping processes. And on a simple\nwarehouse, we had thousands of processes to\ntry and really understand what could change. So we don't really have\ntime to go in detail, but we did learn quite a bit. We have a paper that details\nthe methodology that we used, but that just by looking\nat one warehouses and just by looking in\ndetail at something, you can really learn\na lot of things. Just like I showed you what\nwe learned on the legal side just by looking at\nthat little grocery cart in that little example. OK? But we did the same\nthing with a warehouse, and basically just by analyzing\na warehouse you can come up with a bunch of areas where\nthere can be improvement, and those are areas where RFID\ncan help the supply chain, and they've been\npublished quite widely. But here also again\nanother way of looking at the different areas where\nthere can be improvement, and that's again to prove the\nvariety of issues that can be researched on the supply chain. That's again a slide that\nSanjay showed yesterday, and I think the nice\nthing also about RFID is that it can really\nhelp you do a very detailed analysis much more\nso than you could before. And I mean our inspiration\nfor some of that work was the scientific management,\nthe work that Taylor did almost 100 years ago\non really understanding the details of shoveling. He did these sort\nof studies on how you shovel to try\nand design what's the perfect shovel, right? So what happens is\nthat with RFID-- and that's something that\nwe've learned recently is that RFID really that's the\ntime-in-motion work for you. So you no longer need to go\nwith a camera and videotape and analyze because RFID\ntells you what happens. So I think that there are\nmajor areas for improvement that can be researched. Now, after doing\nthis analysis, we tried and go a bit forward and\nsay, OK, how will the world be? Now that we've seen\nin a few places how things are, let's try\nand look for the future, and our rationale-- I mean, imagine that fellow\nin the car, Mr. Ford. If Ford could have come\nto the Harvard Bridge and Ford had sat there in the\nHarvard Bridge for 10 minutes, maybe 15 minutes, jump\n100 years and then go back, just in those\n15 minutes he probably would know how the world was. If you just have a snapshot of\na small place, how it evolves, you really can tell a\nlot about the future. And I think here, if you\nwork on RFID, probably-- and on the supply\nchain, which is where I think there's the\nmost advanced applications, by looking at one small\nplace in the supply chain and really seeing how RFID\nimpacts that small space you can then make some claims\nas to how the future will be. And that's what we've\nactually try and do by looking, having some\nexperience in a few places with very deep experience, and\nso these are some of the things that we think will happen, and\nwe probably don't have time. I think a trend that\nwe believe will happen is for increased fragmentation. So there will be a\nlot of fragmentation, and the supply\nchain will go more from a sort of supply-dedicated\nline of products that you ship to more of\nlike a packetized network, if you know what packetizing\nis in telecommunications. So we think there will be\na lot more fragmentation, and we also think that there\nwill be new store formats, and a product may become\na store in itself, and you may have stores that\nonly sell one product, which is itself, because with RFID if\nyou encapsulate everything you can have all the accounting. You can have a little ERP\nsystem associated to a tag, and that's [INAUDIBLE]\nto the end in a way. [INAUDIBLE] the to end. OK?"}, {"content": "Good. So, again, this\nis another example where by looking at\nbusiness cases you have to make assumptions, or\nby doing the analysis you make the assumptions\nof the technology and then you need to come\nup with the application. So again, all these things\nare very, very related. So if we think-- OK, same problem. Good. So then a question that we have\nif we take this view that we need to work as a group-- you know, I think the question\nis how we collaborate. And I think some of the\ninitiative, the [INAUDIBLE] Initiative that\nMIT has also been involved and a bunch of the\npeople here in the audience. I think that may be an example\nof how we need to collaborate, so do more of these\nevents and try and see what are the different\nareas that we can work on, try and divide the work, and\ntry and make progress together. Because I think to me\none of the messages of the slide that Sanjay\nshowed is that there is too much to be done almost. It's like there is work\nfor the next 100 years, so let's try and organize\nit, and let's try and build a research agenda. So these are just\nslides from the Peloton that we don't have\ntime to go through, but the idea here is\nthat different players from the industry are trying\nto collaborate and see how things are evolving. And [INAUDIBLE] over there is\nlike the expert on this right now. So he can tell you\nall about it, and this is all the different players\nand how they see evolving, and it'd be nice\nif we had something like this for academia. What are the different\nareas of where we're going to do research? What are the problems we're\ngoing to try and solve? I mean, Hilbert\ndid this in math. You know, he came up\nwith the Hilbert problems for the 20th century, and some\nof them are still not solved. OK?"}, {"content": "So very, very briefly. So when is it necessary to\nmake an academic discipline? So let's say we wanted\nto make a discipline. What are the things\nthat we need to have? Right? So I think obviously it needs\nto be a fun thing so we all want to work about it, so we can\nget all these tenured people to collaborate,\notherwise they won't. So then there needs\nto be relevance, which there needs\nto be something that really makes sense. There needs to be some\nform of rigor to it, and then I think very\nimportant is the distance to other disciplines. I mean, is this\nsomething different that deserves its own\nspace, or is it just sort of an incremental change? And then we need\nthe community, and I feel that that's something\nthat we can really have, a community. And I see people-- I have colleagues. When my colleagues start\ndoing research on RFID, they can't stop. It's very hard to find people\nthat stop working on RFID. Once they start,\nthey stay with it because it's very fascinating,\nand there's lots of stuff, and I think there's\na business interest. So if you take, for\nexample, civil engineering, or architecture, or even\ncomputer science or AI, I think we have something\nsimilar to those disciplines. You know, if architecture\nhas its own discipline, isn't this something a lot\nbigger than architecture? This is going to change the\nvirtual world in a way that's, I mean, not even funny. If you look at computer\nscience, all computers have all been linked so\nfar to screens, right? And all of a sudden RFID now\nbrings computing to the world, and the whole world\nis a computer. If that doesn't\ndeserve a discipline-- I mean, so that's my view. OK? I may be wrong. So that's my instinct, that we\nshould create a discipline, OK? And obviously like civil\nengineering and architecture, we're going to draw from others. I mean, it's not like we're\ngoing to do everything from start, and I\nthink we need to start close to the real\nbuildings like architecture and civil engineering. And I think the best\nplace is the supply chain, so that will be my\nsuggested laboratory. OK?"}, {"content": "And obviously we need to define\nthe research agenda, and so now my final slide. In terms of next\nsteps, I think we need to start with\nthese Hilbert problems. What are the big problems? And I think we need to\ntake Sanjay's three columns and decide whether\nit's three or four, but then I think we need\nto go across and say, OK, what are some of the\nthreats that we can work on? And I think we need to do it\nwith industry and EPCglobal, so close to reality,\nand then we need to build the academic community\nand grow the discipline. Anyway, that's just my thoughts."}, {"content": "OK?"}, {"content": "Thank you. [APPLAUSE] GUEST SPEAKER 4: Thanks, Brian. GUEST SPEAKER 2: You're welcome. GUEST SPEAKER 4: OK. We actually have built in\nsome time for questions, so I'll open the\nfloor to questions, and, as we did yesterday,\nif you would come down to the microphone. AUDIENCE: Oliver Hedgepath,\nUniversity of Alaska. From all of your academic as\nwell as business discussions just now, it jumps out\nat me that maybe there are going to be in the future-- again, Brian, if we look\nahead like Henry Ford from the horseless carriage to\nnow the automobile, the Hummer, and all the different\nterms and metaphors we use. Is there a new metric? Is there a new measure of\nperformance that we're missing, or is there a metric\nwe should think about retiring in the future? Henry Ford didn't see the\nmeasures of performance we use today in the\nautomotive industry. So I'd like to hear from\nSimon, you, all of you. Any metrics that are occurring\nor changes to the units such as measuring inventory\nby days versus inventory by second? Any comment?"}, {"content": "GUEST SPEAKER 3: Do you\nhave any suggestions? AUDIENCE: Yes. [LAUGHTER] GUEST SPEAKER 3:\nI knew you had-- AUDIENCE: But I'm\nnot on the panel. [LAUGHTER] GUEST SPEAKER 3: But go ahead. Say your suggesting. AUDIENCE: [INAUDIBLE] GUEST SPEAKER 3: OK. [INAUDIBLE] GUEST SPEAKER 5: I think that-- yeah, this is working. You're right that the metric\nwill change as we move forward. I think right now in\nterms of the metric on readability, read rates,\nappropriate tags, antennas, that sort of thing, that's only\nin its infancy of developing those standards. And I think we've got to\nlearn from the barcoding industry of printing barcodes,\nand readability of barcodes, and how that transferred\nfrom the initial stages to business as usual\nof the business sort of working with\nsuppliers on standards for barcode technology. So we should learn\nfrom the past."}, {"content": "You're right. We've got an opportunity\nto fast track, but in terms of setting\nthose benchmarks, I think we're at a\nvery early stage. And in terms of measuring the\nsupply chain and efficiencies, you know, I guess we don't\nknow what we don't know today. And we're starting to see\nthe first signs of that. And a few suppliers\nthat we're working with are starting to see\nsome of those wows and opening their eyes to what\nthey believed were average lead times and actually seeing\nnatural lead times, which identifies opportunities\nthat may or may not be solved with RFID,\nbut at least it's showing where opportunities\nexists, where they can actually then go and change a process\nto speed that product through. GUEST SPEAKER 4: If I can\nadd one thing to what-- if I could add one thing to\nwhat Simon was saying, I think-- because he said\nwe don't know what we don't know at this point. And I somewhat\nquote Gary Cooper, the CIO at Tyson when he\nsaid the aha moments are yet to come. And I'll use an\nexample from some of the early work at\nWalmart that we noticed when we started looking at the data. What we call the case cycles,\nthe movements of boxes from the back room to the sale's\nfloor and then back again, that wasn't anything or-- Simon, correct me if I'm wrong-- that we kind of went\nin looking to say, hey, let's see how many\ntimes this is happening. The data started\ntelling us this. Well, that actually\ncould be now a metric of the efficiency of the\nshelf-stocking function that we're seeing\nthese boxes going out. But beforehand, nobody had\nwritten down anywhere, oh, let's look at case cycles. That was revealed to\nus through the data, and I think we've got a\nlot of that yet to come, and I think again I'll go back\nto the slide that I had on whether we view things as\nincremental or radical. Especially in that\nradical category, I don't think we yet know what\nsome of those metrics may be, and I think they'll be revealed\nto us as we understand the data and how we can deploy\nthat technology. GUEST SPEAKER 2: Yeah. I think in terms of metrics. I published a paper in\nlike '98 forecasting what's happening with\nGoogle, these metrics where every little ad has its\nown metric and you measure everything. And I think that's\nwhat's going to happen. So we are going to micro\nmeasure absolutely everything, and we're going to know whether\nis it worth it to take it right or take a left, and\nassociates will know should I go to the right or to the left. What is it more likely? That I'm going to generate\nvalue to do zoning, and we're going to make\nsure absolutely everything. And I think the metric will\nbe sort of event-driven. Anything that you do will get\na metric of how positive it is, and you will have\nall these computer scientists helping come\nup with measures and ways to improve it. Yes."}, {"content": "AUDIENCE: All right. Thank you. GUEST SPEAKER 4: Thank you. Question over here. AUDIENCE: [INAUDIBLE] I'm\n[INAUDIBLE] from supply chain and information\nsystem at Penn State. My question is\nabout the actions-- in particular, remedial actions. Every speaker's talked about\nchanged business processes, so I want go narrow. Can you tell me examples of\nsome actions in those business processes? So, Dick mentioned\nif you see some alert on promotional items,\nyou can call them. I would call that remedial\nactions, whereas in the health industry the pedigree\nrequirement, I would say it's changed business\nprocesses, but it's mainly recording the history. It's trace and track, and I\nwant to add that it's not just operational-level actions. At the strategic-level actions\ntoo again use Dick's example. If you can alert and you\ncan see the displacement, maybe that means you can have\na different kind of contract on promotions at the strategic\nlevel with your retailer now that the supplier\ncan see better. So, again, my\nquestion is can you tell us some actions\nand remedial actions that you can take?"}, {"content": "Thank you. GUEST SPEAKER 4: So,\nthis would probably be for Dick, Simon, and Klaus. What are some actions\nthat you've actually taken based upon data or process\nchanges that you've made? GUEST SPEAKER 5: Let me just\ncomment on the observation you made around [INAUDIBLE]\nthat Dick referenced. In the proof of\nconcept in the trials, yes, you would react\nto those situations, but going forward\nwhat that identifies is how we can be proactive. And so we're not always\nchasing and firefighting, and so that on\npromotions, for example, instead of getting to a\nsituation where today is D-day, and that product should have\nbeen out on the sales floor and it's not, actually\nusing the data to count down and to alert in advance to\nstore management and associates. So a week out it's, OK, you're\ngetting prepared for this. All the merchandise is there. In two days out, OK, that\nshould be moving out, and it's not, so that we're\nactually more proactive and helping the\nassociates and store management get that\nproduct out on time rather than just reacting. GUEST SPEAKER 3: I would\nagree very much with Simon. I think it's a combination,\nand you kind of answered, I think, your own question by\nsaying we're using the data, we're using the alerts\nto give associates information proactively in\nadvance of the occurrence. And then it comes\ndown to training them to use new equipment,\nhandheld readers, which give them the ability\nto do their job faster, more accurately. GUEST SPEAKER 5:\nThe other thing I would add as well\nis that we talk about the out-of-stock study\nthat the U of A conducted and the results\nwe saw from that, but what that doesn't show and\nwhat is really hard to measure is how many out-of-stocks\ndid we prevent. How many products did\nwe save from going to the last two or three bottles\nof a product on the shelf? Which then maybe\ndented, or nobody wants to pick the last\nitem off the shelf. How many of those\ninstances did we avert? And keep that shelf\nfuller and the display fuller, which we all know then\nsells more of that product. So I think we are being\nproactive in terms of that and obviously for those\nfast-selling items or things that didn't\nget executed correctly. Then we're getting that\nproduct out much quicker to fill that gap as well. GUEST SPEAKER 4: Klaus. GUEST SPEAKER 1: And\nfrom our point of view is in former times it was only\na game between the supplier and the retailer, and\nthe logistics service provider was not existent. And if you have then a proactive\nway that you bring forward, that you go into that\ntechnology that you learn, then you can show that\nyou are connecting a binding medium and to help\nthose to improve their supply chain. And this is what we learned\nout of the overall RFID trials, and the other one\nis that you have to work very close together,\nyet you have exchanged data. The biggest problem\nin the beginning was to have the right data\navailable to start for RFID. Yeah."}, {"content": "GUEST SPEAKER 3: OK. I think also having\nbetter accuracy is going to lead to more\naccountability and better forecasting. For example, one of the\npromotional examples I used-- promotion didn't do. Well, did it not do well\nbecause it didn't get out to the floor on time? Or did it not do well because\nit didn't attract the consumer demand? Having accurate\ndata which allows us to execute the\ndisplay flawlessly is then going to\nallow the manufacturer to be more accountable\nto the retailer for all the right kinds of\nproducts and promotions, the retailer more accountable\nto the manufacturer, and together that will create\na more ideal forecasting mechanism. GUEST SPEAKER 4: OK."}, {"content": "Thank you. Harold. AUDIENCE: Hello. My name is Harold Beck from\nthe University of Sherbrooke. I'm a professor of marketing. First of all, I'd\njust like to say I'm really excited about\neverything I heard today, and I have so many questions\nmy head is spinning, but, Bill, I'm only going to start\nwith one question right now. OK? [LAUGHTER] So I'm really glad to see\non a retail point of view the diffusion of the RFID\ntechnology within the store, and we're going more and more\ntowards promotions eventually helping customers do\ncross-selling and up-selling opportunities. Now, I know a while back\nGillette did a pilot with RFID tags and the whole\nCaspian movement of consumers against supermarket privacy\ninvasion and numbering. They started boycotting Gillette\nbecause of that initiative, and I know also Walmart had\nsome difficulties with consumer tracking within their\nstores a while back. Now, my question\nis do you foresee in the near future\nusing RFID in order to interact with consumers. And the second part\nof the question is, what are you doing\nagainst the possible push-back from different consumer groups? GUEST SPEAKER 3: Well,\njust two quick answers. I think right now it is a\nsupply chain technology. It is a technology that's\ngoing to better enable us to satisfy shopper\nneeds through supply chain improvements. So, no, we're not thinking\nabout what comes beyond that. Secondly, over\nthe last few years through the good work of\nEPCglobal and EPCglobal subscribers who sit on\nthe public policy steering committee, principles\nand guidelines have been put in place\nfor the responsible use of the technology. GUEST SPEAKER 5: I'll\njust add to that then. I think it is around\ncommunicating that message, educating our associates and\ncustomers of why we are using it, and how it's\ngoing to benefit them from us using it in the supply\nchain to better serve them. AUDIENCE: Do you see within\nthe next maybe 10 years the possibility of having enough\neducation among the consumer in order to be able to use that\ntechnology to help them be more informed about their purchases? GUEST SPEAKER 5: I\nthink like the EPC and the logo will\nbecome ubiquitous, much the same as Intel inside. You know, consumers\nwill relate to that and see the advantages of that\ncase or product having that-- AUDIENCE: Thank you. GUEST SPEAKER 5: [INAUDIBLE] GUEST SPEAKER 3: Question?"}, {"content": "AUDIENCE: Hi."}, {"content": "I'm John [INAUDIBLE]\nfrom Mars Incorporated. So my question's about\nthe benefit case, and if we take Dick's\nkind of diffusion model and cross it with Klaus's\nsupply chain model and you look at the\nbenefits and the costs, they seem to be\nlumpily distributed through that matrix. So, one of the\nbarriers to adoption is the dislocation or\nthe benefit and the cost, and I know that's\nstarting to cause slowing the rate of adoption\njust even thinking about it, and not even working\nbut thinking. If that's a barrier to thinking,\nit seems to me one of the areas and if the combination between\nthe cells in that matrix are really where the benefits\nare in collaboration. So I'd like you to\nanswer two questions."}, {"content": "One is, what about\nthat benefit case? Is it really as lumpily\ndistributed as it seems? And two, is there a need for\nresearch in collaborative tools to help the supplier\nor the manufacturer and the retailer to\nget together to have-- if the benefit's in the\ncollaboration space, do we need some research\nto help drive that? Because I think that's\na big gap right now. GUEST SPEAKER 3: I think\nthe answer to your question is absolutely. The more research we have to\nunderstand the collaboration, so understand the cause and\neffect of putting an EPC tag on a product, and being\nable to track that product, and then adjust process to\nincrease the efficiency of flow to the shelf to the\nconsumer is clearly where there needs\nto be collaboration between manufacturer,\nretailer, and academia. I think what I\ntried to communicate is we believe that you\ncan't do everything at once."}, {"content": "You have to pick your lumps. AUDIENCE: Right. GUEST SPEAKER 3: We've picked\ndisplays, promotional displays, new [INAUDIBLE] as a big\nlump that can justify a favorable benefit-cost ratio. And some higher advantage type\nproducts like Gillette blades and razors or Crest Whitestrips\nthat are high-margin, high-velocity, high-shrink,\nthey are lumps that can justify the cost. So start there because you\ncan't do everything at one time, and then based on that learning\nyou want to better understand how to do other products,\nhow to smooth out the lumps, and get consistency\nacross your product line. GUEST SPEAKER 4: Klaus."}, {"content": "GUEST SPEAKER 1: Yeah, we\nhave done exactly the same. We are looking more to\nthe fashion industry because we can see\nthat the garment which costs $100 or $200 can\nbear more the costs, but we have more the efficiency\nthere, and we will do this. We will roll out more\non the fashion side, and we will do trials on\nthe other sides to learn. This is our philosophy,\nand this we will follow."}], "Lec 16 | Special Topics in Supply Chain Management": [{"content": "MARK ROBERTI: OK, thank\nyou, ladies and gentlemen. My name is Mark Roberti. I'm going to be\nmoderating this session. I'm from RFID Journal. You've already met\nall of our panelists. We have Simon\nLangford from Walmart, Claus Garbisch from DHL, Dick\nCantwell from Gillette / P&G and Mike Rose from J&J. So\nthe purpose of this panel is to try to crystallize\nsome of the ideas that have been discussed\nover the past two days, and try to help you to begin\nthinking about how we can facilitate collaboration between\nthe companies that are using RFID and the researchers\nwho are investigating various aspects\nof deploying RFID. So I'll throw it out to\nany of our panelists. What's the primary\nrole that you see the academic-- would like to\nsee the academic community playing in terms of helping you\nand your sector, your industry at large, achieve\nthe level of adoption that you're hoping to achieve? DICK CANTWELL: I\nthink one thing that occurred to me in\nthe morning session was not only the ability\nto do the research, but the ability to share that\nresearch across a broader community. I find in the\nconferences that I attend people are very\ninterested in what I have to say, what others\nhave to say, which says to me that there's a great need for\ncoordinating that research, synergizing that research. And I think it's great that this\ncommunity is coming together to try to do that. MICHAEL ROSE: I think the\nother area, too, that-- this is a great two-day session. I think we're were\ncaught up in responding to the needs of our\ncustomers, the regulators, wherever we might be. And I think that the important\nrole that the research team plays is what's coming next. So we're working on today's\nproblem in my organization. What we need to be looking at is\nwhere the future might take us. And that's where I think\nit's invaluable to have the academic help and support. SIMON LANGFORD: I\nwould add also I think it's important that\nthe academic world actually is in step with the end users. And by that, what I mean is that\nthey are learning alongside us as we pilot, as we learn,\nas we grow our deployments. And so the business\nissues we're trying to solve they're\nfully versed in those, and we're not creating\nisolated companies. CLAUS GARBISCH: And\nfrom my point of view, when we are doing the\ntest, we are always having some challenges, and that\nthe academic community is taking up these challenges. Like we were\nmentioning, these metal and fluid problems, or what\nis the right technology for item tagging? Is this HF, is this UHF?"}, {"content": "So there, to get\nthe right answers, that we go forward with our\ntesting with our customers that we do there\nthe right steps. MARK ROBERTI: There's\nsome talk earlier about incremental\nchange Bill brought up versus radical change. Do you see the academic\ncommunity focusing on the incremental\nchange, or should they be more focused on what's\npotentially transformative in your businesses-- a combination of the two. DICK CANTWELL: I think\nthey have to look at a combination of the two. I mean, we're still at the very,\nvery beginning of implementing this technology, and\nwe have some very basic fundamental questions to solve. Shaving a couple cents\noff of a tag price is very meaningful in getting\nus to the next incremental step versus having it. MICHAEL ROSE: I think\nwhat's also interesting, if you equate it to\nthe web, if you go back where it all started-- MARK ROBERTI: Sorry,\nare these mics working? MICHAEL ROSE: I don't\nknow if they're working. Yeah. OK, if you put it\nto the web, I think what we're missing\nhere with RFID yet is the killer application. And we have to create some\nconsumer draw on this right now is all very\nsupply chain-focused. It's very interesting. But the reality is, when\nyou look at the internet, you look at the world\nwide web, it really didn't get interesting until\nthe consumer got engaged in it. So I see RFID the same way. That's why I was particularly\nexcited about some of the work that's\ngoing on in Korea. You start seeing some,\nwhere maybe in the future you go aha, that\nmakes some sense. Then you can start taking\nthat back to our brand teams and building that into\nfuture design of products. So I support the academic-- Dick's statement\nof working on both. But I'm really looking for\ntransformational ideas. And that's our view within\nthe RFID effort within Johnson & Johnson is trying\nto look through it and how it may\ntransform our business. MARK ROBERTI: Is there\na danger, though, that academics will\ngo off and explore pie in the sky, potential\napplications that really don't have any relevance,\nor wind up not being practical? SIMON LANGFORD: I think\nthat's where Dick was saying about a little bit of both. That at least at the feet on the\nground that you can understand our current processes and\nhow we're trying to evolve, and really when is\nthe tipping point when we can take those\ntransformational changes without disrupting the workflow\nand pattern of the associates on the ground? MARK ROBERTI: All right. Simon, I know you've\nworked closely with Bill at the University of Arkansas. Any thoughts on\nthat collaboration and the benefits\nthat your company has received from that work? SIMON LANGFORD:\nLots of benefits. In terms of that\nworking relationship, it's been very much\na two-way street. And not just giving a prospectus\nor a piece of work or research for building his team\nto go and look at, but also to get those\nideas coming back to us and those challenges and\nthose questions of why do you do it this way. Why not this way\ncurrently with systems you already have in place? But that two-way dialogue\nand regular updates-- and I think Bill would agree,\nboth him and his team have benefited from\non-the-ground, working the process in our stores,\nin our distribution centers, understanding our\ncurrent situation, and where we want to move\nto, whether that be pie in the sky or blue sky stuff. But those are iterative\nsteps to get us to. MARK ROBERTI: Do\nany of the other panelists have any experience\nwith the academic community? I know, Dick, you worked\nclosely with the Auto-ID Center in the early days. Any thoughts on\nthis collaboration, how important it is\nand where it should go? DICK CANTWELL: Well,\nwe've worked very closely really from the early days of\nthe Auto-ID Center with MIT. Part of it is our\nproximity across the river. Not only is there the\nability to stretch the mind and bring real creative\nacademic thinking, but there's a point\nof independence. I mean, we deal with many, many\ntechnology vendors, all of whom are trying to sell us stuff. And that's valuable\npartnerships. But having an independent\nthird party who's academically grounded, who can look at a\nbusiness problem, a supply chain issue, engineer a\nsolution, and come back to you with data, it adds a certain\namount of credibility to help us with our\nmanagement, and also, I think make industry comfortable\nwith the conclusions. MARK ROBERTI: OK, you've\nheard some of the papers that have been presented today. I wonder if the panel could\ngive some general thoughts about the research\nthat you heard about. Is it practical? Is it productive? Are the things you\nwould like to see being done that aren't being done? MICHAEL ROSE: What I\nheard over the last days, I it was very worthwhile. I thought the research seems to\nbe very, very practical, very well-oriented to\nthe issues that we have within our industry\nof trying to figure out the right approach for tagging\nof our products, the effect of various material\ntypes and on RFID. I think it's very well\ndirected research. I think it was an\ninteresting point-- I think maybe,\nSimon, you may have raised in your\npresentation, around can we ever get to replacing\nEAS tags with RFID? It'd be great to get\nan answer on that. So, I mean, just some\nbasic stuff like that, and I think that's\nvery, very basic applied research that's needed. MARK ROBERTI: Right,\nOK, any other thoughts? OK, Mike, you mentioned\nduring your presentation that there's a\nneed, particularly in the pharmaceutical industry,\nfor the whole industry to move forward. That you can't have\ntrack and trace if the distributors don't\ndo it, but the retailers and the manufacturers do. Is there any opportunity for\nindustries to come together to support larger-scale\nprojects, such as the Promise Project that Dmitri presented? It seems to me there's a\nlot of research to be done, and it would be\nhelpful if there was some way of funding that\nand organizing that. MICHAEL ROSE: Yeah, I think\nwhat we all struggle with is, when you bring\nin a new technology, how do you adopt it\nwithin the industry. So I think somewhere, we need\nhelp from academia in how best to adopt RFID in the industry. So yeah, I think some\nof those larger efforts are very important to\nbe funded, absolutely. MARK ROBERTI: Are there\nindustry organizations that could potentially drum\nup some funding from members that could go into\nthat kind of thing? MICHAEL ROSE: Well, if\nI'm speaking specifically around health care,\nand specifically for pharmaceutical, health\ncare is very complicated. There's various segments\neven within health care. But just talking\npharmaceutical right now-- and I see Ted nodding\nup there, we're each members of various\nindustry associations. So the manufacturers\nare represented by pharma, bio,\nwholesalers by HDMA, you have the chain drugstores\nMECDS and CHPA, I think. So there's a wide variety\nof associations out there. That's another avenue\ncertainly to reach out for funding, because\nright now, as we look at trying to adopt RFID within\nthe pharmaceutical supply chain, it's an area that as we\ntalk within our associations, we recognize that\nwe need other help. And it's not just\nthe associations."}, {"content": "So academia plays a role\nin that, absolutely. MARK ROBERTI: Right. Is there a-- I think the problem\nthat I see is, if you want to develop,\nsay, a good item-level tag, and you want to\nfund that research, you put a lot of\nmoney into that. You work with a\nresearch department, and then your entire\nindustry, your competitors, all get the benefits of that. So how do we overcome\nthat problem? And my example is\npharma, but I think it applies to logistics or\nretail or other areas as well. MICHAEL ROSE: Well, I'll\ntake the discussion away from pharma. I think what we're looking\nat here, regardless of the industry, is\nthe adoption of RFID. The only power in it is if\nyou have a large network that can adopt it. So there's a huge network effect\ntowards the adoption of RFID. So when you get to\nareas around the tag and some of the supporting\nnetwork infrastructure required to communicate that\ninformation I'm not quite so sure that's\na competitive advantage. I think it's the actual use of\nthat information, maybe design of new products and services off\nof that common infrastructure, that become areas where we\nstart to differentiate ourselves from each other. SIMON LANGFORD: And I\nthink Claus mentioned in our discussion earlier today,\nabout collaborating and working within EPCglobal, and\nagreeing on the direction and where we need to\nhave detailed research, whether that be on item level\nas we move forward or not. But I think that group of\nend users coming together and that that collaboration\nhas worked great so far, and has really\nmoved us on a pace, and make sure that\nthe technology that's being developed is applicable,\nand will be deployed and used. CLAUS GARBISCH: And I think\nwe have a responsibility as big global companies to\nmove it forward, because we see so many advantages. But I would not say\nthat, on the long run, it is a competitive advantage. You may be an early\nstarter, and you want that all the\nothers follow you, because you believe\nin that technology. And therefore, I think as\nthe biggest global companies, we have to go forward. We have to show\nthat as an example, and then the others will follow. MARK ROBERTI: Right. Claus, you're in the\nmiddle of the supply chain. And we often hear about the\nbenefits for the retailer, we hear about the benefits\nto the manufacturer. Are there benefits for\nthe logistics provider, and are there applications where\nyou can work with the research community to drive some of\nthe benefits for the logistics providers? CLAUS GARBISCH: Yeah,\nI see it very strongly in parts which I have\nnot mentioned today. I would say the asset tracking\nfor all our containers in the-- if you go there on\nthe sea freight side or for our freight\npart, this is one of the very important things. And in the supply chain,\nwe have advantages as well. The only thing what\nI was mentioning was that it might be\nthat the retail side has more advantages if it is fully\ndeployed and fully implemented. MARK ROBERTI: Right. When the internet\ncame along, companies found some new\nbusiness opportunities, new business models that\nhadn't existed before. Are there going to be some\nnew business models for RFID, and should the\nresearch community be looking at those as well? And I'll just give you\na potential example."}, {"content": "As a logistics provider, you're\ncollecting a lot of data. You could be aggregating\nthat data, and somehow perhaps selling\nthat data to whoever would get value out of it. Do you see\nopportunities like that, and would you like to see the\nacademic community looking at those opportunities? CLAUS GARBISCH: Yeah, I think\nwe are as a logistics provider. In the past, we were only\ncarrying goods, parcels, and pallets, and so on. And today, I think\nwe have another-- we carry more information. And we see in this\nnew community fields where we can find new products,\nand where maybe that we can store all the\ninformation, and that we are a mediator\nbetween our customers. So we see there chances that\nwe get their new products, and where we can\nfind new products. MICHAEL ROSE: One thing\nthat I received within J&J is an RFID incubator fund. And it's a pot of money that\nwe put together last year, and we fund it\nagain for this year. And we'll see how long we can\ncontinue this pot of funding. And the idea is to help\nus to think differently about this technology--\nthink transformationally. And so yes, I think there\ncould be some new business models popping out of this. Some of the work that\nwe've done internally, J&J tells us that that\ncould be the case. So at least some\nexciting areas here. DICK CANTWELL: I\nthink we're just starting to scratch the\nsurface with some very basic applications. If I use the display examples\nfrom this morning from Procter, that's very basic stuff. It's just knowing that\nyour display went out onto the sales floor. Once that display is\non the sales floor, it opens up a whole new\nset of opportunities to understand how the consumer\napproaches that display, how fast does the\nmerchandise sell, what locations in the store\nsell better than others? There's a tremendous amount\nof data and information. And then you can take it to the\nnext level of actually starting to have an interactive\nexperience with the consumer. That's all greenfield\ninnovation, once we lay the basic pipe\nto establish the technology. MARK ROBERTI: OK, I'd like\nto go back to Simon's comment about working through\nEPCglobal and making sure that the research is\non track with where the industry is going. How do the people in the\naudience participate in that? Can they get involved\nwith the BAG meetings? Is there a potential\nfor opportunities for perhaps ancillary\nmeetings around the BAGs, where maybe they're not\ninvolved in the BAG meetings, but then come in\nafterwards and contribute? I think a lot of\npeople are struggling. They're reading what's\ngoing on from the outside, and they're not really\nable to jump in. MICHAEL ROSE: Mark, in\nwork with the EPCglobal, I'm a tri-chair of the\nHealthcare & Life Science Business Action Group. And not to get into\nthe detailed structure, we actually have one work\ngroup that's focused on R&D. And we would welcome\nsome participation from the various auto-ID\nlabs to participate directly on that team. Frankly, it's a team that is\nlooking at some longer range areas of opportunity,\nbut we really could use the help\nfrom folks from MIT, and any other auto ID lab. MARK ROBERTI: OK,\nthat's good to hear. What about the sensitivity\naround your business processes and what you are doing? So if the academics come in,\ndo they need to sign NDAs? Is that an issue? How does that relate to-- if they're going to produce\na paper or some publication, how do we manage\nthat sensitivity, so that we're getting\nwork done, but we're not compromising a company's\nproprietary secrets? DICK CANTWELL: Any\nacademic institution that does work with\nProcter will sign an NDA. But the value of having\nan academic institution do that work is, after\nthe work is done, the proprietary information or\nconnection can be culled out of it. It can be genericized. It could be combined with\nother companies' data that they're also working on. And an academic white paper\nthat's very compelling, that sends a message to the\nindustry, can be published. SIMON LANGFORD: That's\nexactly the same situation from our perspective. MARK ROBERTI: OK. I'd like to open it\nup to the audience. This is a chance to ask some of\nthe leaders in the RFID sector questions. So if you've got a question,\ncome on down and fire away."}, {"content": "I know everybody's a little\nbit tired after two days. But Steve, go ahead. STEVE MILES: I have a\nquestion that's an area that we've been working on. But with this notion that\nwe now have a common XML representation of\nsomething, now part of our challenge as\nindustries is to work out how we communicate this. And in the XML\nworld, those of us who live in web\nservices-- and we started out this\nwhole convocation with Steve Bradt\nsaying, well, remember it's not so much an\ninternet of things as things connecting to the internet from\nthe World Wide Web Consortium's point of view. So many of the technologies\nand opportunities for data sharing that we've\ntalked about over the last two days rely on a multipoint-- something like a web\nservices transport. John presented the notion of\nGoogle capabilities and so on. But all of our legacy\ntransaction systems, the businesses that we run\ntogether traditionally, the way in which we've\nshared data traditionally, those of us who have\ndone that successfully, has been in point-to-point\nEDI protocols. And there's the larger-- I mean, when we talk about\nwhat are some of the underlying trends around us,\nthis notion of moving to services-oriented\narchitectures and something, many of us have initiatives\ninside our companies. But when we get down\nto supply chains, we're still used to\nconnecting point-to-point. So when I connect for data, even\nif it's the same XML schema, it's a separate call setup\nand a separate protocol if I'm connecting with\none retailer or another. And I just wondered,\nfrom your perspectives of how you're moving\nwith your organizations to this world of\nmachine-readable content, where you saw yourselves\nin that process, and whether there were roles\nfor academia to play to assist in making that transition. SIMON LANGFORD: In terms\nof where the data resides, I think that the data will\nstill reside in our case with Walmart, and with\nthe holder of that data. And so I think it's\ndifficult to get away from that point-to-point,\nand even on sort of track and trace, than using\nthe ONS to find out where that case or all\nthat batch of product has actually been seen last. But I still think you're\ngoing to get to a stage where you're then going into\npoint-to-point connections to retrieve that information\nmore detailed information. MICHAEL ROSE: Yeah,\nI think the challenge you articulate there,\nSteve, is based upon our businesses\nbetween J&J and Walmart. It's not J&J and the\ncommunity at large you know. I think, however,\nthere are mechanisms that we could posit that\nallow us to more open up the way we exchange data. So even though the business\narrangements are one-on-one, and the data appears to\nbe exchanged one to one, or will exchange\none-to-one, there could be some common\nmechanisms that are used to exchange it\nacross the community. Because I can't see nature\nof business changing. This case is, Simon's\n[INAUDIBLE] with Walmart. It's not with 55 other\nretailers talking to Simon. SIMON LANGFORD:\n[INAUDIBLE] to this is to have a common format. So when you pull up various\ndata from different retailers, then it's in the same format. And then you can just,\nmachine to machine, handle that in the\nsame way, which is less of a burden\non the supplier or whoever is making that query. And I think there is,\nthen, the big advantage. DICK CANTWELL: I think where\nacademia can play a role, once you have that common\nformat, is to do some modeling and demonstrate uses\nof the data that can go beyond point-to-point exchange. Maybe it's a discovery\nmechanism that has something to do\nwith counterfeiting and authentication. I don't know, but use\nthe academic research to show how it can be done,\nand then industry will follow. SIMON LANGFORD: I guess\nbuilding on that, Dick, it's really some\nof the triggers. What triggers are\nyou looking for, not just on track and trace,\nbut on replenishment and out of stocks and things like that? One of the key\ntriggers that you would want to then build\nalgorithms and some logic within your systems\nthat, instead of having to troll through\nmasses of data manually, you're really then just\nworking by exceptions."}, {"content": "MARK ROBERTI: Question there?"}, {"content": "AUDIENCE: My name is\n[INAUDIBLE] I'm from China. So I actually looked\nat you four here. I know you have big\nbusiness in China-- like Walmart, purchase\nstuff from China, and you open one more\nstore in China DHL is really famous in China. But look at something-- China from the RFID\nindustry and infrastructure is very different from\nthe States and from Europe because it's really backward. So my question is, do you\nhave special strategies for the RFID adoption for a\ncompany, especially for China? Or you will say that\nChina will be just a part of your global strategies? And also the second question,\nI'm doing some research work for RFID in China. So do you have any requests or\nsomething we can do for you? Because we know China and we\n[INAUDIBLE] the technology to see whether we\ncan do some research work for these global\ncompanies to have some suitable things for China? DICK CANTWELL: I think there's\na huge opportunity for China to leapfrog a whole\ngeneration of legacy systems that exist elsewhere\nin the world, and be a real innovator,\nwhich of course would be led by people like yourself. CLAUS GARBISCH: From\nour point of view, we are working at the\nmoment on one trial, which we call China Europe-- and this is on the\nfashion side, where we have one manufacturer\nwho is sourcing in China. And we want to bring\nthis stuff into Europe, and we want to do\nit first with RFID. So this is the first trial where\nwe want to go outside of Europe and try the overall\nsupply chain. But this is only\non a fashion trial. SIMON LANGFORD: In terms of\nhow we operate our business in China, we try to have a level\nset of applications or systems, so that if we make a\nchange in any country, we can very quickly roll up\nbenefit to all our countries. And so in respect to that\nof your first question, we would treat China\nas any other country in terms of internal systems. But I think it's\nimportant that we can all work towards a common\nstandard for RFID. That will mean that, whether\nwe're operating internally with systems, or importing\ngoods from China, those products are able to\nbe read, whether they're going to Europe, the US, South\nAmerica, around the world. DICK CANTWELL: If you're\nlooking for a single application for research, one that\nwould be particularly germane to my company\nis counterfeiting and authentication--\nthe ability to have a more reliable way to screen\nproducts and know what's real and what's not. MARK ROBERTI: OK. AUDIENCE: [INAUDIBLE]\nwe are going to do, really working out the\nanti-counterfeit issues. And we joined the\nEuropean committee project called [? Bridge, ?] and we're\nactually especially working for [INAUDIBLE]. Thank you. MARK ROBERTI: OK. [INAUDIBLE] AUDIENCE: Yes, hello. We've been meeting with\ncertain manufacturers, which have been strongly asked\nto add RFID technology to their shipments that are\ngoing to a certain distribution center or certain retail\nstores in the United States. The manufacturers\nthat we met said that they would be\ncomplying with the mandate. However, they were not\nalways very happy to do it."}, {"content": "Some of them that we've met\nsaid that they don't really see any short-term\nbenefit in doing that at the present moment. They're trying to take a look\nat where the ROI would be. They think there is an ROI,\nbut they don't know for sure. And their answer to us was,\nwhen we started asking them questions, we're going to go\nahead with this because we have to do. And we've met other\nretailers afterwards who think that the whole Walmart\ninitiative is a great idea, and they'd also like\nto do the same thing. But their suppliers\nare probably going to give them some\npushback to implementing the RFID technology. So my question\nwould be, what would be some of the best\npractices that you've encountered or\ndeveloped in order to help the suppliers and\nmanufacturers implement the RFID technology? SIMON LANGFORD: A couple of\npoints I would make and-- I referenced this earlier\nin my presentation is-- an educational piece. Educating and sharing,\nas Dick mentioned, of whether internally you're\nworking on white papers or with the academic\nworld working white papers, and\nresearch and development, to share that with\nthe wider community. And that's what we try to do to\ncollaborate with our suppliers. When we've asked our suppliers\nto join us on our initiative, it's not been, OK, you\ntake these products or all these [? SKUs ?]\nin your portfolio. And if you think back to Dick's\npresentation this morning about his advantage products. And Gillette and P&G\nboth independently looked at their portfolio\nand chose the products that were right to tag. And there's one\ncompany now, P&G, looking at the\nright products that deliver ROI in the\nshort term, but looking at those next products that are\njust on the brink of delivering a positive ROI, and how do\nwe get those into the program as well. And it may be cost. It may be technology\ndevelopments. But I think it's incumbent on\nus all to share information, where we're finding positive\nROI in business cases to share those. And that'll be different\nfor every company, depending how they're set up, how\nmature their systems are."}, {"content": "And really, the same answer\nto the gentleman from China, where some smaller companies\nhave got older systems. Then there's an\nopportunity for them to leapfrog their\ncomputation as well. AUDIENCE: If I may have\na follow-up question, companies like Procter\nand Gamble and Gillette are definitely seeing advantages\nto RFID from the presentation that we saw today. So according to you,\nwhat would explain some of the manufacturers\nare seeing advantages and are going forward\nwith the RFID initiative, whereas others will\nprobably give more pushback? SIMON LANGFORD: I\nthink it comes down to taking a hard look\nat your business."}, {"content": "If you look at P&G, for example,\na very efficient company. But yet, they've\nstill identified-- and I don't want to speak for\nyou, Dick, internal benefits and collaborative benefits. And that's a very\nefficient company. And as a technology matures,\nand we get into other areas, that will only go from\nstrength to strength. And we've would not\njust got our top 100 and the suppliers that are\ngoing live during this month, but we've also got\nvolunteers, don't forget. We have over 60 volunteers\nnow in the program. And those companies just\nhaven't volunteered just for the heck of it. They see real value, and\na lot of those companies are getting real\nvalue out of it. And knowing that where we are\ntoday in a number of DCs-- a handful of DCs and stores\naligned to those DCs, yes, we're all investing\nin the technology today. But we're going to\nreach a tipping point very quickly where we start\nto get that critical mass. MICHAEL ROSE: I think our\nobservation around ROI is you can't expect an\nROI right away in this. I think there's a fair amount\nof experimentation and design of experiment that's required\nto figure out where the ROI is. So my advice to anyone\ngetting into this, don't expect a real\nquick ROI right now, because you have to understand\nwhat products-- like, Dick went through\nselection of products. It's a very important\nstep to go through."}, {"content": "And also, then you\nadjust your focus as you learn more\nand more about it. So I think we're very\nclose to Simon's point, getting to the\ntipping point on this. But to the observations that\nyou're hearing and comments you're hearing from\nsome manufacturers, they're very real comments. I can't deny that. But I think what they have to do\nis understand that dealing with a little bit of an R&D-based\napproach and they're going to have to invest a bit. And they may not see\nthat return right away, but they will see\nit down the road. And a question comes in is, if\nyou're going to play the game, do you want to sit\nin the sideline and watch the game occur\nand not participate? Or do you want to\nget on the field and practice and participate? DICK CANTWELL: I would\nreinforce what Mike said. If you believe RFID\nis going to happen-- and I think 99.9%\nof people believe that, then why don't you\nwant to get in the game and help shape the future? In the process learn together,\nwith your retail partner, and create a strong partnership\nthat takes the focus away from the day-in grind of price\nand promotion and negotiation, and begin to plan a future that\nis going to benefit you both? MARK ROBERTI: OK, question here?"}, {"content": "AUDIENCE: I'm Alfonso\nGutierrez from the University of Wisconsin in Madison. My question is related\nto the impact of RFID in core management\ninformation systems, like manufacturing, resource\nplanning, MRP, and DRP, the distribution of\nresource planning. Those systems which let's say\nthat we could consider core, but also may consider legacy\ntypes of systems related to new technologies, are\nbased on, basically, inventory levels. And if we say that\nRFID is bringing a new paradigm,\nwhere planning should be done based on velocity-- meaning velocity,\nspeed, and direction, those are terms that MRP and DRP\ndon't know, don't understand. So my question is, do\nyou see those systems as resilient enough to\nabsorb these new concepts, or they have to be revamped? If so, when do you\nsee that happening? SIMON LANGFORD: One\nthing I mentioned earlier is, I think\nthere's a lot to learn still replenishment systems. And that's one area that\nacademia can help with as to really what does RFID and the\nUPC data mean to replenishment, for example, and our\ndistribution methods? And as I said earlier, we\ndon't know what we don't know."}, {"content": "And as we start to peel\nthe layers of the onion and look further into the data,\nthere are more wows and ahas along the way. DICK CANTWELL: Procter &\nGamble believes very strongly in a consumer-driven\nsupply network. And RFID is going\nto be a huge enabler to get that real\ndemand signal that can then be worked back upstream\neventually to the factory. And IT infrastructure\nand IT systems are going to have to\nevolve to accommodate it. But I'm not smart enough to\ntell you how that's actually going to happen. MARK ROBERTI: OK."}, {"content": "Question here? AUDIENCE: Yeah, given that your\ncompanies have been studying RFID business models\nfor some time, and you've crunched\nthrough the numbers, I had a question related to\nitem level and smart shelves. We've all talked about the\n$0.05 tag for a number of years. But I was wondering if\nthere's an equivalent price point for readers\nthat you'd like to mention, either per reader or\nper linear foot of shelf space. SIMON LANGFORD: In terms\nof readers on shelves and that sort of thing,\nthat's not really anything we've looked at right now. We've been looking at the\nsupply chain, case and pallet. I think as we get\nto that stage, I don't believe you're\ngoing to need readers on every single\nshelf in the store. I think there will\nbe different ways. And when we started to\nlook at out-of-stocks, using case level\ninformation, some people said, well, how can you do that? Surely, you need to\nbe at the item level. And we walked\nthrough our process and shared that\nwith the industry as to how we can affect\nthe customer shopping experience at case level\nwithout going to the item. And that surprised\nquite a few people, and I think that there\nare still more things there that we can do with\ncase than we know today. But in terms of reader\nprices in general, I would like to see\nthose fall in line with how tech prices are falling\nover the last few months. MARK ROBERTI: I'll\nadd a comment to that."}, {"content": "Meadwestvaco, there's\na gentlemen here-- they are smart systems. They developed a\nnetworking architecture that allows one reader\nto control many antennas. So you don't necessarily have\nto have a reader on every shelf. You just put the antennas\nin, and then you cycle through the antennas\nand read the inventory. So it's a possible solution. AUDIENCE: I have a\nfollow-up question now to Steve's question of whether\nit's peer-to-peer [INAUDIBLE].. I understand the\nbusiness is always going to be peer-to-peer,\nJohnson & Johnson, Walmart, because if you\nlook at the supply chain, the movement\nof the RFID tags, the merchandise is\ngoing to, let's say, Walmart, DHL, and then\nJohnson & Johnson. Then that's just one chain,\nand Walmart is simultaneously have many suppliers,\nand Johnson & Johnson is supplying to many retailers. So Simon says they are always\ngoing to keep their own data. They're not going to\nsurrender the data. So given those constraints, how\nwill this kind of Google sync happen? Because Google here,\nthere's a spider that visits all the websites\nunwilling to publish those data. So put it another\nway, do you see EPC as a data clearinghouse\nof all those potential traces of data? Or also, do you\nsee other companies or other organizations\nthat, if they follow, let's say, EPC standard, they\ncan set up their whole data warehouse, the\nclearinghouse, sorry, so as to achieve the sort\nof overall holistic view of traceability [INAUDIBLE]. SIMON LANGFORD: [INAUDIBLE]\nis held by others. But I think we need to\ntake a step back as well as to what business problem\nwe're trying to solve. Where do we want to get to? How do we want to\nwork in the future? And then design\napplications and systems around that and\nmeeting those needs. MICHAEL ROSE: Yeah,\nand your question about how will this Google\napproach be implemented? I don't know that."}, {"content": "I haven't a brain big enough\nto figure that one out. Sorry. But I think to Simon's point,\nwe believe in a distributed architecture around this. To think that it's all going\nto be centralized in one place and one clearinghouse-- if Google could pull\nit off, maybe we'd would be willing\nto entertain it. But I'm not so sure yet\nthat I see that and envision that right now. CLAUS GARBISCH: Actually, in my\nopinion, it's not [INAUDIBLE] MICHAEL ROSE: Right,\nbut Google [INAUDIBLE].. MARK ROBERTI: But\nwhat about the issue specifically to the\npharma industry, where you've got track and trace. You may have the FDA that\nneeds to go and get data. You may need a customs official\nthat needs to go and get data on a shipment. There's got to be somebody who's\ngoing to hold those Epedigrees and guarantee that they're\nsecure so that that data can be recovered, isn't there? MICHAEL ROSE: Well, that's\na very interesting question."}, {"content": "It's still being sorted\nout with the regulations. So I think it's a little\ndifficult to presuppose what the FDA may want to do. I mean, they comment about\nhow they may want to dip in and access this information. But even if they do\nthat, today, if they want to access information\nin, say, Johnson & Johnson, and subpoena us for\nthat information. So the question comes in is\njust because magically, we're now producing pedigrees,\ndoes it all of a sudden this open up access to everyone. I'm not so sure that\nthat's realistic."}, {"content": "MARK ROBERTI: Right\nOK, question here? AUDIENCE: I'm [INAUDIBLE],,\nand I'm a faculty at MIT. So it seems to me\nthat RFID is currently in the adoption phase. So the next stage would\nbe business processing and so forth. And then even further it would\nbe business applications. So my question is, have\nyou started thinking about business applications? And by that, I mean,\nfor example, by using RFID data to say improve\nreplenishment strategies, improve forecasting,\npromotions, and so forth, even with respect to\nmarketing, for example? And a related, similar question,\nis, so RFID will definitely could save less\nstress on planning, because the supply blockchain\nwill become more event-driven. Because RFID, it's about\nreal time information. So when do you see\nthis transition, or gradual transition,\nfrom planning more to being event-driven,\nmore responsive, agile supply chains? SIMON LANGFORD:\nI would say we're moving into that next phase. We're right on the cusp of\ntaking that next step with some of our key trading partners. AUDIENCE: I mean, I guess\nthe question is, have you started thinking\nabout the next step, which is business applications? SIMON LANGFORD:\nAbsolutely, involving various parts of our business,\nas well as our trading partners, whether that be\non accounts payable to look at proof of deliveries\nand how that would work, and how we can evolve and\nwork differently and smarter in the future-- to how do we replenish and how\ndo we really work by exception, but automate everything else in\nthe background using EPC data? So I think there's a lot of\nwork that's still to be done."}, {"content": "Lots of ideas and lots of\ninput from lots of people and different organizations. DICK CANTWELL: Once the\ntag reader infrastructure is in place, on whatever\nscale, business applications are going to come very, very\nquickly and exponentially. Because as you prove value, it's\njust going to beget more value."}, {"content": "AUDIENCE: Thank you."}, {"content": "MARK ROBERTI: [INAUDIBLE]."}, {"content": "AUDIENCE: Hello, I'm Antonio\nRizzi with the University of Parma, Italy. My question is especially\nfor the manufacturers, but also for Walmart. We've carried out\nresearch in Italy to assess the\npotential of and RFID and NPC global network on the\nfast-moving consumer goods. And we came out with one\nconclusion was that-- one possibility to cover the\ncosts of tagging all pallets and cases could be found in\nthe reduction of stock levels. And I've heard a lot about\nreducing out-of-stocks, but very little\nhas been mentioned about the reduction of stocks. And I'm talking of both cycle\nstock levels and safety stock levels-- safety stocks because of the\nvisibility and cycle stock levels because of\nthe possibility to adopt transshipment\nstrategies. I'm just wondering whether\nthe manufacturers here-- so Gillette and J&J have\nbenefited from those stock reductions from weeks to days,\nbecause of the visibilities, if they quantify the reduction? And finally, if they've\nbeen able to adopt transshipment inventory\npooling strategies because of this visibility\nfor the APC global network? DICK CANTWELL: I think one of\nthe reasons you don't see a lot of emphasis being put on\ninventory reduction right now is we don't have absolute\nconfidence in what our in-stock position is-- accurate book\nstock, if you will, and accurate location\nof the merchandise. That is beginning to happen. We are beginning to identify\nreal out-of-stock levels, and beginning to put\nin place new processes to be in stock more often and\nbe able to locate inventory to keep the shelves filled. Once that happens,\nand we have confidence in our in-stock\nposition, then you're going to find that there's more\nappetite for lowering safety stock and reducing inventory. MICHAEL ROSE: I would say\nthose areas you identified are ones that we're\nvery interested in and we're monitoring, but\nit's very early stage. But they are key areas\nthat we'd say, yeah, this could make some sense for us. It could justify\nour approach here, and could justify the\nexpense of tagging products. So they are two key areas that\nwe're certainly focused on. AUDIENCE: Yeah, thank. CLAUS GARBISCH: And you\ncan see it done on fashion. They do it. Because the stock due\nto the short season, there it is worthwhile to\ndo it with was tags fully. And then you can get these what\nyou have mentioned, the stock reduction. MARK ROBERTI: OK, question here?"}, {"content": "AUDIENCE: Yeah, I\nhave a question. On your slides, and also\non your colleagues' slides, the word privacy occurred. And the question I have-- I mean, MIT in many\npapers the kill function is the optimal answer to solve\nall those privacy problems. Some research I\ndid also proposes that at least in\nGermany, 75% of consumers rather want to have chips killed\nat the retail store instead of using a privacy-enhancing\ntechnology. Now, the question\nI have is, do you think that killing the\nchip at the store exit will be the answer to privacy? Do you think that's realistic? DICK CANTWELL: No. I think as Simon said\nearlier, education is going to be the\nmajor opportunity to dispel concerns about\nprivacy than respect of guidelines for the\nsafe and responsible use of the technology. The kill function, as it becomes\ntechnologically available, is a necessary right\nof every consumer. I think what we will find\nis the educated consumer who has the option to\nkill or not kill is not going to kill the tech. SIMON LANGFORD:\nAnd also somebody-- I forget who it was,\nforgive me, mentioned about through that\neducation and through people using the technology and\ntouching the technology will transfer to a consumer pool. They want to use the technology\nfor things either in their home or for returns and\nwarranty information-- that type of thing. And there are huge\nbenefits for consumers. And rather than\nkill tags, then I know through some of the working\ngroups, the Hardware Action Group, they're\nlooking to reduce, for example, the read range. So a tag that may read 5 to 10\nfeet today to reduce that read range to contact. And so for privacy, if there's\nany concerns in the consumers' mind about privacy,\nand they would normally want that killed, then to\nreduce that read rate to contact still allows that customer\nto return that product, to look at the warranty\ninformation, the retailer. And so it can service them and\nstill add all those benefits. But that's all for the future. But as Dick said,\nthe key thing is education and why we're using\nit, and what's on the tag and what's not on the\ntag about personally identifiable information. AUDIENCE: Thank you. MARK ROBERTI: Doesn't\nit also depend on the category of products? So if it's clothes, I may\nnot want a tag in my clothes. I may either want to cut it\noff if it's on a hang tag or kill it. Whereas if it's on-- I don't know, my\ntelevision, I might want to keep it so that if\nmy television is stolen, I could possibly recover\nit if it's found. DICK CANTWELL: I\nthink you're right. I think that\ndifferent products are going to have a greater\nor lesser likelihood to want to have the tag removed. But I think as Simon points\nout, as the benefits of the tag and keeping the tag are more\nunderstood by consumers, and they don't fear\ninvasion of privacy because read range is\nnot that large, it's not going to be necessarily\nsomething they're very interested in spending\nthe time to disable or remove, because it's really not\nan issue in their life. In fact, it's\nsomething that's going to give them more opportunities\nto interact with the product after purchase."}, {"content": "MARK ROBERTI: OK."}, {"content": "AUDIENCE: I promise this\nis my last question. If RFID can have the potential\nto help Walmart better manage their inventory,\nreduce out-of-stocks, and et cetera,\nother benefits, then could the same be true\nfor their manufacturers? Could their manufacturers--\nif they ask their suppliers to incorporate RFID,\ncould they also better manage their\nproduction inventory and their own processes? Do you see this happening\nwhere the technology will diffuse throughout the\nsupply chain in that direction? SIMON LANGFORD: I don't want\nto speak for the manufacturers. But certainly, if there are\nbenefits from our suppliers to ourselves, and to the\nend consumer, then surely, there are benefits all the\nway up the supply chain. And the session previously\nabout packaging and the goal for tags [INAUDIBLE]\nto be incorporated into packaging, how\ncan we better manage that packaging, that raw\nmaterial in the process? MICHAEL ROSE: Right\nnow, we're very much focused on working\non the consumer side with the retailers. And then also in this whole\narea of patient safety, there are two areas of focus. Over time, you see it\nexpanding from there. So yes, we can see where,\nas we receive goods, we may want to\nhave a tag as well. We're not going to\nput it in a mandate to say [INAUDIBLE] tagged\nproduct that's coming into us. We're not at a point where\nthat makes sense for us. AUDIENCE: OK, but this\nwas a two-part question. [LAUGHTER] So if the suppliers are\ngoing to incorporate that into their product\nline, eventually, the products that have the\npackages that have RFID are not only going to\ngo towards Walmart. But also, there\nare other buyers, there are other clients. So perhaps other\nretailers, because they're receiving packages\nthat have RFID, will simply be putting\nreaders and antennas in order to benefit from the\nchip that's already there. Do you see this happening? And if that's the case,\nhow can Walmart keep their competitive advantage? SIMON LANGFORD: I would\nsay that's a good thing. To date, their product is\nproduct flowing here in the US. That is flowing to us as part\nof a manufacturer's tagging initiative that is flowing\nto other retailers. Whether they choose to\nuse it and take advantage, that's up to that\nindividual retailer. But we talk about adoption, and\none of the questions earlier was about competitive\nadvantage, and sharing and not sharing information. And we're all about adoption\nand driving adoption, and sharing that information. How you choose to use\nthat data internally, that's up to individual\ncompanies, then? That's where you get your\ncompetitive advantage. But in terms of\nbroad scale adoption and receiving product and\ndistributing product, then they're the givens. It's then how you\nuse that intelligent within your own four walls. MARK ROBERTI: Thank you. OK, we got time for\none more question, then we're going to wrap it up. AUDIENCE: This is about\nthe security of RFID before the point of sale. Do you think it's a concern\nfor suppliers, manufacturers, and retailers that, if the track\nthat you ship your products are scanned by\nmalicious reader-- say by a [INAUDIBLE]\nor a competitor, is there really a concern? Or if it is, what can be\ndone to solve this problem? SIMON LANGFORD: I think one\nof the discussions in the HLS working group is around\nsecurity and encryption of data. In the general throes of\nthings, in terms of privacy, then somebody standing\noutside the store, trying to read\nproduct and determine what a person is carrying, then\nobviously, that's not viable. It's just a number,\nand it'd have to have the database\nto go along with that to tell what that consumer has. But in terms of within\nthe supply chain, then we're looking at how\nwe secure that merchandise and encrypt those tags so\nit can handle properly. MICHAEL ROSE: Yeah, so I\nthink Simon is right on. I'd look at it and say, well,\nwe want to use the technology to secure the supply chain. But at the same time, if we put\nit on there, it could be red. So there, it may be\nmore easily identified. So there has to be some sort\nof mechanism of which we can either allow\npeople to read it, but then don't get the\ninformation about what that product is, or there's some\nsort of encryption mechanism. So we're working for a way\nof how to sort this through. It's been identified in the\nHealthcare & Life Science Business Action Group\nas an area where some companies have expressed\nsome concern around this. The reality, though-- if you\nlook at the technology today as it exists, if you're\ntrying to read it through the back of a truck,\nif the truck door is shut, it'd be hard. But I think that's\nour experience. I mean-- but on the other hand,\nwe do have to be vigilant, and making sure that\nwe just don't, through the enabling of new\ntechnology, open up a whole other area of weakness. So that that's a very hot\ntopic of discussion right now. AUDIENCE: Just a\nquick follow up, do you think the current\ngen 2 [INAUDIBLE] pack is ready to be deployed\nbroadly in the supply chain before the answer for my\nprevious question is answered? I mean, is for sure the-- the security problem\ncan be surely solved, do you think the gen 2 class\ntag be broadly deployed? Is it a concern? SIMON LANGFORD: There are\nsome security features built into gen 2. Whether they're robust enough\nto satisfy some of the concerns that Mike referred\nto [INAUDIBLE] from the requirement. MICHAEL ROSE: I\nwould say that it's a step forward with those\nsecurity features built in. But I think we're looking at\ngen 2 also for other reasons-- not just for security purposes. If anything, I think\nwe're all hoping-- and I think our\ntests are showing that we're getting some\nbenefits of improved read rates and readability. So again, with any\nnew technology, it opens up a new potential\narea that needs to be secured."}, {"content": "So we need to be vigilant\nabout that through our business practices. We just can't always look to the\ntechnology providing security as well. You have to look at\nthe business practices, the way the products\nare handled. There's a whole host of issues\nthat are wrapped around it, not just the technical issues. MARK ROBERTI: OK, that's\nall we have time for. Ladies and gentlemen,\nplease join me in thanking our\ndistinguished panel. [APPLAUSE] And I'd like to hand it back to\nSteve for some closing remarks. STEVE MILES: Well, [INAUDIBLE]\nthank you everyone, all of you, very much for coming. There's some cabs out front. I know people have to\nrush off to the airport. Just a reminder-- everyone is\ninvited to the May 1 next RFID academic convocation. We'll want to open this up. We've initiated sessions on the\nmodel of the discussions that have happened over\nthe last two days on the Auto-ID Lab's website. Feel free to share that\nwith your colleagues. We're opening this\nup beyond just this immediate academic\ngroup to vendors as well, and looking for\nopportunities to collaborate. Thank you all very\nmuch for coming."}], "Lec 14 | Special Topics in Supply Chain Management": [{"content": "ROBERT CLARKE: My\nname is Rob Clarke. I'm from Michigan State\nUniversity School of Packaging. And I've been doing RFID\nresearch since 1999. We just are in the process of\nfinalizing purchase of a 10,000 square foot facility to\nmove a new lab that'll look at both active and passive."}, {"content": "But moving forward with\nthis whole session now-- a couple of things that kind\nof bothered me about this. I keep walking in and\non the bulletin boards right outside the door\nthere is a big advertisement for Urinetown, and I want to\nknow who leaked information about this session. OK. From a packaging\nstandpoint, how many people here are directly\ninvolved with packaging? OK. A few. You may or may not know\nJim Goff-- the name-- who founded the\npackaging discipline at Michigan State University--\nthe first academic packaging facility in the world. He just passed away last week."}, {"content": "And it's a very\nunfortunate situation. And for those of you that didn't\nknow, on the news this morning, Super Value just\npurchased Albertsons. And we don't know what\nthat's going to do to their RFID announcements. But I also want to\nthank the sound guy up there because he's\nbeen working his tail off for this entire conference. And the rest you'll wait for. Special thanks to Steve\nMiles and the MIT crew, the conference committee\nfor all the work and phone calls that\nwe had back and forth, all the speakers, and of\ncourse, all the attendees because without the interchange\nof our ideas and your input this wouldn't move very far. And we can all say that we're\npart of the soakers club now due to our walk yesterday\ncrossing the puddles on the way there. We're bound for life. Why study packaging? What does packaging have to\ndo with \"riff-id\" or \"arr-fid\" or anything else. Well, Throckmorton P. Ruddygore\nIII, one of my favorite people once stated, packaging is\na center of the universe. You might as well accept it now. All these other disciplines and\nfields just support packaging. And once you come\nto that agreement, you'll see why packaging\nis relevant to RFID. Our work is a ground-level,\nhands-on approach. A lot of the\ninformation that I hear is the 30,000 foot overview,\nwhich is wonderful. It sets direction."}, {"content": "It sets precedence. And it doesn't mean\nshit if it doesn't work. You have to have the\nground-level, hands-on approach to make sure it works. That's what packaging does. Now, why is this so? What is packaging? Real quickly. You're all going to be\nexperts in 30 seconds or so. There are different\nlevels of packaging. Primary, secondary,\ntertiary, all the way up through unitized loads\nor cargo container loads, however you want to look at it. Primary physically\nholds the product. A bottle is a primary container. Secondary container--\nwe're really brilliant-- holds primary containers. So a six pack of your\nfavorite amber beverages would be a secondary container. Tertiary-- I bet you're\ngetting this now-- holds the secondary. So a shipping case would be the\ntertiary, and on and on and on. Not all products have the\nsame levels or requirements. You also have\ndifferent requirements for different industries. You have the consumer\nsector, which we've heard a great deal about. Retail outlets, these\ntypes of things. Industrial sector really\nhasn't gotten a lot of play here except some of the\nDHL and larger volumes, but it's a huge market. When you think of all the\ntrucks you see driving down the highways with the big boxes\nof materials or machinery, that's a sector that\nis completely ignored. Military-- think about\npackaging something that you don't know when\nit's going to be used, where it's going to be used, or\nwhat the evaluation is for once it is in use somewhere between\nnow and 20 years from now. How do you package that? Real interesting dichotomy. And then, of course, medical\npharmaceuticals, which we've heard a great deal about. How many of you\nwant your packaging to fail prior to\ntaking your medicine? Huge implications. And what goes on for powdered\nsoap for Mr. Procter & Gamble, Cascade-- you brought that up earlier-- is a little different\nthan what goes on for a life-saving\nmedicine, and there are different requirements. So packaging has\nto address those. It then has to do all of that\nwith an eye towards cost. And for 10 seconds, cost and\nprice are not equal, period. You don't talk about\nthe aversion to moving into RFID because of the cost. It's actually the price\nthat most people-- if they look at the cost,\nyou have insurance costs, liability costs, opportunity\ncosts, goodwill costs. And you have to balance the\ncosts, positive and negative, before you can make\na business case. And that's more of the\nbusiness side coming out. Price is what you\npay for something. Environmental issues. Simply put, your product\nplus your package, at worst, has to equal the\nenvironment that you're going to ship it through. Now, you know everything\nyou need to know about packaging, pretty much. If a package doesn't do its\njob in that environment, it breaks down, product\nspills or breaks. No good. If it's too good\nand your product all gets there in perfect\nshape, you're losing money. So there's a fine\nbalance and trade off there that you\nhave to look at. Four main functions of\npackaging, any one of which can be considered a\npackage by itself, but ideally, most of the\npackages we're talking about have all four. Containment."}, {"content": "Physical holding. You'll see my little\nno sign up there."}, {"content": "Why? Because the containment\nfunction of packaging really doesn't mean\nanything with respect to RFID implementation. Convenience and utility. Easy opening features. Reclosability. Secondary uses. These types of\nthings in packages. Maybe, maybe not have\nan opportunity with RF. If you're looking at material\nsorting in a recycling center, maybe. That's a possibility."}, {"content": "Now, communication\nand protection. A package should\ncommunicate to the users how to open it, how\nto close it, safety. Don't try ripping the damn\nplastic clam shells apart with your hands because\nyou'll cut your fingers up. That's the number 1 hated\npackage in the world. It doesn't matter what\ncountry you're from. People hate it. But protection is a really key\none for almost every package system. You have to protect the\ncontents from the environment, be it a physical environment,\nshock, vibration, compression, during\ndistribution and handling. Be it environmental and\ntemperature, humidity. Salt spray. We had conference, too. These types of things. You also in many\ncases need a package to protect the environment\nfrom the product. Nuclear waste. How many of you\nwant those packages to fall apart and leak, say,\nany time within the next 20,000 years? So these are functions\nof packaging that you need to take into account. The impact of this? It's cool. Everything has to be packaged. It's the center of the universe. You can go into any field\nyou want through packaging, and students like that. All companies and industry\nuse it at some level. And it should withstand the\nrigors of distribution, manual and mechanical handlings,\nwarehousing and storage, not create any problems,\nand give the ultimate user a sense of value. What is value in this case?"}, {"content": "You ask somebody, what\nis it that you want? And then you give it to them. And if you really want\ncustomer satisfaction, give them something extra. What a bargain. What a value. Package and can do this. Now, at the MSU\nSchool of Packaging, we focus on RFID applications\nfor the supply chain because the item level is\na great marketing thing, but it's a ways off. Supply chain has some\nreal applications. You're using bulk quantities. You have fixed\npoints that you're using, although they're\nnebulous depending on the company and\neverything else. And you want to ultimately\ndefine the perfect purchase order so when a store or\ncustomer orders a product or a truckload of products,\nthey get exactly what they want, when they want it,\nin the right count. And that's one of the\nthings RFID can do. Little aside-- MSU\nSchool of Packaging, we have 600 undergraduates. We have just about 100\nmaster students and 20 PhDs. Who'd have thunk\npackaging would be that big all in our own school? Now, we haven't spent\na lot of time talking about active packaging\na little bit yesterday, but there is a real\nopportunity for complementary active and passive application. If you start down here with\nitem levels, going into a case, going into a pallet, ultimately\ngoing into a container, you can track those. And if you note, some of\nthose require passive tags, meaning the tags don't have\nany internal energy source. They collect it from the\nreaders at read points. Versus an active tag, which\nmay go on the container. And you'll see if you\nlook in the bottom corner, I stole this from the\nDepartment of Defense because this is how\nthey ship things over to the Middle East for\nthe Iraqi and Afghanistan or wherever the hell\nthey are over there. Pakistan having problems. And I like it because\nit shows that you have different responsibilities\nat different parts of the supply chain. And there's a lot\nmore to this, but I want you to realize that\nbecause here's the key. There's nothing that says\nsome of those applications have to be RFID. I love RFID. I live and breathe RFID. But it's just a technology. And there are a lot of different\nfrequencies and applications, some of which are better\nsuited by other technologies. There are different\nfrequencies than 9:15. I think the opportunities\npresented by EPC for bringing a lot of\npositive things to the world is a strong, well-defined,\nforward-thinking process. But it's not the only\nRFID process out there. They cover 13.56, 9.15. Those are key elements. But there's a lot of\nother applications that don't use those\nfrequencies, that don't use the same\ntype of applications, and we will have to be careful\nnot to exclude those and focus only on certain areas. Now, why look to a university? Well, I made this one up\nbecause it looks cool. And of course, packaging's in\nthe center of that universe, as we all know it\nshould be by now. But we interact with\nusers, we interact with institutional players,\nand we interact with suppliers. We have nothing to sell except\nour work and our research. And if we don't do a good job,\nwe don't sell it very well. So these are some of the\nareas that I would encourage you to look at, think about,\nand if it makes sense for you to develop some of the work\nwith the university, hey. We encourage it. Packaging materials. This is where I\nwas afraid you were going to fall asleep so I put\nin a moving picture for you. What are the materials,\nmajor materials of packaging? Well, in somewhat of\nan order here, wood. Huge area. Very few people talk about it. They think wood pallets. That's it. But there are so many\nwooden packages out there that you lose sight of\nthem because they're a tree in the forest. Paper, which is slightly\ndifferent than that. And there are a lot of\ndifferent types of paper. There is not one\ntype of paper, so you say it reads through paper. If you say that, you don't\nknow what the hell you're talking about because\npapers are treated, coated, laminated, different\nprocessing issues that go on. And all of those can affect\ntheir characteristics and either lend\nthemselves to benefit RFID or, on the\nother side, block it. Then we have problems. Plastics. And these are things that I'll\nspend a little more time with. Name me the plastic. You can't. There's, like, a\nbillion of them. That's the beauty of plastics. You can build in the\nrequirements that you want. You add a little bit of\nthis, a little bit of that, and you have a gas barrier,\na light barrier, a moisture barrier, all these things. And you can mix and\nmatch so you can design any plastic you want. Metals. Huge opportunity. A lot of metal packaging\nstill available. And glass, of course. Now, the interesting thing about\nglass-- well, I'll skip that. Now, from a\npackaging standpoint, there is no cardboard. And I've heard several\npeople make reference, and it's OK because you\nhadn't had this class before. But cardboard is not\na technical term. And if you'll notice, in\nsome of the presentations, cardboard in one presentation\nreferred to rolled paper. Cardboard in another\nreferred to corrugated. And then another\nreferred to paperboard, which is like what's on the\nback of your writing tablets. And so from that standpoint,\nthey're different terms. Solid, fibreboard\nwould be your cardboard if you'll use that on\nthe back of your writing. Corrugated board. Your cardboard box. And I'm going to put this up\nnot because you're idiots, but I just like to draw on the. Board OK? That's a cross section\nof a corrugated box. And I have it there\nfor a reason that I'll get to in a little bit. Now, the impact of\nthese materials on RFID. Different frequencies\nexhibit different behaviors around different materials\nand applications, period. That's the nature of the world."}, {"content": "Can't change it."}, {"content": "You need to find\nthe one that works. Metal has a large effect\non most RF systems, and particularly\ntrue at the UHF. It blocks it. It re-radiates it. It sends it off elsewhere. Now, interestingly,\nif you know that, you can use that\nto your benefit. And in our facility, we've\nbeen able to double our read range by directing\nthings at an angle off of large metal structures. And it works wonderfully if you\ncan control the environment. Now, in the supply\nchain, good luck. But in certain situations,\nyou can do that. Plastics. Now, plastics, because there\nare so many different ones, they vary all over the map. Some plastics are RF friendly. Some hate it. Common ones, your PETs, your\nPolyethylene Terephthalates, your Coke bottles,\nyour polyethylene, or polypropylene, all of\nthose are pretty RF friendly and can be utilized. But not all plastics\nreact the same. Now, interestingly, glass\ncan be claimed to have little to no effect on RF. That can be true,\nbut you keep in mind that glass is not glass. Whoa."}, {"content": "All right. This is metaphysical already. Glass is like plastics. It's like paper. You order the type of\nglass you want to use. There are different\nclasses of glass. Those of you in the\npharmaceutical industry know that. You can have class 1,\nclass 2, class 3, class 4. And in changing that, when\nyou're making your glass, you add a little bit of\nthis, a little bit of that, to get different properties. Think about in your kitchen,\nthose of you that have Pyrex. Cookware. If you drop it, it bounces. That's not the same\nas your light bulb. Glass is not always glass. And the composition of glass\ncan impact the properties. Now, I didn't have time\nto put in a slide of this, but here's some testing we did. We took panes of glass,\nand it was class 2. If anybody cares,\nlike your window pane. Clear, amber, and green. We took an RF tag\nread, got a read range, and then we put the\ntag behind glass. Our read range dropped. We sandwiched the\ntag between glass. Our read range dropped. Huh?"}, {"content": "Shouldn't happen that way. But it depends on what's added. You need to think about that. Paper, little to no effect. And for the reasons\nI stated earlier, that's not always the case. Paper has some real\nunique properties even if it's just\nplain, brown paper like you get in grocery bags. We're going to look at that. Composites-- they're\nall over the map. The only real\ncomposite I saw here was the bubble pack with Mylar,\nso it was a foilized plastic. And I'll tell you right\nnow, that one doesn't read. It's a good insulator\nfor RF energy. Problematic matching has come\nup because sometimes, you have packaging and\nwater-based products. Water is particularly\nproblematic with UHF. If you move to a\ndifferent frequency, the water issue can decrease so\nthat it becomes more friendly. They use energy\nthrough the Earth to communicate with submarines. We got a 500-mile-long antenna\nburied in the upper peninsula of Michigan so that they can\nuse frequencies through water to communicate with submarines. You can use it, but\nit has to be really low in that particular case."}, {"content": "The lower you go,\nthe better it works. But not all liquids are water. If you think about it, how many\nof you put water in your car? I hope it goes into your washers\nrather than your fuel tank because there's a difference. Gasoline doesn't\nhave water in it. Your gas engine doesn't like it. Motor oil does not\nhave water in it. And so if you take\nthose liquids, RF can go through those. So water-based problems is. Any product put in\nmetal packaging. So your product can\nbe perfectly read, and if it goes into a\npackage that doesn't work, you have problems. Now, the Cascade powdered\nsoap that I mentioned earlier is a good example of a\ndry, granular product. Let's put it in a\nstandard corrugated box. Will it read or not read? What do you think? Read? Not read? Doesn't read. Why? It's not the water. One of the things you\ndon't think about. Heavy iron content in the soap. And hence, it scatters\nthe RF all over the place. So you can't generally read\nthrough a powdered soap. Who'd have thunk? Just looking up\nhere, we have meats. We've been testing meats. Silly thing. Water-based I talked about? This is a TI. That's a 13.56 tag. We also tested it with 9.15. Single cut of beef. And by the way, it was\nsteak, and damn good when we were done testing. We ran them at\nchilled temperatures and we ran them at\nfrozen temperatures 13.56, six you could read two\nor three stacked in a column and still be able\nto read through. UHF, you could read the\ntop one, and that was it. Wouldn't penetrate\nto the bottom. When we froze those same cuts\nfrom each with the same tags, we could read five and six deep\nbecause the water molecules get bound up as ice crystals,\nand they leave gaps for RF to go through. So you can read frozen a\nlot of times with water. And for those of you that\ncare, it's dielectric constant. If you look at dielectric\nconstant of water, it's 70 to 80 depending on\nthe properties involved. If you look at the dielectric\nconstant of ice, 3.5. Right through it."}, {"content": "So pretty good stuff. Some of the research. I just want to touch\non these real briefly. Somebody give me a five minute\nsign when it's five minutes. Warehouse environment. This was starting way\nback when we first-- where in the heck do\nyou use this stuff? And so back in '99, we\nstarted this research. Well, let's see if we\ncan use it in a warehouse in developing a model for it. Transponder effects\non bloom time. That meat study, the\nfirst one we looked at, because the inlay\nwas printed on PET, if you put that on the\nsurface over the meat, even though it was on the\nplastic over the meat, it created a double barrier. And when you peeled the\nplastic off of the meat, you had a big purple\nspot there because meat turns red during oxygenation. And so when you\npeel the tag off, because you have an\noxygen barrier on top, you left a big purple splotch. And people were returning\nmeats to the store saying, this crap is spoiled. And yet, if they\nwaited 20 minutes, it would disappear because\nthe meat becomes oxygenated. Frozen and refrigerated\ntemperatures. I just gave you that."}, {"content": "That's John [INAUDIBLE]. He's now at Kimberly-Clark,\nan EPC member and a task group on something. Jeff Taslar, now with\nSimon, wherever Simon went. Effects of tag orientation and\npackage content on readability. I have a slide on this that\nI'll share with you shortly. Failure modes of\nclass 0 in the lab. When you look at ISTA,\nthe Institute of-- International Safe\nTransit Association. Excuse me."}, {"content": "Mental burp. They run a series of dynamic\ntests that UPS, FedEx, DHL, may use for liability\nissues if something goes wrong with your package. They beat the hell out of it,\nand the product has to survive. That's their test in a nutshell. Now, American Society of\nTesting Materials, ASTM, also has a series of tests. Shock, vibration, compression. And you can build your own\ndistribution to fine tune so you don't over package. We did both test side by\nside with the same tags. And one of the interesting\nthings we found was that it was really\nhard to kill them in transit, unless there is\na direct impact to the chip. The chips were not affected by\nvibration, which surprised me. They were not affected by shock. And also, the shock was\ndirectly over the chip. And it's a brittle piece of\nsilicon, glass, so it breaks. And here's why I\ndid this earlier."}, {"content": "A little trivia for you. It makes a difference\nfor survivability whether the chip on a tag is\nover the peak, over the slope, or over the valley. Try and plan that into\nyour automated system."}, {"content": "Yes."}, {"content": "[LAUGHING] I'll tell you later. And we just finished one\nantenna configurations looking at product\nand tag types. And we evaluated with\nthe same readers, and we looked at\nmultiple readers. We looked at multiple\nclasses of tags. Whether one antenna, two\nantennae, three antennae, or four antennae\ngave better results. And we looked at both\nlinear and circular for different product mixes. So real interesting. We hope to have that one\npublished shortly because it's a fascinating study. One that's going on right now. And Richard-- he disappeared. This is what I wanted\nto talk to him about. Electromagnetic property\nmeasurement and RF signal absorption, evaluation\nfor product stimulant. I love these titles because\nthey don't mean anything. But we are evaluating how much\nthe materials downgrade moving through various materials\nbecause they all have a different-- if you\nlook at a freeze equation, for those of you that know\nwhat I'm talking about, you have a signal strength. And when it reaches a barrier,\nif it's a packaging material, you'll get a lessening as\nit crosses that barrier. And you can actually\nmeasure these. And we developed some\nnew equipment to do that. And that's coming up\npretty interesting. And then we did\none on Department of Defense's RFID mandates. Blah, blah, blah."}, {"content": "This is the chart that's\nkind of interesting. On the one side here\nwith the product, you'll see that I have empty\nfoam, empty bottles, rice, and water bottles. Those are 12 by 12 by\n12-inch cases, cubic foot, with nothing in\nthem, foam in them only because we are looking\nat some of the shipping uses. Empty PET bottles,\nrice in PET bottles, and filled water\nbottles, the same PET. And if you start going across,\nthe tag orientation at the top says outward-facing tags,\nwhere we had as many tags facing outward as we could. Inward-facing tags. You'll see problems there. Forward-facing\ntags on the pallet. Upward-facing tags on the\npallet, and bottom-based tags on the pallet. One of the DHL movies\nwhich I thought was interesting, when they\nleft the tag on the product in the loading dock and\nthe guy had to stop and run back and get it, the tag\nwas sitting on the side so the reader could pick it up. If you took that same package\nand turn that tag down on the floor, chances are\nthe reader wouldn't get it. Why?"}, {"content": "There's metal in the floor. Because when you\nbuild facilities, you put rebar down there. It scatters waves all over hell. So anyway, this just shows\nthat the wait are good reads. This is 1,200 reads\nfor each test. Each box is 1,200 reads. Percentages of reads. And you can see that\nthe white to the red, you had orientation issues. Not a lot, but\nstatistically significant. And in the product categories,\nas you moved across, you had certain\norientations of the tags on cases that were problematic. And when you looked at\nboth of those areas, the orange squares up\nthere did come out orange. Real problems. Packaging and product\nissues that affect RFID readability,\ntransmission, any number of words you want\nto put in there. Now, again, this is\nRichards, and I won't-- this is some of the\nstuff we're doing on the electromagnetic\nproperties and being able to\nmeasure energy that gets through packaging\nto a tag and being able to measure how\nmuch energy it takes to breach the threshold\nfor a passive tag to send a signal back. And it's neat."}, {"content": "We can talk about that later. Now, corrugated board. When you add humidity to\npaper, so hydroscopic paper, it absorbs moisture\nto some level. It changes the moisture content. And if you see here, we\nhave storage conditions. Three different\nstorage conditions, three different moisture\ncontent associated with that. And guess what happens? As you go up in\nmoisture content, . You start having an\neffect on the paper. Now, it doesn't look like\nmuch, but what it boils down to is corrugated board or any\nother paper can affect RFID. It does not say it will."}, {"content": "It does not say it has to. It can when you\nhave high humidity. If you have low to medium\nhumidity but the paper has already been exposed\nto high humidity. Hysteresis is this thing-- Mark and I had a\nconference in Tokyo where we discussed\nthis very topic. When you change a paper's\nchemistry by adding moisture and then dry it out,\nit never goes back to the same static condition. There's always a residual there. So it's easier to pick up\nmoisture the second time around. So with leakage or\ncondensation, that's an issue. If you guys are\ntrying to ship down to help Katrina in\nthe southern coast where it literally rains inside\nof warehouses in the summer, this could be an issue. However-- and here's\nan interesting point."}, {"content": "I hear this a lot\nin my presentations. That means we should go to RPCs. Returnable Plastic Containers. Or actually, there's about\nfive acronyms for RPCs depending on the industry\nyou're talking to. And here's one of the things\nthat I find with that."}, {"content": "If you do-- and you can. They'll go, you\ncan embed the tank. Absolutely right."}, {"content": "No question. You can embed the tag,\nand it's protected. Those are good things. You can embed the tag\nin corrugated, too. I don't know."}, {"content": "I have to look ahead. Don't look."}, {"content": "OK, good. If you embed the\ntag in corrugated, there's a problem with that\ninherently from an operations standpoint because you embed\nit, and those of you that have been in a\ncorrugated plant, you see this stuff whipping off. And you place tags\nevery so often. You then have to die cut\nto get that box plank. And if you're\nchanging the dye, you don't know what's going\nto happen to that tag. So that's really problematic. That means you end up\nhaving to inventory 8 foot sheets of board rather than dye\ncute blanks or dye cut boxes. That's a huge amount\nof space increase. Now, the other thing that-- OK. With RPCs, I'll cover\nthat in a second. Don't let me forget."}, {"content": "These are just some of\nour test conditions. We have loading\ndocks at the school. And in the bottom\ncorner over here, you'll see one of my labs. And I run that-- I just put the corrugated\nup to make it look clean because it's a dirty\nlab, if you will, literally and figuratively. But we'll turn on\nall the equipment and see what electromagnetic\ninterference does with respect to different tag and\nreader combinations. We did shock testing. So we're doing drops on here. Here's a vibration test\nunit load, as well as column stack where we're trying\nto get the columns rubbing against each other to see\nif we could braid through to destroy the chip. Compression tests. You can do these all day long."}, {"content": "Nothing ever happens, but\nthey're cool to look at. Now, here's some\nissues that we also found in looking at a lot of\nproduct package combinations. This is a seven-down footprint. Four and three cases. And the white means you\ngot 100% reads in 25 trials going through a portal. And you have the red with\nvarious percentages up there. Those tags didn't read, but\nthey're all perimeter tags, and they didn't read because\nmetal forklift blocked the RF from getting to them. I don't have it in here. We did a lot of\nstudies on how much room do you need to measure\nin that gap and reflect off, but it's different. Now, on the bottom is\nthe exact same product, the exact same cases\nand tags, but I changed the stacking pattern. I agree it is a stacking\npattern that rhymes with shit. It's a terrible pattern. You'd never use this. However, my point\nwas to demonstrate that if you change\nstacking patterns, it can impact readability. That's all I wanted to prove. Now, here's one where\nthe green is the tag location on these cases. And we have the same\norientation, the same patterns. And I just turned the tags\ninward rather than leaving them at the perimeter. And those red reads, 25\ntrials, not a single time that case read. These are individual tiers\nstacked on top of each other. Same with the bottom ones, even\nthough you have the channels into the tags. Now, here's just\nkind of a reversed. All perimeter tags\non the top one. Good readability,\nlike no failures. That's real handy. Inward-facing tags-- death,\nexcept on the top where there was no case surrounding it. Now, what this means,\nand we've done a lot of talking on processes here. One of the processes\nthat may have to occur before RFID enables\nall the benefits we're talking about in the supply\nchain is to look at changing the pallet\npatterns or case counts to maximize perimeter tagging. In the video that DHL\nshowed, did anybody noticed that that pallet\nwas a half pallet? It wasn't a typical GMA\npallet like we use here in the United States. Grocery Manufacturers\nAssociation. A 40 by 48-inch pallet. It was a half pellet. And if you go through\nEurope, you'll also see quarter pallets. And one of the benefits of\nthat system is you always-- almost always. Pretty much always get that\nperimeter tagging opportunity, and therefore,\nbetter readability. Now, product impact. Water, we tested\nsome, and we found a case that would be\nreadable, non-readable. Readable, non-readable. Nothing changed. What the hell's going on?"}, {"content": "I'm going crazy here, all right? What we found was we\nhad heads of lettuce. And a head of lettuce\nwould rotate and put a flat portion of the head\non the back side of the case where the tag was. And if that hunk of\nwater in a lettuce was up against the\ncorrugated behind the tag, you can't read the tag. And that was\ndemonstrated last night with the different thicknesses\non the water with Dan. When you rotated the\ntray full of water, you needed additional\nthicknesses because you need to separate\nthe tag from the water to allow the signal to get\nin and energize the antennae. And here's a perfect example."}, {"content": "We found it in lettuce. We found it in peaches. We found it in a couple\nof different products. We started looking at reading\non shrinkwrap situation. Stretch wrap, excuse me. And this is a portal\nthat has four antennae. This is part of the antennae\nconfiguration and readability. And reading, wow,\nit's because it's on there for roughly 30\nseconds while the stretch wrap is being put on. And we found some fun results,\nbut that'll be out shortly, and I'll teach all of that."}, {"content": "One of the things we're\ncurrently doing now is looking at water\non corrugated. Corrugated has huge markets and\nopportunities within produce. No question about it."}, {"content": "And here, we've started a\nseries of tests on these. We have produce\ntrays, and in the top, we just have a squeeze bottle. Real scientific. Cost me, I think, $0.89 to\nempty and fill up with water. And we spray on the tag. The one below that, we\ngot a squeeze bottle. That was cheaper. That was, like, $0.39, and we\njust squeeze a stream on it. Now, on the top, we run the tap,\nand we have a calibrated hand splashing water across the tag,\nlooking at different levels of moisture on the tag. And here's one of the\ninteresting things that we found. If you have untreated board,\nany water on the tag, the tag disappears. You can't read it. And in the bottom\npicture, you see how the water has\nworked its way into some of the holes of the case. It holds moisture. You still have problems. And you can't always read those\nafter the water application stops. However, if you\nhave treated board and you do this exact test,\nas soon as you stop the water, whether it's a spray, a squeeze\nbottle, or a flood of water, the second you stop\nthat water that drains, that tag reads again. That is no different\nthan a tag embedded in a returnable\nplastic container. If you run water over those,\nthe tags still disappears. There's no magic that if\nyou bury a tag in plastic, you can still spray it with\nwater and get the tag to read. You can spray it with\nwater for cleaning. But here was an\ninteresting thing."}, {"content": "My wife is here with me. And I was talking to her\nabout returnable containers, and she worked for\na company in Dallas. We made a one-way\ncontainer for a company, and it was electronic\ncompany in Dallas. They came back a\nyear later and wanted us to rebuild this\ncorrugated box, and it was about the length\nand width this table. About this deep. They had used it every week\nfor a shipment to a plant and back for a\nyear, and then they asked them to rebuild it\nrather than a new one. Out of corrugated. Now, it was triple wall, but\nthere is a good reusable. Frito-Lay uses returnable\ncorrugated boxes for all of their chips\nbecause they control the placement on the\nshelves, and then they tick the boxes back,\nand they reuse them, and they can reuse them\nfor months at a time because they don't get beat up. Some good opportunities."}, {"content": "Now, There's what\nwe talked about. Read stopped during water. They'll continue to fail\nif it becomes saturated, but they resume failing\nif it's treated. And with that, I'm going to\nsit down and start drinking, and I'm going to turn this\nover to Dr. [INAUDIBLE] from Florida."}, {"content": "Appreciate your\ntime and attention. [APPLAUSE] [INAUDIBLE] PROFESSOR 2: I'm very\nsorry for some of you that were done with me yesterday. I'm always back. It's like a bad cold. I always come back. I'm going to go fast\nbecause we have lunch, and so I'm sorry\n[INAUDIBLE] listening. OK. Well, thank you again. And we are going to\ntalk about packaging and a few of the thing that Rob\ndiscussed a little bit earlier, I'm going to dressed them. That's very interesting. Very quick, it's-- I explained about the Center for\nProduction Retailing yesterday. And we are trying to\ndevelop smart packaging for the food industry and\nthe pharmaceutical industry. And the one thing\nthat we started is well in this area of RFID. This is what happened in 2003,\nwhen Fretwell, a company based in Plant City, Florida,\ncame to us and said, we're going to give\nyou an RFID lab and start to play\nwith this thing. And the reason why it was a\ngood mass for the food industry is because we have\nalready infrastructure. We have 14 cold\nrooms, temperature, humidity programmable. And we have two freezers. We have two\n[INAUDIBLE] container. One refrigerated trailer,\nand even a 727 freighter that they don't allow me to\nfly, luckily for everybody. But the goal was,\nat that time, is that to get all the RFID\nhardware and software manufacturer to\nhave a start there. And since then,\nevery three months, they come and they upgrade our\nequipment, software, hardware. So that's pretty neat for that. We always get the new\nstuff pretty ahead. So our lab is designed like Rob. We have everything to measure\nall the impact of packaging and things like. That you see, Rob? I use a drawing so I don't\nhave to put the boards to hide my dirt under this thing. OK, so very quick. The first thing that we\ndid was with [INAUDIBLE] and Fresh Express, prepared\nsalad and a head of lettuce. And we did shipping\nfrom coast to coast. Studied the tag location, and\nthen the speed of the load. I'm going to go fast on that. The idea was to map a pallet\nand see where are the good place to put a tag and the other\nplace that were critical not put a tag because you\ncouldn't read it because the effective\nof the pallet. So we played for it\nwith that for a while until we decided that we\nhave to go further with that. So we started to compare\ndifferent materials-- plastic, wire bound, wooden\ncrate, and corrugated boxes. And I apologize\nfor the wood crate. I was still learning how to\ndrive the Publix forklift. And the corners were\npretty stiff on this thing. But the idea was to compare\nwhat was the effective if you're using because these are\ncoming from the same grower, and they have different kind of\npackaging and things like that. So we wanted to study that."}, {"content": "And we came with\na different thing. But the problem\nthat we discovered is that when people were looking\nfor readability, read rate, we were having\npretty low number. And they were like, oh, man."}, {"content": "That's awful. But I said, welll, it's\nbecause we were losing the tag. You cannot read them if\nthey are not there anymore. And we started to discover a\nlot of falling tags everywhere. And that was a big issue. One thing that we\ndecided is that, why we don't look a little\nbit further and see when you pack\nsomething in a package, it should stay with the package,\nthe RFID tag all the time, at least identification. And this is where it started to\ntrigger that we should always try to improve the\nway of mixing tag, RFID tag with packaging by\ntrying to know if we cannot, in the process of\nmaking the package, just get the RFID tags inside\nand make it one step rather than many steps. So we started to\ninvestigate that. So what we did is\nthat we started to embed RFID tag on\nreusable plastic container. Of course, we reduced the\nuse, the cost per use. So this tag is there. You cannot see it,\nbut it's there. And that was doing with the\ninjection molding plastic that we did that. So we can always track\nthe base because the wall, you can change them. Because if you break one\nwall, you can change it. But the base is always the base. And so we can always track\nthe life of these RPC because sometimes, they are\nused in different industry where you're trying to use\nthem for different use. And so what we did during\ninjection molding process, and it cannot be removed. But the interesting\nis that we were always wondering, when\nyou inject plastic, it's very high temperature, and\na lot of friction in the mold. And is a tag can survive that,\nand can we locate the tag at the right place. And if that happened, it\nwas easier than we expected. They were very tough. And what is\ninteresting is that I can cut pretty much the price\nof that by skipping a few steps. So I don't need a fancy\nRFID tag in this thing, and I don't need very much. In fact, we're\ngoing to see later, I only need the chips\nonce in a while. So the price of the\nwhole process is made. And this thing doesn't\nrequire any new equipment for the injection\nmolding company. Any company can do that. So what we did was so\nwe started to invest on smaller container,\nwhich was more challenging because the\nthickness of the chip is what is limit the\nthickness of my wall. But you can always\nfind a place to put it. So as I said, we don't\nneed these fancy labels. We need a chip, and that's it. And everybody can do it. So that's pretty cheap\nat what point, OK? So injection molding\ncompany can run a batch of regular\ncontainer and just switch in a matter of a few\nseconds to with one with embedded RFID tag. And this is what we\nare doing right now. Of course, everybody's\nlike, oh, item level. It's way too ahead, you know? But it just proved that I\ncan take RFID tag container and put that in this\ncontainer and drive the price to very, very cheap to the\nprice on most of the chip only. So that's very\ninteresting on that. Well, and also, we\nstarted to do with project with different\nnature of product. And of course, because of\nthe water and what we said, one of the big issues that if\nI have my tag and with water, I cannot read it. So we sometimes, we have\nto redesign the package. Not very much, but slight chance\nthat I can position my RFID tag to a location that\nit will not interfere with being interpreted by\nthe food that I put inside. So we're able to pinpoint\nlocation and redesign slightly some of these container\nand get these good read. In fact, I'm going to tell\nyou something interesting. We did 1,000 container with tags\nembedded, and only one of them failed. All of them were good."}, {"content": "So that was pretty\ninteresting on that. What we are doing also\nin 2000, right now, is that we are looking for\nsmaller and thinner design. Some of them, we just want\nto read 6 to 12 inches. So we're going to need some\nhelp on the design, antenna design on this thing. So I'm just asking\neverybody to help. On the pharma\nindustry, well, this is something interesting\nbecause your container, if you put pills\nin it and you want to follow that, I\ncan remove a label. But if it's inside\nthe container, I cannot remove the label. It's inside the container. I can stack it up\nwith tons of label. Well, it's still part\nof the container. And we did that. And sort of the thing is\nthat prescription drug, using [INAUDIBLE] container\nhas been something that we're working\nin pretty close to be successful on this one. And also, we work\non the blood product right now and packaging\ncompany that you put in when you ship the\npharmaceutical product on this thing. That was a fast\npresentation, but I'm going to let you,\nafter that, just doing that, we're working very much,\nbut we need some cooperation also. So thank you very much. [APPLAUSE] Once it's fast. Thank you. [INTERPOSING VOICES] PROFESSOR 3: We'll do\nthe Laurel, Hardy thing. Jean-Pierre and I were talking. We were going to have a\nrace to see who could do the presentation the fastest. So I'm going to win on this one."}, {"content": "But all right. I'm first going to tell\nyou guys who we are and why you should care. Then I'm going to\ntell you about what we're looking for out\nthere as far as technology and why we want that. So we'll flip through. Or once I figure out\nhow to work this. Ah. MeadWestvaco. We're into consumer packaging. We're into three\nprincipal areas. And if you look at these\nthree principal areas and you think about RFID\nand ubiquitous tagging, these are three\nareas where they're prime candidates for RFID. The media and\nentertainment here, you have these high-value\nitems in small packages. You're ending up\nwith high shrinkage. You have the area where an\nout of stock is a big issue because if somebody\nwalks in there and they don't find\nthe title they want, they're not going to\nbuy some other title. They're going to go\nto some other store with all the money\nin their pocket, and they're going to buy\neverything else they needed at that other store also. Or to the consumer products. We heard about simply\nwhite earlier on here. That's another interesting thing\nfor tracking out of stocks. And then we have the\nhealth care industry, where you have to be the\nhuge issue of counterfeits. The FDA's basically\njust come down that basically strongly suggests\nRFID in the packaging there. The other area that\nyou're probably more familiar with MeadWestvaco,\nor at least the Mead name, is in the consumer and\noffice products area. And we're into RFID in that\narea thanks to Walmart. We're at the pallet level in\nshipping our goods on that, in that product line. Now, we've been\nlooking at the RFID, participating in this area\nfor about six years now. We were one of the early\nmembers of the auto ID center up here at MIT. We also founded a\ngroup in our company called intelligent systems. And I'm not sure if any of\nyou had read about them. They were in the news for\nquite some time now here. Basically, that group came\nup with an intelligent shelf system where, unlike some\nof the other ones that were out there where people\nhad sent multiple wires to each of the antennas trying\nto reduce the cost, we found a way of\ntaking one reader and basically using that\nreader to read 100 antennas and having a single wire running\ndown the line of 100 antennas and bringing it back. This, as you can\nthink about this, is a really unique\nway of reducing the cost of the infrastructure\nby several orders of magnitude."}, {"content": "And that was essentially our\ngoal here as we got into this. We looked at the\nfield and realized that for tags to be\nubiquitous, the tags were going to have to\nbe commodity-type items. And there were plenty\nof players out there like Alien-- we were in\ndiscussions with them-- who were doing a very\ngood job of that. But people weren't really\naddressing the infrastructure successfully. So now, this last year, we\nsold the intelligent systems or most of the Intelligent\nsystems to an outside group so that they could really\nconcentrate on it full force. We still have a\nresidual interest and are following\nit because we do think that it's very\nimportant for success in this field for that\ncompany to be successful. Our capabilities at\nthis point in time, we do have production\ncapabilities for doing item-level coding\nof tags, of testing those tags in line and tagging boxes. We have not seen the pool\nfrom the customer base, but we're positioned,\nand we're trying to improve that position\ndaily so that we're ready when the market pull comes along. But basically,\nour ability to tag is anything we can put\nan EAS tag on today, we can switch over\nand do an RFID tag. Now, what we need as\nfar as technology. We're looking for\nthe things that are going to make our\ncustomers successful. And probably the\nprimary thing there would be item-level\ntracking at low cost. And we've been hearing\ndiscussions yesterday and today on some of the things that\nwould go into reducing the cost of the tag and such. But I also want\nto remind everyone that this doesn't\nnecessarily mean 96-bit RFID. There's a lot of attention\nout there to chip lists, tags, to organic RFID. And these are areas\nthat we're exploring and discussions with\nvarious companies on. But if you were going forward\nlooking into the field, one of the things that I want\nto caution everyone on is that these things cannot sit\nthere and expect to come in and require a whole\nnew infrastructure through the whole supply chain. I mean, the 96 bit\nis a juggernaut going down that road there. We're going to see that\ninfrastructure go in there. Or if the organic RFID can\nonly read at 125 k megahertz and you don't have\nan agile reader that can read at both\npoints, you're going to have a hard\ntime selling those, that organic RFID in there. The same with the chipless tags. If you've looked at the\nvarious chipless technologies, it's very interesting, and\nit's very appealing to us because our desire is\nto be able to just print the RFID on there. That's the way we're going\nto reduce the cost down for our customers. But again, what are you looking\nat when you get a chipless tag? You're probably not\nlooking at 96 bits, and you're definitely not\nlooking at the same reader that you're using for reading\nyour silicon-based tag. Now, there are value\npropositions out there that would draw towards\nthings, these technologies. If you can identify the niche\nmarket that you're going after, we may have something\nto talk about. All right. The other thing\nis we have to look at the total cost of the\ntag over the lifetime. It's not just the\ncost of the chip, and I think we've\nheard that before. It's the cost of-- how much does it cost to\nactually put it on the package? What's the antenna cost? What's the crossover cost there? What's going to happen\nas far as testing. How do you handle\nthe rejects when you get failed reads\nin the supply chain? How are you going\nto handle those? Those represent costs. And then finally, and we've\nheard that alerted to before, this disposal issue is going\nto come and catch up to us at some point in time here. What is in your antenna? Look at Europe. Look at-- there's\nplaces in New England that are starting to look at\nthe metals and the landfills. And then the last\narea is one that's gotten a lot of attention\nhere, and I'm glad to see it, and that's in the whole\narea of track and trace. Track and trace is a\nvery, very valuable thing to look at,\nespecially when you're looking at the\npharmaceutical industries. But what's going to be out\nthere in the data management infrastructure? Who's going to\ncome up with that? How are we going\nto handle this data asset from various entities\nin the supply chain? That's it. I beat John Pierre. [APPLAUSE] PROFESSOR 1: I guess we're\ngoing to run this ourselves, so if you have any\nquestions, come on down. Sorry, we felt like Miles\nDavis there with our back to the audience. But we wanted to learn\nsomething new at the same time, and it's hard to do it that way. So excuse us. Yeah. AUDIENCE: OK."}, {"content": "Just one question. Again, for what\nchanges are you seeing to the metrics as you\nexperiment with this technology in your packaging design? What do you measure to\nmeasure the performance? Is that measure changing? Units that you measure\nmay be changing. What have you experienced\nwith all your testing or what you might\nsee in the future? PROFESSOR 1: I'm not sure\nthis is the same answer, but I don't see much new at all. Honestly, units are the same\nthat I used in my graduate work studying random vibration\nas an energy source. RF is a great area, and I'm not\nhere to disavow it in any way. But it's a technology. It's not a panacea. With all due deference\nto previous speakers, I don't think it's\nworthy of a discipline. I did get caught short here\nbriefly by asking the gentleman from IBM if they still had\nbar code specialists on staff, and he said, yes, we do,\nalthough he might not have been sure what they did. But I know in Michigan,\nwe've gotten rid of bar code specialists. And one of my students is\nworking with Simon Langford at Walmart running their lab. And when he was hired,\nthe comment Simon made was that we hope in\nthree years you don't have a job because the\nintent was to make this so broadly based across\nthe company that it becomes as common as UPC, and you\ndon't need anything special. It's just an adaptive\ntechnology that helps. AUDIENCE: Thank you. AUDIENCE: Hi, I'm\nPatrick [INAUDIBLE] from Georgetown University. And I want to thank you guys\nfirst for this presentation. I think it outlined a\nlot of the shortcomings in this technology set in on-- PROFESSOR 1: Opportunities. AUDIENCE: Opportunities, yes. Absolutely. And I've sat in on several\ngovernment meetings on launching RFID for\nidentity management. And the shortcomings that\nI'm seeing time and time again are the lack of\nunderstanding from, and I don't want to name\nnames, but these consulting companies that kind of\npromise RFID in a box and passive allowing\nyou to drive through at 55 miles an hour\nand things like that. And it's just not\nbeing answered. So I think research\nlike this would be very advantageous\nto be out there in the public for the industry. My question is on\npharmaceutical. The FDA has asked to secure\naround counterfeiting that there's over,\ncovert, and forensic. And in the launches\nthat are out there now, you're seeing overt and covert. You're not seeing forensic. And I'm curious if\nanyone can answer that. And then my last as a\npotential wag of the finger. I hope none of you are\nresponsible for those damn huge plastic containers for\nmy small microchip that are frustratingly\nimpossible to open. So thanks again. [INTERPOSING VOICES] PROFESSOR 1: I don't have\na really good answer, I guess, on the forensic. We have done work with\nboth overt and covert, and there are much better\nexperts than at least the three of us on that. It's kind of interesting. If you guys pull out $100 bill\nand just pass them forward, I'll show you how they\nhave all of these security or anti-counterfeiting\ntechniques involved. And for those of you visiting\nfrom different countries, our currency now has 20 overt\nor covert anti-counterfeiting detection devices built into\nit and at least 10 others that they won't talk about. But there's roughly 30\nanti-counterfeiting features in an individual bill. And those are the\npeople you really need to bring into\nthis discussion because I'm certainly\nnot a specialist in that. PROFESSOR 3: All right. Well, I'll answer that. I'd like to answer that. We're on it. We can't tell you what we're\ndoing, but we're on it, and we have those\nthree areas covered. PROFESSOR 2: Very quick. And also, on the area\nthat we never discuss is temporary evidence. And have you seen\nsomething of plastic? It said if it's not\nthere, don't use it. But if it's out there, you\ncannot read it anymore. And so we're discussing, and the\nbest protection for [INAUDIBLE] is that nobody can see it. And this is what\nwe're working on. AUDIENCE: Thank you. Good presentation. PROFESSOR 1: Thank you. AUDIENCE: Yeah, John\nHelford from Mars. My question is, how\nis RFID starting to work its way into the\ncurriculum, particularly undergraduate, because it's\nimportant that they know? And also, how are you\ntraining the guys that aren't in school anymore about\nthe utilization of technology in a supplier base? I'd be interested\nin that question. PROFESSOR 1: From a Michigan\nState standpoint, at least, I started teaching\na class on RFID and packaging two years ago. And it's been an\nelective class, but I've had really good turnouts\nnot only from packaging but from engineering,\nfrom supply chain management, logistics, because\nall of these cross over, these areas. And we now have\ncompanies coming in to hire our undergraduates and\neven our graduate students that will not even interview\nthem if they haven't had the class because this is an\narea that companies are trying to find those answers and\nthey're trying to find people that have hands-on work and feel\nso that they come in hitting the ground running, as opposed\nto having to be trained. They're also, just\nlike yourself, conferences up the wazoo. You know, JP and I've\ntalked repeatedly about we could be at a\nconference every week of the year. And if you could get more\nof them in Hawaii, we might. But that's really what\nyou're almost limited to. But we are developing an\nonline, web-based program for recent graduates\nlike yourself. PROFESSOR 2: Yeah, very quick. And [INAUDIBLE] our\npackaging science program. Almost every course is where\nyou can have RFID, a topic in it is already there. So we have a chapter in\ntransportation, distribution, food packaging. We have customer product. All these packaging\ncourses, they all have chapter where it's\nsuitable to put it inside. PROFESSOR 3: And\nwell, basically, we've been talking to our\nsenior management about it for probably six years now."}, {"content": "And I think they get it now. We're way past the\nearly days when people thought RFID meant you'd\nread them from satellites. I think if you cornered anyone\nin our senior management team, you'd have a pretty\ngood conversation. PROFESSOR 1: Last question. AUDIENCE: Hi. My name is [INAUDIBLE]\nfrom Gillette P&G. As part of EPCglobal,\nthere's a strong effort to try and standardize\non measures and metrics and, like you mentioned,\nJohn [INAUDIBLE] and both Jeff [INAUDIBLE]\nare co-chairs. I'm the third co-chair\nof that group. We'd like to get more academic\ninvolvement into that group. How do we make that\nhappen so that you're not going off and doing these tests\nthat are very interesting, but if we can try and\nsynergize on those? I'd like to hear your\nthoughts on that. PROFESSOR 1: Great issue. I actually had a\nconversation with some people from EPCglobal yesterday\nto try and synthesize some of the work we're doing. I'm a co-chair--\nactually, a chair of a new task group within\nASTM, the American Society of Testing Materials, where\nwe are developing procedures and reporting on how you test. Case loads, pallet loads,\nmilitary applications, pharmaceutical\napplications, et cetera. We want to work in conjunction\nwith you on developing these and share all the data. Where we're different from EPC,\nbecause it always comes up, is that we are reader\ntag frequency agnostic. We don't care what you use. Here is a standard\nprocedure for measuring readability of a case,\nreadability of an item. Read distance, read fields. Military. Whatever it is. And so if it fits into\nEPC through 13.56, 9.15, they might be identical. But if you have a\nclosed loop system and are looking at\nthat 134 or 2.4, 5.8, and even different\nfrequencies from that-- there's so much in\nmilitary at 4.33 right now that we just want\nto set up what that is. And if anybody contacts me,\nI'll be happy to respond. PROFESSOR 2: Maybe I can do\nsomething about it, about EPC and the standardization. I would say that in our lab,\nwe touch many, many areas. Some of them will fall under\nEPC what you're looking for, and that's going to be good. But we still are going\nto keep doing other areas because the request of other\napplications is pretty big. And so if some of\nthe things that we do will fall under EPC what are you\nlooking for, this is great, OK? But we still going to\nhave a lot of things that we want to\nkeep running also. Thank you. [INAUDIBLE] [APPLAUSE]"}]}